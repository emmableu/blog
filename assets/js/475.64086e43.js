(window.webpackJsonp=window.webpackJsonp||[]).push([[475],{847:function(t,r,_){"use strict";_.r(r);var e=_(9),v=Object(e.a)({},(function(){var t=this,r=t._self._c;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"深度强化学习-deep-rl-学习路线与资源整理-中文翻译"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#深度强化学习-deep-rl-学习路线与资源整理-中文翻译"}},[t._v("#")]),t._v(" 深度强化学习（Deep RL）学习路线与资源整理（中文翻译）")]),t._v(" "),r("blockquote",[r("p",[t._v("来源："),r("a",{attrs:{href:"https://www.reddit.com/r/reinforcementlearning/comments/zi7qae/best_reinforcement_learning_course/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Reddit r/reinforcementlearning"),r("OutboundLink")],1),t._v(" 由 E-Cockroach 提供")])]),t._v(" "),r("hr"),t._v(" "),r("h2",{attrs:{id:"🎓-教学背景"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#🎓-教学背景"}},[t._v("#")]),t._v(" 🎓 教学背景")]),t._v(" "),r("p",[t._v("我连续三年担任我们学院的深度强化学习课程讲师（共8个学分），但我本身仍是一名学生，因此请谨慎参考。")]),t._v(" "),r("p",[t._v("我们"),r("strong",[t._v("不考虑模型型强化学习（Model-based RL）")]),t._v("，因为那是另一个主题/更复杂/有点蠢（玩笑）。")]),t._v(" "),r("hr"),t._v(" "),r("h2",{attrs:{id:"📚-学习路线建议"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#📚-学习路线建议"}},[t._v("#")]),t._v(" 📚 学习路线建议")]),t._v(" "),r("h3",{attrs:{id:"✅-阶段一-强化学习基础"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#✅-阶段一-强化学习基础"}},[t._v("#")]),t._v(" ✅ 阶段一：强化学习基础")]),t._v(" "),r("ol",[r("li",[t._v("理解基础概念，例如经典强化学习（Chapters 1~9 of Sutton & Barto）")]),t._v(" "),r("li",[t._v("完成至少一半的课后题目")]),t._v(" "),r("li",[t._v("理解并实现以下核心算法：")])]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("算法")]),t._v(" "),r("th",[t._v("推荐操作")]),t._v(" "),r("th",[t._v("附注")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("DQN")]),t._v(" "),r("td",[t._v("理解并实现")]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Rainbow DQN")]),t._v(" "),r("td",[t._v("实现 Double-Q + 分支 Q 网络")]),t._v(" "),r("td",[t._v("只需实现这两部分")])]),t._v(" "),r("tr",[r("td",[t._v("REINFORCE")]),t._v(" "),r("td",[t._v("按照 "),r("a",{attrs:{href:"https://lilianweng.github.io/lil-log/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Lil'log 博客"),r("OutboundLink")],1),t._v(" 实现")]),t._v(" "),r("td",[t._v("非论文版本")])]),t._v(" "),r("tr",[r("td",[t._v("A2C")]),t._v(" "),r("td",[t._v("理解并实现")]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("A3C")]),t._v(" "),r("td",[t._v("阅读论文（不需实现）")]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("PPO")]),t._v(" "),r("td",[t._v("阅读 PPO 和 TRPO 论文 + 博客")]),t._v(" "),r("td",[t._v("不需实现")])]),t._v(" "),r("tr",[r("td",[t._v("DDPG")]),t._v(" "),r("td",[t._v("理解并实现")]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("TD3")]),t._v(" "),r("td",[t._v("理解并实现")]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("SAC")]),t._v(" "),r("td",[t._v("理解并实现")]),t._v(" "),r("td")])])]),t._v(" "),r("hr"),t._v(" "),r("h3",{attrs:{id:"✅-阶段二-进阶专题与论文推荐"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#✅-阶段二-进阶专题与论文推荐"}},[t._v("#")]),t._v(" ✅ 阶段二：进阶专题与论文推荐")]),t._v(" "),r("blockquote",[r("p",[t._v("*号代表建议实现的内容。")])]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("主题/论文")]),t._v(" "),r("th",[t._v("是否建议实现")]),t._v(" "),r("th",[t._v("附注")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("HER (Hindsight Exp Replay)")]),t._v(" "),r("td",[t._v("✅")]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("Inverse RL（2-3 篇基础论文）")]),t._v(" "),r("td",[t._v("❌")]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("World Models Paper")]),t._v(" "),r("td",[t._v("✅")]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("RIAL&DIAL（Foerster 论文）")]),t._v(" "),r("td",[t._v("❌")]),t._v(" "),r("td",[t._v("多智能体 RL")])]),t._v(" "),r("tr",[r("td",[t._v("MADDPG")]),t._v(" "),r("td",[t._v("✅")]),t._v(" "),r("td",[t._v("多智能体 RL")])]),t._v(" "),r("tr",[r("td",[t._v("Intrinsic motivation（Pathak）")]),t._v(" "),r("td",[t._v("✅")]),t._v(" "),r("td",[t._v("入门级推荐")])]),t._v(" "),r("tr",[r("td",[t._v("Offline RL（S. Levine 讲座）")]),t._v(" "),r("td",[t._v("❌")]),t._v(" "),r("td",[t._v("建议看讲座不看论文")])]),t._v(" "),r("tr",[r("td",[t._v("Conservative Q-Learning (CQL)")]),t._v(" "),r("td",[t._v("❌")]),t._v(" "),r("td")])])]),t._v(" "),r("hr"),t._v(" "),r("h3",{attrs:{id:"✅-阶段三-高级内容推荐-代表领域或重要概念"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#✅-阶段三-高级内容推荐-代表领域或重要概念"}},[t._v("#")]),t._v(" ✅ 阶段三：高级内容推荐（代表领域或重要概念）")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("内容")]),t._v(" "),r("th",[t._v("建议了解")]),t._v(" "),r("th",[t._v("附注")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Mirror Learning")]),t._v(" "),r("td",[t._v("✅")]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("AlphaGo")]),t._v(" "),r("td",[t._v("✅")]),t._v(" "),r("td",[t._v("推荐了解 MCTS 与模型型 RL")])]),t._v(" "),r("tr",[r("td",[t._v("RLHF（人类反馈强化学习）")]),t._v(" "),r("td",[t._v("✅")]),t._v(" "),r("td")]),t._v(" "),r("tr",[r("td",[t._v("IMPALA")]),t._v(" "),r("td",[t._v("✅")]),t._v(" "),r("td",[t._v("分布式 RL 算法")])])])]),t._v(" "),r("hr"),t._v(" "),r("h2",{attrs:{id:"🧠-最重要建议"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#🧠-最重要建议"}},[t._v("#")]),t._v(" 🧠 最重要建议")]),t._v(" "),r("blockquote",[r("p",[r("strong",[t._v("实现你学到的内容！")])])]),t._v(" "),r("p",[t._v("很多人读完论文就觉得自己懂了，但只有当你真正动手实现算法，才会意识到你还不是真的懂。"),r("strong",[t._v("实现是理解的唯一方式。")])]),t._v(" "),r("p",[t._v("否则你就像看着足球大喊“我也能踢”的人一样，实际上根本不懂怎么踢。")]),t._v(" "),r("hr"),t._v(" "),r("h2",{attrs:{id:"🔧-实现示例库"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#🔧-实现示例库"}},[t._v("#")]),t._v(" 🔧 实现示例库")]),t._v(" "),r("ul",[r("li",[t._v("SBX（Stable-Baselines3 实现工具箱）\n"),r("ul",[r("li",[t._v("GitHub 链接："),r("a",{attrs:{href:"https://github.com/araffin/sbx",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/araffin/sbx"),r("OutboundLink")],1)])])])]),t._v(" "),r("hr"),t._v(" "),r("h2",{attrs:{id:"📺-强烈推荐的学习资源"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#📺-强烈推荐的学习资源"}},[t._v("#")]),t._v(" 📺 强烈推荐的学习资源")]),t._v(" "),r("h3",{attrs:{id:"视频课程"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#视频课程"}},[t._v("#")]),t._v(" 视频课程")]),t._v(" "),r("ul",[r("li",[t._v("David Silver 强化学习课程："),r("a",{attrs:{href:"https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ",target:"_blank",rel:"noopener noreferrer"}},[t._v("YouTube 视频列表"),r("OutboundLink")],1)])]),t._v(" "),r("h3",{attrs:{id:"教科书"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#教科书"}},[t._v("#")]),t._v(" 教科书")]),t._v(" "),r("ul",[r("li",[t._v("Sutton & Barto: "),r("a",{attrs:{href:"https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Reinforcement Learning: An Introduction (第2版PDF)"),r("OutboundLink")],1)]),t._v(" "),r("li",[t._v("Phil Winder 的《Reinforcement Learning》书籍（O'Reilly）")])]),t._v(" "),r("h3",{attrs:{id:"sutton-barto-课后习题解答"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#sutton-barto-课后习题解答"}},[t._v("#")]),t._v(" Sutton&Barto 课后习题解答：")]),t._v(" "),r("ul",[r("li",[t._v("GitHub 解答合集："),r("a",{attrs:{href:"https://github.com/LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions"),r("OutboundLink")],1)])]),t._v(" "),r("hr"),t._v(" "),r("h2",{attrs:{id:"📘-更多推荐课程和资源-无特别顺序"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#📘-更多推荐课程和资源-无特别顺序"}},[t._v("#")]),t._v(" 📘 更多推荐课程和资源（无特别顺序）")]),t._v(" "),r("blockquote",[r("p",[t._v("推荐结合第1-3阶段内容一起学习，帮助理解。")])]),t._v(" "),r("ul",[r("li",[r("a",{attrs:{href:"https://www.microsoft.com/en-us/research/event/reinforcement-learning-day-2019/agenda/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Microsoft RL Day 2019"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://web.mit.edu/dimitrib/www/RLbook.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Dimitri Bertsekas MIT RL Book"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021",target:"_blank",rel:"noopener noreferrer"}},[t._v("DeepMind RL 2021 讲座系列"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"http://rail.eecs.berkeley.edu/deeprlcourse/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Berkeley DeepRL 课程主页"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/",target:"_blank",rel:"noopener noreferrer"}},[t._v("UW CS885 RL 课程"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://www.andrew.cmu.edu/course/10-703/",target:"_blank",rel:"noopener noreferrer"}},[t._v("CMU 10-703 课程"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://web.stanford.edu/class/cs234/index.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Stanford CS234 课程"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://amfarahmand.github.io/IntroRL/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Amir-massoud Farahmand RL 教程"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://sites.google.com/view/deep-rl-bootcamp/lectures",target:"_blank",rel:"noopener noreferrer"}},[t._v("Deep RL Bootcamp"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"http://dalimeeting.org/dali2017/data-efficient-reinforcement-learning.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Data Efficient RL @ DALI 2017"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://www.ias.edu/math/ndrlc",target:"_blank",rel:"noopener noreferrer"}},[t._v("IAS 数学研究所 NDRLC"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://cs330.stanford.edu",target:"_blank",rel:"noopener noreferrer"}},[t._v("Stanford CS330"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://chandar-lab.github.io/INF8953DE/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Chandar Lab INF8953DE"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://www.youtube.com/playlist?list=PLp0hSY2uBeP8q2G3mfHGVGvQFEMX0QRWM",target:"_blank",rel:"noopener noreferrer"}},[t._v("Deep RL 讲座播放列表"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://www.davidsilver.uk/teaching/",target:"_blank",rel:"noopener noreferrer"}},[t._v("David Silver 教学主页"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://www.youtube.com/watch?v=EcxpbhDeuZw",target:"_blank",rel:"noopener noreferrer"}},[t._v("David Silver AlphaGo 背后的视频"),r("OutboundLink")],1)])]),t._v(" "),r("hr"),t._v(" "),r("blockquote",[r("p",[t._v("📌 如果上面的内容不够用，可以继续搜索更多资源，但这些是我为学生精选的、具有代表性和高价值的强化学习学习资料。")])]),t._v(" "),r("h2",{attrs:{id:"rlhf-前需学习的-sutton-barto-章节指南"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#rlhf-前需学习的-sutton-barto-章节指南"}},[t._v("#")]),t._v(" RLHF 前需学习的 Sutton & Barto 章节指南")]),t._v(" "),r("h3",{attrs:{id:"一、学习目标-before-rlhf"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#一、学习目标-before-rlhf"}},[t._v("#")]),t._v(" 一、学习目标 (Before RLHF)")]),t._v(" "),r("p",[t._v("您需要熟悉以下基础概念：")]),t._v(" "),r("ul",[r("li",[t._v("马尔可夫过程 MDP")]),t._v(" "),r("li",[t._v("状态值、动作值、Bellman 方程")]),t._v(" "),r("li",[t._v("动态规划")]),t._v(" "),r("li",[t._v("蒙特卡罗方法")]),t._v(" "),r("li",[t._v("时间差分 TD 方法")]),t._v(" "),r("li",[t._v("策略改进与策略梯度")]),t._v(" "),r("li",[t._v("探索 vs 利用 (exploration vs exploitation)")])]),t._v(" "),r("hr"),t._v(" "),r("h3",{attrs:{id:"二、系统路线列表"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#二、系统路线列表"}},[t._v("#")]),t._v(" 二、系统路线列表")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("章节")]),t._v(" "),r("th",[t._v("标题")]),t._v(" "),r("th",[t._v("是否必学")]),t._v(" "),r("th",[t._v("备注")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("第1章")]),t._v(" "),r("td",[t._v("强化学习概述")]),t._v(" "),r("td",[t._v("✅ 必学")]),t._v(" "),r("td",[t._v("设定应用场景、角色、环境")])]),t._v(" "),r("tr",[r("td",[t._v("第2章")]),t._v(" "),r("td",[t._v("多臂老虎机 (Bandits)")]),t._v(" "),r("td",[t._v("✅ 必学")]),t._v(" "),r("td",[t._v("基础探索/利用机制")])]),t._v(" "),r("tr",[r("td",[t._v("第3章")]),t._v(" "),r("td",[t._v("MRP (有奖马尔可夫过程)")]),t._v(" "),r("td",[t._v("✅ 必学")]),t._v(" "),r("td",[t._v("状态值 V(s)的定义")])]),t._v(" "),r("tr",[r("td",[t._v("第4章")]),t._v(" "),r("td",[t._v("MDP (马尔可夫决策过程)")]),t._v(" "),r("td",[t._v("✅ 必学")]),t._v(" "),r("td",[t._v("动作值 Q(s,a)、策略概念")])]),t._v(" "),r("tr",[r("td",[t._v("第5章")]),t._v(" "),r("td",[t._v("动态规划 (DP)")]),t._v(" "),r("td",[t._v("✅ 推荐")]),t._v(" "),r("td",[t._v("实现 Bellman 策略进化")])]),t._v(" "),r("tr",[r("td",[t._v("第6章")]),t._v(" "),r("td",[t._v("蒙特卡罗方法 (MC)")]),t._v(" "),r("td",[t._v("✅ 必学")]),t._v(" "),r("td",[t._v("根据样本进行值准备")])]),t._v(" "),r("tr",[r("td",[t._v("第7章")]),t._v(" "),r("td",[t._v("时间差分 (TD)")]),t._v(" "),r("td",[t._v("✅ 必学")]),t._v(" "),r("td",[t._v("TD(0)、SARSA、Q-learning 基础")])]),t._v(" "),r("tr",[r("td",[t._v("第8章")]),t._v(" "),r("td",[t._v("策略梯度")]),t._v(" "),r("td",[t._v("✅ 必学")]),t._v(" "),r("td",[t._v("PPO 等方法基石")])]),t._v(" "),r("tr",[r("td",[t._v("第9章")]),t._v(" "),r("td",[t._v("函数估计")]),t._v(" "),r("td",[t._v("✅ 强烈推荐")]),t._v(" "),r("td",[t._v("与深度学习接连")])]),t._v(" "),r("tr",[r("td",[t._v("第13章")]),t._v(" "),r("td",[t._v("Policy Gradient with Approx")]),t._v(" "),r("td",[t._v("✅ 推荐")]),t._v(" "),r("td",[t._v("提供 RLHF PPO 基石")])])])]),t._v(" "),r("hr"),t._v(" "),r("h3",{attrs:{id:"三、可暂时省略章节"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#三、可暂时省略章节"}},[t._v("#")]),t._v(" 三、可暂时省略章节")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("章节")]),t._v(" "),r("th",[t._v("标题")]),t._v(" "),r("th",[t._v("备注")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("第10章")]),t._v(" "),r("td",[t._v("离策略学习 Off-policy")]),t._v(" "),r("td",[t._v("RLHF 并不直接用")])]),t._v(" "),r("tr",[r("td",[t._v("第11章")]),t._v(" "),r("td",[t._v("eligibility traces")]),t._v(" "),r("td",[t._v("高级课题，可后看")])]),t._v(" "),r("tr",[r("td",[t._v("第12章")]),t._v(" "),r("td",[t._v("Planning (Dyna)")]),t._v(" "),r("td",[t._v("RLHF 并不使用")])])])]),t._v(" "),r("hr"),t._v(" "),r("h3",{attrs:{id:"四、结论"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#四、结论"}},[t._v("#")]),t._v(" 四、结论")]),t._v(" "),r("p",[t._v("RLHF 为 PPO + 人类评价 + 奖励模型组成，你需要基础策略和值准备方法的全面理解。")]),t._v(" "),r("p",[t._v("如果你想我按照这个路线进行 Leetcode 风格练习题和代码实现，我可以从第3章或第4章开始给你练习并输出 markdown + code solution")])])}),[],!1,null,null,null);r.default=v.exports}}]);