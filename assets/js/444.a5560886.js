(window.webpackJsonp=window.webpackJsonp||[]).push([[444],{815:function(t,e,s){"use strict";s.r(e);var a=s(9),r=Object(a.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("p",[e("a",{attrs:{href:"https://www.youtube.com/watch?v=efR1C6CvhmE&t=416s",target:"_blank",rel:"noopener noreferrer"}},[t._v("svm youtube"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("margin: distance between observation and margin")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("maximum margin classifier problem: (problem with hard margins)")]),t._v(" "),e("ul",[e("li",[t._v("sensitive to outliers")])])]),t._v(" "),e("li",[e("p",[t._v("soft margin: when we allow misclassifications, the distance between the observations and threshold is soft margin.")]),t._v(" "),e("ul",[e("li",[t._v("how to decide soft margin: use it as a hyperparameter and use cross validation")])])]),t._v(" "),e("li",[e("p",[t._v("soft margin classifier / support vector classifier:")])]),t._v(" "),e("li",[e("p",[t._v("what's support vector: the observations on the edge and within the soft margin are called support vectors.")])])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209191710708.png",alt:""}})]),t._v(" "),e("p",[t._v("the kernel trick:")]),t._v(" "),e("ul",[e("li",[t._v("calculate the relationship between data as if they are in higher dimension (when they are actually not in higher dimension).")])]),t._v(" "),e("h2",{attrs:{id:"intuitions-what-is-support-vector"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#intuitions-what-is-support-vector"}},[t._v("#")]),t._v(" Intuitions, What is Support Vector")]),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-0.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-1.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-2.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-3.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-4.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-5.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-6.png",width:"100%"}}),t._v(" "),e("h2",{attrs:{id:"definition-of-hyperplanes-and-margin"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#definition-of-hyperplanes-and-margin"}},[t._v("#")]),t._v(" Definition of Hyperplanes and Margin")]),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-7.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-8.png",width:"100%"}}),t._v(" "),e("h2",{attrs:{id:"using-lagrangian-multipler-method-to-maximize-the-margin"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#using-lagrangian-multipler-method-to-maximize-the-margin"}},[t._v("#")]),t._v(" Using Lagrangian Multipler Method to Maximize the Margin")]),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-9.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-10.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-11.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-12.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-13.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-14.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-15.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-16.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-17.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-18.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-19.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-20.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-21.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-22.png",width:"100%"}}),t._v(" "),e("h2",{attrs:{id:"inner-products-similarity-and-svms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#inner-products-similarity-and-svms"}},[t._v("#")]),t._v(" Inner Products, similarity, and SVMs")]),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-23.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-24.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-25.png",width:"100%"}}),t._v(" "),e("h2",{attrs:{id:"non-linear-svms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#non-linear-svms"}},[t._v("#")]),t._v(" Non-Linear SVMs")]),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-26.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-27.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-28.png",width:"100%"}}),t._v(" "),e("p",[t._v("举个例子，对于这个 polynomial kernel (a*b + 1/2)^2, 其实也就是 = (a, a^2, 1/2) 点乘 (b, b^2, 1/2)")]),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-29.png",width:"100%"}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-30.png",width:"100%"}}),t._v(" "),e("p",[t._v("除了上面这三个，还有一个比较常见的是Gaussian kernel")]),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-31.png",width:"100%"}}),t._v(" "),e("h2",{attrs:{id:"soft-margin-regularization-and-surrogate-loss-function"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#soft-margin-regularization-and-surrogate-loss-function"}},[t._v("#")]),t._v(" Soft margin, regularization, and Surrogate loss function")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-32.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-33.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-34.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-35.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/svm-36.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"difference-between-svm-and-logistic-regression"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#difference-between-svm-and-logistic-regression"}},[t._v("#")]),t._v(" Difference between SVM and Logistic Regression")]),t._v(" "),e("ul",[e("li",[t._v("LR gives calibrated probabilities that can be interpreted as confidence in a decision.")]),t._v(" "),e("li",[t._v("LR gives us an unconstrained, smooth objective.")]),t._v(" "),e("li",[t._v("SVMs don’t penalize examples for which the correct decision is made with sufficient confidence. This may be good for generalization.")]),t._v(" "),e("li",[t._v("SVMs have a nice dual form, giving sparse solutions when using the kernel trick (better scalability)")])]),t._v(" "),e("h2",{attrs:{id:"how-to-choose-which-kernel-to-use"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#how-to-choose-which-kernel-to-use"}},[t._v("#")]),t._v(" How to choose which kernel to use")]),t._v(" "),e("ul",[e("li",[t._v("（1）如果特征维数很高，往往线性可分，可以采用LR或者线性核的SVM；")]),t._v(" "),e("li",[t._v("（2）如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM；")]),t._v(" "),e("li",[t._v("（3）如果不满足上述两点，即特征维数少，样本数量正常，可以使用高斯核/RBF核的SVM。")])])])}),[],!1,null,null,null);e.default=r.exports}}]);