(window.webpackJsonp=window.webpackJsonp||[]).push([[441],{812:function(t,e,a){"use strict";a.r(e);var s=a(9),r=Object(s.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h2",{attrs:{id:"concept-of-ensemble-learning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#concept-of-ensemble-learning"}},[t._v("#")]),t._v(" Concept of ensemble learning")]),t._v(" "),e("p",[t._v("集成学习归属于机器学习，他是一种「训练思路」，并不是某种具体的方法或者算法。\n集成学习会挑选一些简单的基础模型进行组装，组装这些基础模型的思路主要有 2 种方法：")]),t._v(" "),e("ol",[e("li",[t._v("bagging（bootstrap aggregating的缩写，也称作“套袋法”）")]),t._v(" "),e("li",[t._v("boosting")])]),t._v(" "),e("h2",{attrs:{id:"bagging-bootstrap-aggregating-concept"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#bagging-bootstrap-aggregating-concept"}},[t._v("#")]),t._v(" bagging (bootstrap aggregating) concept:")]),t._v(" "),e("p",[t._v("bagging means bootstrap aggregating. It's a method to train an ensemble where, each constituent model trains on a random subset of training examples, sampled with replacement. For example, a random forest is a collection of decision trees trained with bagging.")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://www.youtube.com/watch?v=2Mg8QD0F1dQ",target:"_blank",rel:"noopener noreferrer"}},[t._v("youtube"),e("OutboundLink")],1),e("br"),t._v("\nBagging 的核心思路是 — — 民主。"),e("br"),t._v("\nBagging 的思路是所有基础模型都一致对待，每个基础模型手里都只有一票。然后使用民主投票的方式得到最终的结果。"),e("br"),t._v("\n大部分情况下，经过 bagging 得到的结果方差（variance）更小。")]),t._v(" "),e("p",[t._v("步骤：")]),t._v(" "),e("ol",[e("li",[t._v("create a number of subset of data, train each of them.\n"),e("ul",[e("li",[t._v("e.g., grab n' of the data from the original n dataset, randomly, with replacement (同一个bag里面可能会有重复的data)")])])]),t._v(" "),e("li",[t._v("use each of them to predict, and then take their mean.\n"),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-0.png",alt:""}})])]),t._v(" "),e("p",[t._v("举例：")]),t._v(" "),e("ul",[e("li",[t._v("在 bagging 的方法中，最广为熟知的就是随机森林了：bagging + 决策树 = 随机森林")])]),t._v(" "),e("h3",{attrs:{id:"随机森林算法-random-forest"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#随机森林算法-random-forest"}},[t._v("#")]),t._v(" 随机森林算法 random forest")]),t._v(" "),e("p",[t._v("Definition: the process to train an ensemble where, each constituent model (a decision tree) trains on a random subset of training examples, sampled with replacement.")]),t._v(" "),e("ol",[e("li",[t._v("一个样本容量为N的样本，有放回的抽取N次，每次抽取1个，最终形成了N个样本。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。")]),t._v(" "),e("li",[t._v("当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。")]),t._v(" "),e("li",[t._v("决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。")]),t._v(" "),e("li",[t._v("按照步骤1~3建立大量的决策树，这样就构成了随机森林了。")])]),t._v(" "),e("h2",{attrs:{id:"boosting-concept"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#boosting-concept"}},[t._v("#")]),t._v(" Boosting concept:")]),t._v(" "),e("p",[t._v("A technique that iteratively combines a set of simple and not very accurate classifiers into a classifier with high accuracy by upweighting the examples that the model is currently misclassifying.")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-1.png",alt:""}}),t._v("\nBoosting 的核心思路是 — — 挑选精英。"),e("br"),t._v("\nBoosting 和 bagging 最本质的差别在于他对基础模型不是一致对待的，而是经过不停的考验和筛选来挑选出「精英」，然后给精英更多的投票权，表现不好的基础模型则给较少的投票权，然后综合所有人的投票得到最终结果。"),e("br"),t._v("\n大部分情况下，经过 boosting 得到的结果偏差（bias）更小")]),t._v(" "),e("h3",{attrs:{id:"例子-adaboost-adaptive-boost"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#例子-adaboost-adaptive-boost"}},[t._v("#")]),t._v(" 例子：adaboost （adaptive boost)")]),t._v(" "),e("ol",[e("li",[t._v("AdaBoost combines a lot of “weak learners”, learners that are only slightly better at classifying the observations than random guesses, to make classifications. The weak learners are almost aways stumps, (stumps are  classification trees with only a root and two leaves).")]),t._v(" "),e("li",[t._v("The better a stump is at correctly classifying the training data, get more say it gets in the final classification.")]),t._v(" "),e("li",[t._v("Each stump is made by taking the previous stump’s mistakes into account.")])]),t._v(" "),e("p",[t._v("红色的点： 被当前model错误predict的data\n"),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-2.png",alt:""}})]),t._v(" "),e("h3",{attrs:{id:"adaboost-procedure"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#adaboost-procedure"}},[t._v("#")]),t._v(" Adaboost procedure")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212212265.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212213216.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212213224.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212214479.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212214446.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212214462.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212215085.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212216231.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212217762.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212218473.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212218551.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"bagging-和-boosting-的4-点差别"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#bagging-和-boosting-的4-点差别"}},[t._v("#")]),t._v(" Bagging 和 Boosting 的4 点差别")]),t._v(" "),e("h3",{attrs:{id:"样本选择上"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#样本选择上"}},[t._v("#")]),t._v(" 样本选择上：")]),t._v(" "),e("ul",[e("li",[t._v("Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。")]),t._v(" "),e("li",[t._v("Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。")])]),t._v(" "),e("h3",{attrs:{id:"样例权重"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#样例权重"}},[t._v("#")]),t._v(" 样例权重：")]),t._v(" "),e("ul",[e("li",[t._v("Bagging：使用均匀取样，每个样例的权重相等 （random with replacement)")]),t._v(" "),e("li",[t._v("Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。in the current training data, choose the sample that's mean predicted wrongly in the previous model")])]),t._v(" "),e("h3",{attrs:{id:"预测函数"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#预测函数"}},[t._v("#")]),t._v(" 预测函数：")]),t._v(" "),e("ul",[e("li",[t._v("Bagging：所有预测函数的权重相等。")]),t._v(" "),e("li",[t._v("Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。")])]),t._v(" "),e("h3",{attrs:{id:"并行计算"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#并行计算"}},[t._v("#")]),t._v(" 并行计算：")]),t._v(" "),e("ul",[e("li",[t._v("Bagging：各个预测函数可以并行生成")]),t._v(" "),e("li",[t._v("Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果")])]),t._v(" "),e("h2",{attrs:{id:"ensemble-learning-中的-base-classifier-基分类器"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#ensemble-learning-中的-base-classifier-基分类器"}},[t._v("#")]),t._v(" Ensemble learning 中的 Base Classifier (基分类器)")]),t._v(" "),e("p",[t._v("最常用的是decision tree，原因：")]),t._v(" "),e("ul",[e("li",[t._v("决策树做判断不受样本imbalance影响")]),t._v(" "),e("li",[t._v("数据样本的额扰动对决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性比较大，这样的“不稳定学习器”更适合作为基分类器。此外，在决策树节点分裂的时候，随机的选择一个特征子集，从中找出最优分裂属性，很好的引入了随机性。")])]),t._v(" "),e("p",[t._v("除了决策树外，神经网络模型也适合作为base classifier，主要由于神经网络模型也比较不确定，而且还可以通过调整神经元数量，连接方式，网络层数，初始权值等方式引入随机性。")]),t._v(" "),e("h2",{attrs:{id:"can-we-replace-the-base-classifier-in-random-forest-to-be-linear-classifier-or-k-means-classifier-why"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#can-we-replace-the-base-classifier-in-random-forest-to-be-linear-classifier-or-k-means-classifier-why"}},[t._v("#")]),t._v(" Can we replace the base classifier in Random Forest to be linear classifier or K-means classifier? Why?")]),t._v(" "),e("p",[t._v("This is not necessary.")]),t._v(" "),e("p",[t._v("bagging 本身是减小方差，所以需要高方差，低bias的base classifier，比如 decision tree， neural net")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-3.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"why-bagging-reduces-variance-and-tends-to-underfit"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#why-bagging-reduces-variance-and-tends-to-underfit"}},[t._v("#")]),t._v(" Why bagging reduces variance and tends to underfit")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-5.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-7.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"why-boosting-reduces-bias-and-tends-to-overfit"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#why-boosting-reduces-bias-and-tends-to-overfit"}},[t._v("#")]),t._v(" Why boosting reduces bias and tends to overfit")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-8.png",alt:""}})]),t._v(" "),e("h3",{attrs:{id:"gbdt-mart"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#gbdt-mart"}},[t._v("#")]),t._v(" GBDT (MART):")]),t._v(" "),e("p",[t._v("Gradient-boosting-based Decision Tree (or Multiple Additive Regression Tree)")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209221346340.png",alt:""}})]),t._v(" "),e("h3",{attrs:{id:"gbdt-pros-and-cons"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#gbdt-pros-and-cons"}},[t._v("#")]),t._v(" GBDT pros and cons")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-9.png",alt:""}})])])}),[],!1,null,null,null);e.default=r.exports}}]);