(window.webpackJsonp=window.webpackJsonp||[]).push([[435],{805:function(e,t,i){"use strict";i.r(t);var a=i(9),s=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"random-forest-v-s-gradient-boost"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#random-forest-v-s-gradient-boost"}},[e._v("#")]),e._v(" "),t("RouterLink",{attrs:{to:"/pages/63f233/#éšæœºæ£®æ—ç®—æ³•-random-forest"}},[e._v("random forest")]),e._v(" v.s. gradient boost")],1),e._v(" "),t("ul",[t("li",[t("RouterLink",{attrs:{to:"/pages/63f233/#éšæœºæ£®æ—ç®—æ³•-random-forest"}},[e._v("random forest")]),e._v(": the process to train an ensemble where, each constituent model (a decision tree) trains on a random subset of training examples, sampled with replacement.")],1),e._v(" "),t("li",[e._v("gradient boosting:\n"),t("ul",[t("li",[e._v("A training algorithm where weak models are trained to iteratively improve the quality (reduce the loss) of a strong model. For example, a weak model could be a linear or small decision tree model. The strong model becomes the sum of all the previously trained weak models.")]),e._v(" "),t("li",[e._v("at each iteration, a weak model is trained to predict the loss gradient of the strong model. Then, the strong model's output is updated by subtracting the predicted gradient, similar to gradient descent.")])])]),e._v(" "),t("li",[e._v("åŒï¼š ensembling methods, combine the outputs from individual trees.")]),e._v(" "),t("li",[e._v("å¼‚ï¼š\n"),t("ul",[t("li",[e._v("ä»classification ç»“æœæ¥è¯´ï¼š\n"),t("ul",[t("li",[e._v("comparing to RF, gradient boost have a lot more modeling capacity. They can model very complex relationships and decision boundaries.")]),e._v(" "),t("li",[e._v("gradient boost: low bias, high variance, can lead to overfitting")]),e._v(" "),t("li",[e._v("random forest: high bias, low variance, can cause underfitting.")])])]),e._v(" "),t("li",[e._v("ä»classification è¿‡ç¨‹æ¥è¯´ï¼š\n"),t("ul",[t("li",[t("RouterLink",{attrs:{to:"/pages/63f233/#bagging-å’Œ-boosting-çš„4-ç‚¹å·®åˆ«"}},[e._v("/pages/63f233/#bagging-å’Œ-boosting-çš„4-ç‚¹å·®åˆ«")])],1)])])])])]),e._v(" "),t("h2",{attrs:{id:"generative-v-s-discrimitive-model"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#generative-v-s-discrimitive-model"}},[e._v("#")]),e._v(" Generative v.s. Discrimitive Model")]),e._v(" "),t("p",[e._v("from "),t("a",{attrs:{href:"https://blog.csdn.net/Oh_MyBug/article/details/104343641",target:"_blank",rel:"noopener noreferrer"}},[e._v("csdn"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("strong",[e._v("generative mode")]),e._v("l:")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("a model that does either of the following:")]),e._v(" "),t("ul",[t("li",[e._v("Creates (generates) new sample from the training dataset. e.g., create poetry after training on a dataset of poems. (e.g., GAN)")]),e._v(" "),t("li",[e._v("Determines the probability that a new example comes from the training set, or was created from the same mechanism that created the training set. e.g., after training on a dataset consisting of English sentences, a generative model could determine the probability that new input is a valid English sentence.(e.g., Naive Bayes)")])])]),e._v(" "),t("li",[t("p",[e._v("A generative model can understand the distribution of examples or particular features in a dataset.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("discriminative model")]),e._v(": A model that predicts labels from a set of one or more features. The goal of a discriminative model is to understand the conditional probability of an output given the features and weights; that is: "),t("code",[e._v("p(output | features, weights)")])]),e._v(" "),t("ul",[t("li",[e._v("For example, a model that predicts whether an email is spam from features and weights")]),e._v(" "),t("li",[e._v("Contrast with generative model.")])])])]),e._v(" "),t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/generative-model-0.png",width:"100%"}}),e._v("\nsimple generative model includes:\n- naive bayes \n- LDA (Linear Discrimitative Analysis)\n"),t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/generative-model-1.png",width:"100%"}}),e._v(" "),t("h3",{attrs:{id:"generative-model-pros-and-cons"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#generative-model-pros-and-cons"}},[e._v("#")]),e._v(" generative model pros and cons")]),e._v(" "),t("h4",{attrs:{id:"pros"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#pros"}},[e._v("#")]),e._v(" pros")]),e._v(" "),t("ul",[t("li",[e._v("å®é™…ä¸Šå¸¦çš„ä¿¡æ¯æ¯”åˆ¤åˆ«æ¨¡å‹ä¸°å¯Œ")]),e._v(" "),t("li",[e._v("ç ”ç©¶å•ç±»é—®é¢˜æ¯”åˆ¤åˆ«æ¨¡å‹çµæ´»æ€§å¼º")]),e._v(" "),t("li",[e._v("èƒ½ç”¨äºæ•°æ®ä¸å®Œæ•´æƒ…å†µ, åŸºäºæ¦‚ç‡åˆ†å¸ƒçš„å‡è®¾ï¼Œæ‰€éœ€çš„training dataè¾ƒå°‘")]),e._v(" "),t("li",[e._v("å¾ˆå®¹æ˜“å°†å…ˆéªŒçŸ¥è¯†è€ƒè™‘è¿›å»")]),e._v(" "),t("li",[e._v("ç¨³å¥å‹å¥½ï¼Œå½“æ•°æ®å‘ˆç°ä¸åŒç‰¹ç‚¹æ—¶ï¼Œåˆ†ç±»æ€§èƒ½ä¸ä¼šå‡ºç°å¤ªå¤§çš„å·®å¼‚å¯¹noiseæ¯”è¾ƒrobust")])]),e._v(" "),t("h4",{attrs:{id:"cons"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#cons"}},[e._v("#")]),e._v(" cons")]),e._v(" "),t("ul",[t("li",[e._v("å®¹æ˜“äº§ç”Ÿé”™è¯¯åˆ†ç±»:Naive Bayesé‡Œé¢å‡è®¾æ¯ä¸ªäº‹ä»¶éƒ½æ˜¯independentçš„ï¼Œæ¯”å¦‚00|01|10 & 11çš„åˆ†ç±»ï¼Œæ ·æœ¬ä¸å‡çš„æ—¶å€™å¯èƒ½ä¼šåˆ†é”™ï¼Œå› ä¸ºmodelå¯èƒ½ä¼šè„‘è¡¥ä¸å­˜åœ¨çš„æƒ…å†µ")]),e._v(" "),t("li",[e._v("å­¦ä¹ å’Œè®¡ç®—è¿‡ç¨‹æ¯”è¾ƒå¤æ‚")])]),e._v(" "),t("h3",{attrs:{id:"discrimitive-model-pros-and-cons"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#discrimitive-model-pros-and-cons"}},[e._v("#")]),e._v(" discrimitive model pros and cons")]),e._v(" "),t("p",[e._v("pros")]),e._v(" "),t("ul",[t("li",[e._v("åˆ†ç±»è¾¹ç•Œæ›´çµæ´»ï¼Œæ¯”ä½¿ç”¨çº¯æ¦‚ç‡æ–¹æ³•æˆ–ç”Ÿæˆæ¨¡å‹å¾—åˆ°çš„æ›´é«˜çº§")]),e._v(" "),t("li",[e._v("èƒ½æ¸…æ™°çš„åˆ†è¾¨å‡ºå¤šç±»æˆ–æŸä¸€ç±»ä¸å…¶å®ƒç±»ä¹‹é—´çš„å·®å¼‚ç‰¹å¾")]),e._v(" "),t("li",[e._v("å¯¹äºå¤šfeatureçš„æƒ…å†µï¼Œfeatureä¹‹é—´å¤šæœ‰correlationï¼Œæ¯”èµ·naive bayesï¼Œmodels such as logistic regression is much more robust with correlated features.")]),e._v(" "),t("li",[e._v("åˆ¤åˆ«æ¨¡å‹çš„æ€§èƒ½æ¯”ç”Ÿæˆæ¨¡å‹è¦ç®€å•ï¼Œæ¯”è¾ƒå®¹æ˜“å­¦ä¹ \ncons")]),e._v(" "),t("li",[e._v("ä¸èƒ½ååº”è®­ç»ƒæ•°æ®æœ¬èº«çš„ç‰¹æ€§ï¼Œåªèƒ½å‘Šè¯‰ä½ çš„æ˜¯1è¿˜æ˜¯2ï¼Œä¸èƒ½æŠŠæ•´ä¸ªåœºæ™¯æè¿°å‡ºæ¥")])]),e._v(" "),t("h3",{attrs:{id:"å’Œdiscrimitiveæ¨¡å‹æ¯”èµ·æ¥-generative-æ›´å®¹æ˜“overfittingè¿˜æ˜¯underfitting"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#å’Œdiscrimitiveæ¨¡å‹æ¯”èµ·æ¥-generative-æ›´å®¹æ˜“overfittingè¿˜æ˜¯underfitting"}},[e._v("#")]),e._v(" å’ŒDiscrimitiveæ¨¡å‹æ¯”èµ·æ¥ï¼ŒGenerative æ›´å®¹æ˜“overfittingè¿˜æ˜¯underfitting")]),e._v(" "),t("p",[e._v("æ›´å®¹æ˜“underfittingã€‚\nthis "),t("a",{attrs:{href:"https://stats.stackexchange.com/questions/91484/do-discriminative-models-overfit-more-than-generative-models",target:"_blank",rel:"noopener noreferrer"}},[e._v("stack exchange"),t("OutboundLink")],1),e._v(" has some very math explanations."),t("br"),e._v("\næ¯”è¾ƒç®€å•çš„è§£é‡Šï¼š")]),e._v(" "),t("p",[e._v("A generative model is typically underfitting because it allows the user to put in more side information in the form of class conditionals.")]),e._v(" "),t("p",[e._v("Consider a generative model ğ‘(ğ‘|ğ‘¥)=ğ‘(ğ‘)ğ‘(ğ‘¥|ğ‘). If the class conditionals are mulitvariate normals with shared covariance, this will have a linear decision boundary. Thus, the model by itself is just as powerful as a linear SVM or logistic regression.")]),e._v(" "),t("p",[e._v("However, a discriminative classifier is much more free in the choice of decision function: it just has to find an appropriate hyperplane. The generative classifier however will need much less samples to find good parameters if the assumptions are valid.")]),e._v(" "),t("p",[e._v("Sorry, this is rather handwavy and there is no hard math behind it. But it is an intuition.")]),e._v(" "),t("h2",{attrs:{id:"gmm-v-s-k-means"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gmm-v-s-k-means"}},[e._v("#")]),e._v(" GMM v.s. K-Means")]),e._v(" "),t("p",[e._v("k-means:")]),e._v(" "),t("ul",[t("li",[e._v("A clustering algorithm that clusters samples by:\n"),t("ol",[t("li",[e._v("Assigns each sample to the closest center points.")]),e._v(" "),t("li",[e._v("Iteratively determines the best k center points.")])])]),e._v(" "),t("li",[e._v("So, k-means algorithm picks center points to minimize the cumulative square of the distances from each sample to its closest center points")])]),e._v(" "),t("p",[e._v("GMM:")]),e._v(" "),t("ul",[t("li",[e._v("A clustering algorithm that clusters samples by iteratively:\n"),t("ul",[t("li",[e._v("assign the probablilities each sample belong to each cluster, and")]),e._v(" "),t("li",[e._v("iteratively determine the best miu, and covariance big sigma for each of the clusters")])])]),e._v(" "),t("li",[e._v("keep iterating until it reaches convergence.")])]),e._v(" "),t("p",[e._v("ç›¸åŒç‚¹"),t("br"),e._v("\néƒ½æ˜¯è¿­ä»£æ‰§è¡Œçš„ç®—æ³•ï¼Œä¸”è¿­ä»£çš„ç­–ç•¥ä¹Ÿç›¸åŒï¼šç®—æ³•å¼€å§‹æ‰§è¡Œæ—¶å…ˆå¯¹éœ€è¦è®¡ç®—çš„å‚æ•°èµ‹åˆå€¼ï¼Œç„¶åäº¤æ›¿æ‰§è¡Œä¸¤ä¸ªæ­¥éª¤ï¼Œä¸€ä¸ªæ­¥éª¤æ˜¯å¯¹æ•°æ®çš„ä¼°è®¡ï¼ˆk-meansæ˜¯ä¼°è®¡æ¯ä¸ªç‚¹æ‰€å±ç°‡ï¼›GMMæ˜¯è®¡ç®—éšå«å˜é‡çš„æœŸæœ›ï¼›ï¼‰;ç¬¬äºŒæ­¥æ˜¯ç”¨ä¸Šä¸€æ­¥ç®—å‡ºçš„ä¼°è®¡å€¼é‡æ–°è®¡ç®—å‚æ•°å€¼ï¼Œæ›´æ–°ç›®æ ‡å‚æ•°ï¼ˆk-meansæ˜¯è®¡ç®—ç°‡å¿ƒä½ç½®ï¼›GMMæ˜¯è®¡ç®—å„ä¸ªé«˜æ–¯åˆ†å¸ƒçš„ä¸­å¿ƒä½ç½®å’Œåæ–¹å·®çŸ©é˜µï¼‰")]),e._v(" "),t("p",[e._v("ä¸åŒç‚¹"),t("br"),e._v("\n1ï¼‰éœ€è¦è®¡ç®—çš„å‚æ•°ä¸åŒï¼šk-meansæ˜¯ç°‡å¿ƒä½ç½®ï¼›GMMæ˜¯å„ä¸ªé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°"),t("br"),e._v("\n2ï¼‰è®¡ç®—ç›®æ ‡å‚æ•°çš„æ–¹æ³•ä¸åŒï¼šk-meansæ˜¯è®¡ç®—å½“å‰ç°‡ä¸­æ‰€æœ‰å…ƒç´ çš„ä½ç½®çš„å‡å€¼ï¼›GMMæ˜¯åŸºäºæ¦‚ç‡çš„ç®—æ³•ï¼Œæ˜¯é€šè¿‡è®¡ç®—ä¼¼ç„¶å‡½æ•°çš„æœ€å¤§å€¼å®ç°åˆ†å¸ƒå‚æ•°çš„æ±‚è§£çš„ã€‚")]),e._v(" "),t("h2",{attrs:{id:"gradient-boost-å’Œ-adaboost-åŒºåˆ«"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gradient-boost-å’Œ-adaboost-åŒºåˆ«"}},[e._v("#")]),e._v(" gradient boost å’Œ adaboost åŒºåˆ«")]),e._v(" "),t("ol",[t("li",[e._v("ç»„æˆéƒ¨åˆ†ï¼š\n"),t("ol",[t("li",[e._v("adaboost starts by building a very short tree, called a "),t("strong",[e._v("stump")]),e._v(", from the training data.")]),e._v(" "),t("li",[e._v("gradient boost starts by making a small leaf, instead of a tree or stump, this leaf represents an initial guess for the weights of all of the samples. ç„¶ågradient boost ä¼šå»ºæ ‘ï¼Œä½†æ˜¯ä¼šæ¯”ä¸€ä¸ªstumpå¤§")])])]),e._v(" "),t("li",[e._v("prediction target:\n"),t("ol",[t("li",[e._v("adaboost predicts the final value")]),e._v(" "),t("li",[e._v("gradient boost predicts the residuals")])])])])])}),[],!1,null,null,null);t.default=s.exports}}]);