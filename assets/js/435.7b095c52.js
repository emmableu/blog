(window.webpackJsonp=window.webpackJsonp||[]).push([[435],{805:function(e,t,i){"use strict";i.r(t);var a=i(9),s=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"random-forest-v-s-gradient-boost"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#random-forest-v-s-gradient-boost"}},[e._v("#")]),e._v(" "),t("RouterLink",{attrs:{to:"/pages/63f233/#随机森林算法-random-forest"}},[e._v("random forest")]),e._v(" v.s. gradient boost")],1),e._v(" "),t("ul",[t("li",[t("RouterLink",{attrs:{to:"/pages/63f233/#随机森林算法-random-forest"}},[e._v("random forest")]),e._v(": the process to train an ensemble where, each constituent model (a decision tree) trains on a random subset of training examples, sampled with replacement.")],1),e._v(" "),t("li",[e._v("gradient boosting:\n"),t("ul",[t("li",[e._v("A training algorithm where weak models are trained to iteratively improve the quality (reduce the loss) of a strong model. For example, a weak model could be a linear or small decision tree model. The strong model becomes the sum of all the previously trained weak models.")]),e._v(" "),t("li",[e._v("at each iteration, a weak model is trained to predict the loss gradient of the strong model. Then, the strong model's output is updated by subtracting the predicted gradient, similar to gradient descent.")])])]),e._v(" "),t("li",[e._v("同： ensembling methods, combine the outputs from individual trees.")]),e._v(" "),t("li",[e._v("异：\n"),t("ul",[t("li",[e._v("从classification 结果来说：\n"),t("ul",[t("li",[e._v("comparing to RF, gradient boost have a lot more modeling capacity. They can model very complex relationships and decision boundaries.")]),e._v(" "),t("li",[e._v("gradient boost: low bias, high variance, can lead to overfitting")]),e._v(" "),t("li",[e._v("random forest: high bias, low variance, can cause underfitting.")])])]),e._v(" "),t("li",[e._v("从classification 过程来说：\n"),t("ul",[t("li",[t("RouterLink",{attrs:{to:"/pages/63f233/#bagging-和-boosting-的4-点差别"}},[e._v("/pages/63f233/#bagging-和-boosting-的4-点差别")])],1)])])])])]),e._v(" "),t("h2",{attrs:{id:"generative-v-s-discrimitive-model"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#generative-v-s-discrimitive-model"}},[e._v("#")]),e._v(" Generative v.s. Discrimitive Model")]),e._v(" "),t("p",[e._v("from "),t("a",{attrs:{href:"https://blog.csdn.net/Oh_MyBug/article/details/104343641",target:"_blank",rel:"noopener noreferrer"}},[e._v("csdn"),t("OutboundLink")],1)]),e._v(" "),t("p",[t("strong",[e._v("generative mode")]),e._v("l:")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("a model that does either of the following:")]),e._v(" "),t("ul",[t("li",[e._v("Creates (generates) new sample from the training dataset. e.g., create poetry after training on a dataset of poems. (e.g., GAN)")]),e._v(" "),t("li",[e._v("Determines the probability that a new example comes from the training set, or was created from the same mechanism that created the training set. e.g., after training on a dataset consisting of English sentences, a generative model could determine the probability that new input is a valid English sentence.(e.g., Naive Bayes)")])])]),e._v(" "),t("li",[t("p",[e._v("A generative model can understand the distribution of examples or particular features in a dataset.")])]),e._v(" "),t("li",[t("p",[t("strong",[e._v("discriminative model")]),e._v(": A model that predicts labels from a set of one or more features. The goal of a discriminative model is to understand the conditional probability of an output given the features and weights; that is: "),t("code",[e._v("p(output | features, weights)")])]),e._v(" "),t("ul",[t("li",[e._v("For example, a model that predicts whether an email is spam from features and weights")]),e._v(" "),t("li",[e._v("Contrast with generative model.")])])])]),e._v(" "),t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/generative-model-0.png",width:"100%"}}),e._v("\nsimple generative model includes:\n- naive bayes \n- LDA (Linear Discrimitative Analysis)\n"),t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/generative-model-1.png",width:"100%"}}),e._v(" "),t("h3",{attrs:{id:"generative-model-pros-and-cons"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#generative-model-pros-and-cons"}},[e._v("#")]),e._v(" generative model pros and cons")]),e._v(" "),t("h4",{attrs:{id:"pros"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#pros"}},[e._v("#")]),e._v(" pros")]),e._v(" "),t("ul",[t("li",[e._v("实际上带的信息比判别模型丰富")]),e._v(" "),t("li",[e._v("研究单类问题比判别模型灵活性强")]),e._v(" "),t("li",[e._v("能用于数据不完整情况, 基于概率分布的假设，所需的training data较少")]),e._v(" "),t("li",[e._v("很容易将先验知识考虑进去")]),e._v(" "),t("li",[e._v("稳健型好，当数据呈现不同特点时，分类性能不会出现太大的差异对noise比较robust")])]),e._v(" "),t("h4",{attrs:{id:"cons"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#cons"}},[e._v("#")]),e._v(" cons")]),e._v(" "),t("ul",[t("li",[e._v("容易产生错误分类:Naive Bayes里面假设每个事件都是independent的，比如00|01|10 & 11的分类，样本不均的时候可能会分错，因为model可能会脑补不存在的情况")]),e._v(" "),t("li",[e._v("学习和计算过程比较复杂")])]),e._v(" "),t("h3",{attrs:{id:"discrimitive-model-pros-and-cons"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#discrimitive-model-pros-and-cons"}},[e._v("#")]),e._v(" discrimitive model pros and cons")]),e._v(" "),t("p",[e._v("pros")]),e._v(" "),t("ul",[t("li",[e._v("分类边界更灵活，比使用纯概率方法或生成模型得到的更高级")]),e._v(" "),t("li",[e._v("能清晰的分辨出多类或某一类与其它类之间的差异特征")]),e._v(" "),t("li",[e._v("对于多feature的情况，feature之间多有correlation，比起naive bayes，models such as logistic regression is much more robust with correlated features.")]),e._v(" "),t("li",[e._v("判别模型的性能比生成模型要简单，比较容易学习\ncons")]),e._v(" "),t("li",[e._v("不能反应训练数据本身的特性，只能告诉你的是1还是2，不能把整个场景描述出来")])]),e._v(" "),t("h3",{attrs:{id:"和discrimitive模型比起来-generative-更容易overfitting还是underfitting"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#和discrimitive模型比起来-generative-更容易overfitting还是underfitting"}},[e._v("#")]),e._v(" 和Discrimitive模型比起来，Generative 更容易overfitting还是underfitting")]),e._v(" "),t("p",[e._v("更容易underfitting。\nthis "),t("a",{attrs:{href:"https://stats.stackexchange.com/questions/91484/do-discriminative-models-overfit-more-than-generative-models",target:"_blank",rel:"noopener noreferrer"}},[e._v("stack exchange"),t("OutboundLink")],1),e._v(" has some very math explanations."),t("br"),e._v("\n比较简单的解释：")]),e._v(" "),t("p",[e._v("A generative model is typically underfitting because it allows the user to put in more side information in the form of class conditionals.")]),e._v(" "),t("p",[e._v("Consider a generative model 𝑝(𝑐|𝑥)=𝑝(𝑐)𝑝(𝑥|𝑐). If the class conditionals are mulitvariate normals with shared covariance, this will have a linear decision boundary. Thus, the model by itself is just as powerful as a linear SVM or logistic regression.")]),e._v(" "),t("p",[e._v("However, a discriminative classifier is much more free in the choice of decision function: it just has to find an appropriate hyperplane. The generative classifier however will need much less samples to find good parameters if the assumptions are valid.")]),e._v(" "),t("p",[e._v("Sorry, this is rather handwavy and there is no hard math behind it. But it is an intuition.")]),e._v(" "),t("h2",{attrs:{id:"gmm-v-s-k-means"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gmm-v-s-k-means"}},[e._v("#")]),e._v(" GMM v.s. K-Means")]),e._v(" "),t("p",[e._v("k-means:")]),e._v(" "),t("ul",[t("li",[e._v("A clustering algorithm that clusters samples by:\n"),t("ol",[t("li",[e._v("Assigns each sample to the closest center points.")]),e._v(" "),t("li",[e._v("Iteratively determines the best k center points.")])])]),e._v(" "),t("li",[e._v("So, k-means algorithm picks center points to minimize the cumulative square of the distances from each sample to its closest center points")])]),e._v(" "),t("p",[e._v("GMM:")]),e._v(" "),t("ul",[t("li",[e._v("A clustering algorithm that clusters samples by iteratively:\n"),t("ul",[t("li",[e._v("assign the probablilities each sample belong to each cluster, and")]),e._v(" "),t("li",[e._v("iteratively determine the best miu, and covariance big sigma for each of the clusters")])])]),e._v(" "),t("li",[e._v("keep iterating until it reaches convergence.")])]),e._v(" "),t("p",[e._v("相同点"),t("br"),e._v("\n都是迭代执行的算法，且迭代的策略也相同：算法开始执行时先对需要计算的参数赋初值，然后交替执行两个步骤，一个步骤是对数据的估计（k-means是估计每个点所属簇；GMM是计算隐含变量的期望；）;第二步是用上一步算出的估计值重新计算参数值，更新目标参数（k-means是计算簇心位置；GMM是计算各个高斯分布的中心位置和协方差矩阵）")]),e._v(" "),t("p",[e._v("不同点"),t("br"),e._v("\n1）需要计算的参数不同：k-means是簇心位置；GMM是各个高斯分布的参数"),t("br"),e._v("\n2）计算目标参数的方法不同：k-means是计算当前簇中所有元素的位置的均值；GMM是基于概率的算法，是通过计算似然函数的最大值实现分布参数的求解的。")]),e._v(" "),t("h2",{attrs:{id:"gradient-boost-和-adaboost-区别"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gradient-boost-和-adaboost-区别"}},[e._v("#")]),e._v(" gradient boost 和 adaboost 区别")]),e._v(" "),t("ol",[t("li",[e._v("组成部分：\n"),t("ol",[t("li",[e._v("adaboost starts by building a very short tree, called a "),t("strong",[e._v("stump")]),e._v(", from the training data.")]),e._v(" "),t("li",[e._v("gradient boost starts by making a small leaf, instead of a tree or stump, this leaf represents an initial guess for the weights of all of the samples. 然后gradient boost 会建树，但是会比一个stump大")])])]),e._v(" "),t("li",[e._v("prediction target:\n"),t("ol",[t("li",[e._v("adaboost predicts the final value")]),e._v(" "),t("li",[e._v("gradient boost predicts the residuals")])])])])])}),[],!1,null,null,null);t.default=s.exports}}]);