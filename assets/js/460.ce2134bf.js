(window.webpackJsonp=window.webpackJsonp||[]).push([[460],{831:function(t,a,e){"use strict";e.r(a);var i=e(9),n=Object(i.a)({},(function(){var t=this._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":this.$parent.slotKey}},[t("p",[this._v("Batch Normalization Normalizing the input or output of the activation functions in a hidden layer. Batch normalization can provide benefits such as:\n- Reduce overfitting.\n- Make neural networks more stable by protecting against outlier weights.\n- Enable higher learning rates.")]),this._v(" "),t("ul",[t("li",[this._v("对于传统的神经网络，对输入做feature scaling也很重要，因为采用sigmoid等有饱和区的激活函数，如果输入分布范围很广，参数初始化时没有适配好，很容易直接陷入饱和区，导致梯度消失，所以，需要对输入做Standardization或映射[0,1]、[-1,1]，配合精心设计的参数初始化方法，对值域进行控制。但自从有了Batch Normalization，每次线性变换改变特征分布后，都会重新进行Normalization，似乎可以不太需要对网络的输入进行feature scaling了？但习惯上还是会做feature scaling。")])]),this._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220029918.png",alt:""}})]),this._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220030828.png",alt:""}})]),this._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220030346.png",alt:""}}),this._v(" "),t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220033400.png",alt:""}})])])}),[],!1,null,null,null);a.default=n.exports}}]);