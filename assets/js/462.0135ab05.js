(window.webpackJsonp=window.webpackJsonp||[]).push([[462],{833:function(t,s,a){"use strict";a.r(s);var n=a(9),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h3",{attrs:{id:"definition"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#definition"}},[t._v("#")]),t._v(" Definition:")]),t._v(" "),s("p",[t._v("A neural network in which at least one layer is a convolutional layer. A typical convolutional neural network consists of some combination of the following layers:\n- convolutional layers\n- pooling layers\n- dense layers\n- Convolutional neural networks have had great success in certain kinds of problems, such as image recognition.")]),t._v(" "),s("h2",{attrs:{id:"goal-of-convolutional-neural-networks"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#goal-of-convolutional-neural-networks"}},[t._v("#")]),t._v(" Goal of Convolutional Neural Networks:")]),t._v(" "),s("ol",[s("li",[t._v("Reduce the number of input nodes (图片有很多个pixel，所以直接变成一个数组会很大，要想办法把它变小)；")]),t._v(" "),s("li",[t._v("Tolerate small shifts in where the pixels are in the image (图片【0010】 和 图片 【0100】通常是表示一样的东西）")]),t._v(" "),s("li",[t._v("Take advantage of the correlation we observe in complex images: （比如一个熊的照片，棕色点的旁边往往也是棕色的点）")])]),t._v(" "),s("h2",{attrs:{id:"解释卷积操作中的稀疏交互和参数共享及其作用。-223-★★☆☆☆"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#解释卷积操作中的稀疏交互和参数共享及其作用。-223-★★☆☆☆"}},[t._v("#")]),t._v(" 解释卷积操作中的稀疏交互和参数共享及其作用。 223 ★★☆☆☆")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220045822.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220046942.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220046455.png",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"卷积神经网络-cnn-如何用于文本分类任务-227-★★★☆☆"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#卷积神经网络-cnn-如何用于文本分类任务-227-★★★☆☆"}},[t._v("#")]),t._v(" 卷积神经网络 CNN 如何用于文本分类任务？ 227 ★★★☆☆")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220049987.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220050458.png",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"常用的池化-pooling-操作有哪些-池化的作用是什么-225-★★★☆☆"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#常用的池化-pooling-操作有哪些-池化的作用是什么-225-★★★☆☆"}},[t._v("#")]),t._v(" 常用的池化 pooling 操作有哪些？池化的作用是什么？ 225 ★★★☆☆")]),t._v(" "),s("p",[t._v("平移（translation）、旋转（rotation）、缩放（scaling）")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220048800.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220049614.png",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"procedure-of-cnn"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#procedure-of-cnn"}},[t._v("#")]),t._v(" Procedure of CNN")]),t._v(" "),s("p",[t._v("以 区分 【0】和【x】为例：")]),t._v(" "),s("h3",{attrs:{id:"_1-filter-nn-conv2d"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-filter-nn-conv2d"}},[t._v("#")]),t._v(" 1. Filter ("),s("code",[t._v("nn.Conv2d")]),t._v(")")]),t._v(" "),s("p",[t._v("计算每一部分的图像和一个filter （aka kernel）的点乘的积, 并加一个bias， 得到一个feature map\n"),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202204181257092.png",alt:""}}),t._v("\n**怎样得到filter的matrix:")]),t._v(" "),s("ul",[s("li",[t._v("filter的matrix最终是由"),s("strong",[t._v("backpropagation")]),t._v("得到的，也就是说，start with random pixel values, then after training with "),s("strong",[t._v("Backpropagation")]),t._v(", we end up with something more useful.")])]),t._v(" "),s("p",[t._v("Because each cell in the feature map corresponds to a group of neighboring pixels, the feature map takes advantage of the correlations between neighboring pixels in an image.")]),t._v(" "),s("p",[t._v("这里的卷积层对应的参数是：")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2d"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                in_channels"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 图片本身有几层，rgb图片有3层，本题是黑白图片，所以只有1层")]),t._v("\n                out_channels"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# n_filters，输出高度，也就是filter的个数，")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 本题只有一个filter，但是可以有多个filter，")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 不同的filter最后通过back propagation 计算出来的 matrix也不一样。")]),t._v("\n                kernel_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("      "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# filter 的边长，3表示3个像素点")]),t._v("\n                stride"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("           "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# filter movement/step")]),t._v("\n                padding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("      "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# filter往后扫的时候可能会多出来一截，")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# padding就是在图片的上，下，左，右，包一圈0，这圈0的宽度就是padding")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果想要 con2d 出来的图片长宽没有变化, 如果 stride = 1，")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 则 padding=(kernel_size-1)/2， 也就是说，这里的长宽已经变小了，但是如果")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 希望它不变小（保持input的边长6），就要把它在外面都包一圈宽度为1的0，")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 把原图片变成一个 8*8 的图片。")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br"),s("span",{staticClass:"line-number"},[t._v("14")]),s("br")])]),s("h3",{attrs:{id:"_2-relu-acrivation-function"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-relu-acrivation-function"}},[t._v("#")]),t._v(" 2. ReLU Acrivation Function")]),t._v(" "),s("p",[t._v("把feature map上所有<0的点都变成 0\n"),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202204181300836.png",alt:""}}),t._v("\n用代码表示就是"),s("code",[t._v("nn.ReLU()")])]),t._v(" "),s("h3",{attrs:{id:"_3-maxpooling"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-maxpooling"}},[t._v("#")]),t._v(" 3. MaxPooling:")]),t._v(" "),s("p",[t._v("calculate the maximum value of each region on the Feature Map, Post ReLU\n和filter 长得最接近的region的值最后会被max pooling给保存下来\n"),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202204181303865.png",alt:""}}),t._v("\n代码："),s("code",[t._v("nn.MaxPool2d(kernel_size=2)")]),t._v(", 因为这个max polling的kernel 边长是2")]),t._v(" "),s("h3",{attrs:{id:"_4-use-the-results-from-max-pooling-as-the-input-nodes-to-a-neural-network"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-use-the-results-from-max-pooling-as-the-input-nodes-to-a-neural-network"}},[t._v("#")]),t._v(" 4. use the results from Max Pooling as the input nodes to a Neural Network")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202204181304906.png",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"pytorch-code-for-cnn-mnist-dataset"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pytorch-code-for-cnn-mnist-dataset"}},[t._v("#")]),t._v(" PyTorch Code For CNN - MNIST DataSet")]),t._v(" "),s("h3",{attrs:{id:"import-mnist-手写数据"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#import-mnist-手写数据"}},[t._v("#")]),t._v(" import MNIST 手写数据")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" nn\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" Data\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torchvision      "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据库模块")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" matplotlib"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pyplot "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" plt\n\ntorch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("manual_seed"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# reproducible")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Hyper Parameters")]),t._v("\nEPOCH "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("           "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 训练整批数据多少次, 为了节约时间, 我们只训练一次")]),t._v("\nBATCH_SIZE "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),t._v("\nLR "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.001")]),t._v("          "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 学习率")]),t._v("\nDOWNLOAD_MNIST "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果你已经下载好了mnist数据就写上 False")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Mnist 手写数字")]),t._v("\ntrain_data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torchvision"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MNIST"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    root"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./mnist/'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 保存或者提取位置")]),t._v("\n    train"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# this is training data")]),t._v("\n    transform"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torchvision"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transforms"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ToTensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 转换 PIL.Image or numpy.ndarray 成")]),t._v("\n                                                    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# torch.FloatTensor (C x H x W), 训练的时候 normalize 成 [0.0, 1.0] 区间")]),t._v("\n    download"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("DOWNLOAD_MNIST"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("          "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 没下载就下载, 下载了就不用再下了")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br"),s("span",{staticClass:"line-number"},[t._v("14")]),s("br"),s("span",{staticClass:"line-number"},[t._v("15")]),s("br"),s("span",{staticClass:"line-number"},[t._v("16")]),s("br"),s("span",{staticClass:"line-number"},[t._v("17")]),s("br"),s("span",{staticClass:"line-number"},[t._v("18")]),s("br"),s("span",{staticClass:"line-number"},[t._v("19")]),s("br"),s("span",{staticClass:"line-number"},[t._v("20")]),s("br"),s("span",{staticClass:"line-number"},[t._v("21")]),s("br"),s("span",{staticClass:"line-number"},[t._v("22")]),s("br"),s("span",{staticClass:"line-number"},[t._v("23")]),s("br")])]),s("p",[t._v("同样, 我们除了训练数据, 还给一些测试数据, 测试看看它有没有训练好.")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("test_data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torchvision"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("datasets"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MNIST"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./mnist/'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)")]),t._v("\ntrain_loader "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataLoader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dataset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("BATCH_SIZE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shuffle"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 为了节约时间, 我们测试时只测试前2000个")]),t._v("\ntest_x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unsqueeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("FloatTensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("255.")]),t._v("   \n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 为什么要/255: 压缩到0-1的区间")]),t._v("\ntest_y "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" test_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br")])]),s("h3",{attrs:{id:"cnn-模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#cnn-模型"}},[t._v("#")]),t._v(" CNN 模型")]),t._v(" "),s("p",[t._v("和以前一样, 我们用一个 class 来建立 CNN 模型. 这个 CNN 整体流程是 卷积("),s("code",[t._v("Conv2d")]),t._v(") -> 激励函数("),s("code",[t._v("ReLU")]),t._v(") -> 池化, 向下采样 ("),s("code",[t._v("MaxPooling")]),t._v(") -> 再来一遍 -> 展平多维的卷积成的特征图 -> 接入全连接层 ("),s("code",[t._v("Linear")]),t._v(") -> 输出")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("CNN")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("CNN"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# input shape (1, 28, 28), 1 代表一个data channel，")]),t._v("\n\t\t\t\t\t\t\t\t    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 因为图片只有1层 （黑白），所以是1个data channel")]),t._v("\n            nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2d"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                in_channels"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("      "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# input height")]),t._v("\n                out_channels"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# n_filters")]),t._v("\n                kernel_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("      "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# filter size")]),t._v("\n                stride"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("           "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# filter movement/step")]),t._v("\n                padding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("      "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("      "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# output shape (16, 28, 28)")]),t._v("\n            nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ReLU"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# activation")]),t._v("\n            nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MaxPool2d"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("kernel_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在 2x2 空间里向下采样, output shape (16, 14, 14)")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# input shape (16, 14, 14)")]),t._v("\n            nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2d"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# output shape (32, 14, 14)")]),t._v("\n            nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ReLU"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# activation")]),t._v("\n            nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MaxPool2d"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# output shape (32, 7, 7)")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("out "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# fully connected layer, output 10 classes")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (batch, 32, 7, 7)")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 展平多维的卷积图成 (batch_size, 32 * 7 * 7)")]),t._v("\n        output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" output\n\ncnn "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" CNN"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cnn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# net architecture")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\nCNN (\n  (conv1): Sequential (\n    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU ()\n    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  )\n  (conv2): Sequential (\n    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU ()\n    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  )\n  (out): Linear (1568 -> 10)\n)\n"""')]),t._v("\n\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br"),s("span",{staticClass:"line-number"},[t._v("14")]),s("br"),s("span",{staticClass:"line-number"},[t._v("15")]),s("br"),s("span",{staticClass:"line-number"},[t._v("16")]),s("br"),s("span",{staticClass:"line-number"},[t._v("17")]),s("br"),s("span",{staticClass:"line-number"},[t._v("18")]),s("br"),s("span",{staticClass:"line-number"},[t._v("19")]),s("br"),s("span",{staticClass:"line-number"},[t._v("20")]),s("br"),s("span",{staticClass:"line-number"},[t._v("21")]),s("br"),s("span",{staticClass:"line-number"},[t._v("22")]),s("br"),s("span",{staticClass:"line-number"},[t._v("23")]),s("br"),s("span",{staticClass:"line-number"},[t._v("24")]),s("br"),s("span",{staticClass:"line-number"},[t._v("25")]),s("br"),s("span",{staticClass:"line-number"},[t._v("26")]),s("br"),s("span",{staticClass:"line-number"},[t._v("27")]),s("br"),s("span",{staticClass:"line-number"},[t._v("28")]),s("br"),s("span",{staticClass:"line-number"},[t._v("29")]),s("br"),s("span",{staticClass:"line-number"},[t._v("30")]),s("br"),s("span",{staticClass:"line-number"},[t._v("31")]),s("br"),s("span",{staticClass:"line-number"},[t._v("32")]),s("br"),s("span",{staticClass:"line-number"},[t._v("33")]),s("br"),s("span",{staticClass:"line-number"},[t._v("34")]),s("br"),s("span",{staticClass:"line-number"},[t._v("35")]),s("br"),s("span",{staticClass:"line-number"},[t._v("36")]),s("br"),s("span",{staticClass:"line-number"},[t._v("37")]),s("br"),s("span",{staticClass:"line-number"},[t._v("38")]),s("br"),s("span",{staticClass:"line-number"},[t._v("39")]),s("br"),s("span",{staticClass:"line-number"},[t._v("40")]),s("br"),s("span",{staticClass:"line-number"},[t._v("41")]),s("br"),s("span",{staticClass:"line-number"},[t._v("42")]),s("br"),s("span",{staticClass:"line-number"},[t._v("43")]),s("br"),s("span",{staticClass:"line-number"},[t._v("44")]),s("br"),s("span",{staticClass:"line-number"},[t._v("45")]),s("br"),s("span",{staticClass:"line-number"},[t._v("46")]),s("br"),s("span",{staticClass:"line-number"},[t._v("47")]),s("br")])]),s("h3",{attrs:{id:"训练"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#训练"}},[t._v("#")]),t._v(" 训练")]),t._v(" "),s("p",[t._v("下面我们开始训练, 将 "),s("code",[t._v("x")]),t._v(" "),s("code",[t._v("y")]),t._v(" 都用 "),s("code",[t._v("Variable")]),t._v(" 包起来, 然后放入 "),s("code",[t._v("cnn")]),t._v(" 中计算 "),s("code",[t._v("output")]),t._v(", 最后再计算误差. 下面代码省略了计算精确度 "),s("code",[t._v("accuracy")]),t._v(" 的部分, 细看 "),s("code",[t._v("accuracy")]),t._v(" 见"),s("a",{attrs:{href:"https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/401_CNN.py",target:"_blank",rel:"noopener noreferrer"}},[t._v("github"),s("OutboundLink")],1),t._v(" 看全部代码.")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("optimizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cnn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lr"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("LR"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# optimize all cnn parameters")]),t._v("\nloss_func "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CrossEntropyLoss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# the target label is not one-hotted")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# training and testing")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" epoch "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("EPOCH"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" step"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("b_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_loader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 分配 batch data, normalize x when iterate train_loader")]),t._v("\n        output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cnn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("b_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("               "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# cnn output")]),t._v("\n        loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" loss_func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# cross entropy loss")]),t._v("\n        optimizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zero_grad"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("           "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# clear gradients for this training step")]),t._v("\n        loss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("                 "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# backpropagation, compute gradients")]),t._v("\n        optimizer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# apply gradients")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n...\nEpoch:  0 | train loss: 0.0306 | test accuracy: 0.97\nEpoch:  0 | train loss: 0.0147 | test accuracy: 0.98\nEpoch:  0 | train loss: 0.0427 | test accuracy: 0.98\nEpoch:  0 | train loss: 0.0078 | test accuracy: 0.98\n"""')]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br"),s("span",{staticClass:"line-number"},[t._v("14")]),s("br"),s("span",{staticClass:"line-number"},[t._v("15")]),s("br"),s("span",{staticClass:"line-number"},[t._v("16")]),s("br"),s("span",{staticClass:"line-number"},[t._v("17")]),s("br"),s("span",{staticClass:"line-number"},[t._v("18")]),s("br"),s("span",{staticClass:"line-number"},[t._v("19")]),s("br")])]),s("p",[t._v("最后我们再来取10个数据, 看看预测的值到底对不对:")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("test_output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cnn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npred_y "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numpy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("squeeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'prediction number'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numpy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'real number'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n[7 2 1 0 4 1 4 9 5 9] prediction number\n[7 2 1 0 4 1 4 9 5 9] real number\n"""')]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br")])])])}),[],!1,null,null,null);s.default=e.exports}}]);