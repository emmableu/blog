(window.webpackJsonp=window.webpackJsonp||[]).push([[439],{810:function(t,s,e){"use strict";e.r(s);var i=e(9),r=Object(i.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[s("a",{attrs:{href:"https://web.stanford.edu/~jurafsky/slp3/5.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("mostly from stanford notes"),s("OutboundLink")],1)]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209202349541.png",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"什么是广义线性模型-generalized-linear-model-glm"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#什么是广义线性模型-generalized-linear-model-glm"}},[t._v("#")]),t._v(" 什么是广义线性模型 generalized linear model GLM")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209140008911.png",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"components-of-a-probabilistic-machine-learning-classifier"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#components-of-a-probabilistic-machine-learning-classifier"}},[t._v("#")]),t._v(" Components of a probabilistic machine learning classifier")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-0.png",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"sigmoid-function-in-logistic-regression"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#sigmoid-function-in-logistic-regression"}},[t._v("#")]),t._v(" Sigmoid function in logistic regression")]),t._v(" "),s("p",[t._v("this type of function is also called classification function or link function.\n"),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-1.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-2.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-3.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-4.png",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"the-cross-entropy-loss-function"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#the-cross-entropy-loss-function"}},[t._v("#")]),t._v(" The cross-entropy loss function")]),t._v(" "),s("p",[s("strong",[t._v("bernouli distribution")]),t._v(":"),s("br"),t._v("\nThe Bernoulli distribution is a special case of the binomial distribution where a single trial is conducted (so n would be 1 for such a binomial distribution).\n"),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-6.png",alt:""}})]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-5.png",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"gradient-descent"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#gradient-descent"}},[t._v("#")]),t._v(" Gradient Descent")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-7.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-8.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-9.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-10.png",alt:""}})]),t._v(" "),s("h3",{attrs:{id:"the-gradient-for-logistic-regression"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#the-gradient-for-logistic-regression"}},[t._v("#")]),t._v(" The Gradient for Logistic Regression")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-11.png",alt:""}})]),t._v(" "),s("h3",{attrs:{id:"the-stochastic-gradient-descent-algorithm"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#the-stochastic-gradient-descent-algorithm"}},[t._v("#")]),t._v(" The Stochastic Gradient Descent Algorithm")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-12.png",alt:""}})]),t._v(" "),s("h3",{attrs:{id:"working-through-an-example"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#working-through-an-example"}},[t._v("#")]),t._v(" Working through an Example")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-13.png",alt:""}})]),t._v(" "),s("h3",{attrs:{id:"mini-batch-training"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#mini-batch-training"}},[t._v("#")]),t._v(" Mini-batch training")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-14.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-15.png",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"regularization"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#regularization"}},[t._v("#")]),t._v(" Regularization")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-16.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-17.png",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"multinomial-logistic-regression"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#multinomial-logistic-regression"}},[t._v("#")]),t._v(" Multinomial Logistic Regression")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-18.png",alt:""}}),t._v(" "),s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-19.png",alt:""}})]),t._v(" "),s("h3",{attrs:{id:"features-in-multinomial-logistic-regression"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#features-in-multinomial-logistic-regression"}},[t._v("#")]),t._v(" Features in Multinomial Logistic Regression")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-20.png",alt:""}})]),t._v(" "),s("h3",{attrs:{id:"learning-in-multinomial-logistic-regression"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#learning-in-multinomial-logistic-regression"}},[t._v("#")]),t._v(" Learning in Multinomial Logistic Regression")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-21.png",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"in-logistic-regression-why-we-don-t-use-mse-as-the-loss-function"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#in-logistic-regression-why-we-don-t-use-mse-as-the-loss-function"}},[t._v("#")]),t._v(" in logistic regression, why we don't use MSE as the loss function?")]),t._v(" "),s("p",[t._v("用mse容易出现多个局部最优解（即非凸函数）")])])}),[],!1,null,null,null);s.default=r.exports}}]);