(window.webpackJsonp=window.webpackJsonp||[]).push([[474],{846:function(t,s,a){"use strict";a.r(s);var n=a(9),r=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"📊-为什么神经网络需要对输入特征进行标准化-scaling"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#📊-为什么神经网络需要对输入特征进行标准化-scaling"}},[t._v("#")]),t._v(" 📊 为什么神经网络需要对输入特征进行标准化（Scaling）")]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"🎯-学习目标"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#🎯-学习目标"}},[t._v("#")]),t._v(" 🎯 学习目标")]),t._v(" "),s("ol",[s("li",[t._v("理解神经网络对输入特征进行 scaling 的必要性")]),t._v(" "),s("li",[t._v("掌握标准化（Standardization）和归一化（Min-Max Normalization）的原理与应用区别")]),t._v(" "),s("li",[t._v("通过 PyTorch 实现标准化流程")])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"🤔-为什么需要-scaling"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#🤔-为什么需要-scaling"}},[t._v("#")]),t._v(" 🤔 为什么需要 Scaling？")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("编号")]),t._v(" "),s("th",[t._v("原因")]),t._v(" "),s("th",[t._v("不做 Scaling 的后果")]),t._v(" "),s("th",[t._v("做 Scaling 后的好处")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("①")]),t._v(" "),s("td",[t._v("📉 提升训练速度")]),t._v(" "),s("td",[t._v("梯度更新方向不一致，收敛慢")]),t._v(" "),s("td",[t._v("梯度统一，加速收敛")])]),t._v(" "),s("tr",[s("td",[t._v("②")]),t._v(" "),s("td",[t._v("📐 避免特征偏向")]),t._v(" "),s("td",[t._v("大值特征“统治”网络学习方向")]),t._v(" "),s("td",[t._v("各特征均衡参与建模")])]),t._v(" "),s("tr",[s("td",[t._v("③")]),t._v(" "),s("td",[t._v("🧠 提升泛化能力")]),t._v(" "),s("td",[t._v("模型依赖某些大值特征，验证集表现差")]),t._v(" "),s("td",[t._v("模型更稳健，更容易泛化")])])])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"✅-1-提升训练速度-梯度更平衡"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#✅-1-提升训练速度-梯度更平衡"}},[t._v("#")]),t._v(" ✅ ① 提升训练速度（梯度更平衡）")]),t._v(" "),s("h3",{attrs:{id:"举例"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#举例"}},[t._v("#")]),t._v(" 举例")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("Proline")]),t._v(": 平均 746，最大可达 1680")]),t._v(" "),s("li",[s("code",[t._v("Hue")]),t._v(": 平均 0.96，仅在 0~2 之间")])]),t._v(" "),s("p",[t._v("训练样本：")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("X "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1680")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 不标准化前")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br")])]),s("ul",[s("li",[t._v("Proline 对梯度的影响远大于 Hue")]),t._v(" "),s("li",[t._v("优化器沿 Proline 方向走得远，Hue 几乎无效")])]),t._v(" "),s("p",[s("strong",[t._v("解决")]),t._v("：")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("X_scaled "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" std\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br")])]),s("p",[t._v("将所有特征变为 0 均值、1 方差左右，更新轨迹更流畅。")]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"✅-2-避免模型偏向大数值特征"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#✅-2-避免模型偏向大数值特征"}},[t._v("#")]),t._v(" ✅ ② 避免模型偏向大数值特征")]),t._v(" "),s("h3",{attrs:{id:"举例-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#举例-2"}},[t._v("#")]),t._v(" 举例")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" w1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" x1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" w2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" x2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" b\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br")])]),s("p",[t._v("如果：")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("x1 = Hue ≈ 1")])]),t._v(" "),s("li",[s("code",[t._v("x2 = Proline ≈ 1000")])])]),t._v(" "),s("p",[t._v("为了输出平衡：")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("w1 ≈ 1000 * w2")])]),t._v(" "),s("li",[t._v("会让 "),s("code",[t._v("w2")]),t._v(" 被极度压缩，学习难度大")])]),t._v(" "),s("p",[s("strong",[t._v("标准化后")]),t._v("：")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("x1")]),t._v(" 与 "),s("code",[t._v("x2")]),t._v(" 都在 [-2, 2] 范围")]),t._v(" "),s("li",[t._v("权重不需要极度调整，训练稳定")])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"✅-3-提升泛化能力"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#✅-3-提升泛化能力"}},[t._v("#")]),t._v(" ✅ ③ 提升泛化能力")]),t._v(" "),s("p",[t._v("未经处理的特征在验证集中分布稍变动，模型容易失效。")]),t._v(" "),s("p",[t._v("例：")]),t._v(" "),s("ul",[s("li",[t._v("训练时 "),s("code",[t._v("Proline")]),t._v(" 在 1000 附近")]),t._v(" "),s("li",[t._v("验证时 "),s("code",[t._v("Proline = 2000")]),t._v("，模型崩溃")])]),t._v(" "),s("p",[s("strong",[t._v("标准化后")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("模型更关注的是 z-score，而非绝对值")]),t._v(" "),s("li",[t._v("泛化能力更强")])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"📏-两种常见-scaling-方法比较"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#📏-两种常见-scaling-方法比较"}},[t._v("#")]),t._v(" 📏 两种常见 Scaling 方法比较")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("方法")]),t._v(" "),s("th",[t._v("公式")]),t._v(" "),s("th",[t._v("特点")]),t._v(" "),s("th",[t._v("适用情况")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[s("strong",[t._v("标准化 Standardization")])]),t._v(" "),s("td",[t._v("( x' = \\frac{x - \\mu}{\\sigma} )")]),t._v(" "),s("td",[t._v("将特征转为 0 均值、1 方差的分布")]),t._v(" "),s("td",[t._v("更适合大多数神经网络（如 ReLU）")])]),t._v(" "),s("tr",[s("td",[s("strong",[t._v("归一化 Normalization")])]),t._v(" "),s("td",[t._v("( x' = \\frac{x - x_{min}}{x_{max} - x_{min}} )")]),t._v(" "),s("td",[t._v("将特征缩放到 [0, 1]")]),t._v(" "),s("td",[t._v("用于输入如像素值（图像），或要求非负特征的模型")])])])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"💡-什么时候用哪一个"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#💡-什么时候用哪一个"}},[t._v("#")]),t._v(" 💡 什么时候用哪一个？")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("场景")]),t._v(" "),s("th",[t._v("推荐方法")]),t._v(" "),s("th",[t._v("原因")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("深度神经网络（如 MLP, CNN）")]),t._v(" "),s("td",[t._v("标准化")]),t._v(" "),s("td",[t._v("避免梯度爆炸，训练稳定")])]),t._v(" "),s("tr",[s("td",[t._v("特征是图片像素")]),t._v(" "),s("td",[t._v("归一化")]),t._v(" "),s("td",[t._v("像素范围是 [0,255]，需压缩进 [0,1]")])]),t._v(" "),s("tr",[s("td",[t._v("特征已是非负/密集分布")]),t._v(" "),s("td",[t._v("归一化")]),t._v(" "),s("td",[t._v("保持相对比例更重要")])]),t._v(" "),s("tr",[s("td",[t._v("特征方差相差极大")]),t._v(" "),s("td",[t._v("标准化")]),t._v(" "),s("td",[t._v("统一尺度，防止偏向")])])])]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"🔧-pytorch-实现标准化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#🔧-pytorch-实现标准化"}},[t._v("#")]),t._v(" 🔧 PyTorch 实现标准化")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n\nX "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("14.2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.34")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("21.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("101.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("780.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmean "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("13.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.34")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.36")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("19.5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("99.7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.29")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.03")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.36")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.59")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.96")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.61")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("746.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstd "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.12")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.27")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("14.3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.63")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.00")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.12")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.57")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.23")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.71")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("315.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nX_scaled "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" mean"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" std\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br")])]),s("hr"),t._v(" "),s("h2",{attrs:{id:"✅-小结"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#✅-小结"}},[t._v("#")]),t._v(" ✅ 小结")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("不做 scaling")]),t._v(" 会导致训练慢、结果偏差、模型易过拟合")]),t._v(" "),s("li",[t._v("推荐使用 "),s("strong",[t._v("标准化")]),t._v(" 来处理结构化数据特征")]),t._v(" "),s("li",[t._v("使用 PyTorch 手动实现比依赖 sklearn 更可控，更贴近底层训练逻辑")])])])}),[],!1,null,null,null);s.default=r.exports}}]);