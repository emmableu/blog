(window.webpackJsonp=window.webpackJsonp||[]).push([[434],{804:function(e,i,t){"use strict";t.r(i);var a=t(9),n=Object(a.a)({},(function(){var e=this,i=e._self._c;return i("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[i("ul",[i("li",[e._v("data çš„assumption å¤šä¸å¤š (æ¯”å¦‚featureç›¸äº’ç‹¬ç«‹ï¼Œæ®‹å·®æ­£æ€åˆ†å¸ƒä¹‹ç±»çš„)")]),e._v(" "),i("li",[e._v("data æ˜¯å¦éœ€è¦å…ˆè¿›è¡Œscale")]),e._v(" "),i("li",[e._v("higher dimension é«˜ç»´ç‰¹å¾ è¡¨ç°èƒ½åŠ› ï¼ˆæ¯”å¦‚featureæ¯”sampleè¿˜å¤šï¼‰\n"),i("ul",[i("li",[e._v("feature > sample ä¸èƒ½æ±‚è§£çš„æ–¹æ³•ï¼š linear regression, logistic regression ï¼ˆä½†æ˜¯åŠ äº† L1/L2 regularization å°±å¯ä»¥æ±‚è§£ï¼‰")]),e._v(" "),i("li",[e._v("æ˜¯å¦å€¾å‘äºå¾—åˆ°ç¨€ç–è§£ ï¼ˆæ˜¯å¦å¯ä»¥é€šè¿‡æ±‚è§£è¿™ä¸ªregressionçš„ç»“æœå»æ‰ä¸€äº›æ²¡ç”¨çš„featureï¼‰\n"),i("ul",[i("li",[e._v("å€¾å‘äºå¾—åˆ°ç¨€ç–è§£çš„æ–¹æ³•: L1 regularization, ReLU activation functionï¼Œ svm with hinge loss")])])])])]),e._v(" "),i("li",[e._v("æ˜¯å¦sensitive to outlier:\n"),i("ul",[i("li",[e._v("good model: SVM")])])]),e._v(" "),i("li",[e._v("æ›´å€¾å‘äºoverfit (low bias, high variance),  è¿˜æ˜¯underfit\n"),i("ul",[i("li",[e._v("overfit: Decision Tree, Neural Network")])])]),e._v(" "),i("li",[e._v("è®­ç»ƒé€Ÿåº¦")]),e._v(" "),i("li",[e._v("hyper parameter tuningçš„éš¾åº¦/éº»çƒ¦ç¨‹åº¦")]),e._v(" "),i("li",[e._v("å¯¹äºimbalanced datasetçš„å¤„ç†èƒ½åŠ›")]),e._v(" "),i("li",[e._v("æ¨¡å‹æœ¬èº«æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£ï¼ˆæ¯”å¦‚decision treeï¼Œlogistic regression å°±å¾ˆå®¹æ˜“ç†è§£ï¼‰")]),e._v(" "),i("li",[e._v("èƒ½ä¸èƒ½å¾—åˆ°interpretableçš„feature importance")]),e._v(" "),i("li",[e._v("minimum æ˜¯å¦ = global minimum  (æ˜¯å¦æ˜¯convex function)")])]),e._v(" "),i("h2",{attrs:{id:"linear-regression-è¯„ä»·"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#linear-regression-è¯„ä»·"}},[e._v("#")]),e._v(" Linear Regression è¯„ä»·")]),e._v(" "),i("ul",[i("li",[e._v("data assumption å¤šï¼š\n"),i("ul",[i("li",[e._v("residuals\n"),i("ul",[i("li",[e._v("iid")]),e._v(" "),i("li",[e._v("normal distribution with mean=0 (å¦‚æœä¸è¿›è¡Œbetaç½®ä¿¡åŒºé—´çš„ä¼°è®¡ï¼Œå°±æ˜¯mean 0, constant variance)")]),e._v(" "),i("li",[e._v("independent of X")])])]),e._v(" "),i("li",[e._v("X / features:\n"),i("ul",[i("li",[e._v("independent with each other")]),e._v(" "),i("li",[e._v("linearity: assuming relationship y = wx + b\nå…³äºresidualçš„è§ä¸‹å›¾\n"),i("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202210032015214.png",alt:""}})])])])])]),e._v(" "),i("li",[e._v("data does not need scaling:å¯¹äºçº¿æ€§æ¨¡å‹ y = wx + b è€Œè¨€ï¼Œxçš„ä»»æ„çº¿æ€§å˜æ¢(å¹³ç§»ï¼Œæ”¾ç¼©)éƒ½ä¼šååº”åœ¨wï¼Œbä¸Šï¼Œæ‰€ä»¥ä¸ä¼šå½±å“æ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›ã€‚")]),e._v(" "),i("li",[e._v("does not work well with high dimension (many features)\n"),i("ul",[i("li",[e._v("feature > sample ä¸èƒ½æ±‚è§£ï¼Œ")]),e._v(" "),i("li",[e._v("å¤§éƒ¨åˆ†betaéƒ½ä¸ä¼šå®Œå…¨æ˜¯0ï¼Œ æ‰€ä»¥ä¸å¯ä»¥é€šè¿‡æ±‚è§£è¿™ä¸ªregressionçš„ç»“æœå»æ‰ä¸€äº›æ²¡ç”¨çš„feature")]),e._v(" "),i("li",[e._v("prone to overfit if there are many features / dimensions, causing high variance.")])])]),e._v(" "),i("li",[e._v("sensitive to outliers comparing to models like SVM (å› ä¸º during loss function, treat each sample equallyï¼Œincluding the outliers)")]),e._v(" "),i("li",[e._v("å¦‚æœassumptionå®Œå…¨æ»¡è¶³çš„è¯ï¼Œæ ¹æ®gaussian-markov theorem, it does pretty good in terms of balancing under- and over- fit.\n"),i("ul",[i("li",[e._v("ä½†æ˜¯ï¼Œå¦‚æœassumption ä¸æ»¡è¶³çš„è¯ï¼Œ æ›´å€¾å‘äºunderfit, å› ä¸ºmodel ç›¸å¯¹decision treeï¼Œ neural network æ›´åŠ ç®€å•")])])]),e._v(" "),i("li",[e._v("è®­ç»ƒé€Ÿåº¦å¿«ï¼Œæœ‰ analytical solution, ä¸éœ€è¦ iteratively do gradient descent")]),e._v(" "),i("li",[e._v("hyper parameter tuning å®¹æ˜“ï¼Œæ²¡æœ‰ä»€ä¹ˆhyperparameter")]),e._v(" "),i("li",[e._v("ï¼ˆå› ä¸ºæ˜¯continuous target variableï¼Œä¸éœ€è¦è®¨è®ºimbalanceé—®é¢˜ï¼‰")]),e._v(" "),i("li",[e._v("model is very interpretable")]),e._v(" "),i("li",[e._v("feature importance is very interpretable - can just look at the p value of the beta.")]),e._v(" "),i("li",[e._v("MSE is a convex function, so gradient descent can always reach global minima")])]),e._v(" "),i("h2",{attrs:{id:"logistic-regression-è¯„ä»·"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#logistic-regression-è¯„ä»·"}},[e._v("#")]),e._v(" Logistic Regression è¯„ä»·")]),e._v(" "),i("ul",[i("li",[e._v("data çš„assumption å¤šä¸å¤š (æ¯”å¦‚featureç›¸äº’ç‹¬ç«‹ï¼Œæ®‹å·®æ­£æ€åˆ†å¸ƒä¹‹ç±»çš„)\n"),i("ul",[i("li",[e._v("æ¯”linear regressionå°‘å¾ˆå¤šï¼Œæ‰€æœ‰å…³äºresidualçš„éƒ½ä¸éœ€è¦")]),e._v(" "),i("li",[e._v("åªéœ€è¦ï¼š\n"),i("ul",[i("li",[e._v("X / features:\n"),i("ul",[i("li",[e._v("independent with each other")]),e._v(" "),i("li",[e._v("linearity: assumes linearity of independent variables and log odds.")])])])])])])]),e._v(" "),i("li",[e._v("data æ˜¯å¦éœ€è¦å…ˆè¿›è¡Œscale\n"),i("ul",[i("li",[e._v("è¦ï¼Œå› ä¸ºè¦ç”¨gradient descent æ±‚è§£")])])]),e._v(" "),i("li",[e._v("higher dimension é«˜ç»´ç‰¹å¾ è¡¨ç°èƒ½åŠ› ï¼ˆæ¯”å¦‚featureæ¯”sampleè¿˜å¤š\n"),i("ul",[i("li",[e._v("ä¸å¥½")]),e._v(" "),i("li",[e._v("å¤§éƒ¨åˆ†betaéƒ½ä¸ä¼šå®Œå…¨æ˜¯0ï¼Œ æ‰€ä»¥ä¸å¯ä»¥é€šè¿‡æ±‚è§£è¿™ä¸ªregressionçš„ç»“æœå»æ‰ä¸€äº›æ²¡ç”¨çš„feature")]),e._v(" "),i("li",[e._v("prone to overfit if there are many features / dimensions, causing high variance.")])])]),e._v(" "),i("li",[e._v("æ˜¯å¦sensitive to outlier:\n"),i("ul",[i("li",[e._v("ä¸å¦‚SVM")])])]),e._v(" "),i("li",[e._v("æ›´å€¾å‘äºoverfit (low bias, high variance),  è¿˜æ˜¯underfit\n"),i("ul",[i("li",[e._v("more underfit comparing to neural network")]),e._v(" "),i("li",[e._v("more overfit comparing to naive bayes.")])])]),e._v(" "),i("li",[e._v("è®­ç»ƒé€Ÿåº¦\n"),i("ul",[i("li",[e._v("relatively low comparing to linear regression or naive bayes")])])]),e._v(" "),i("li",[e._v("hyper parameter tuningçš„éš¾åº¦/éº»çƒ¦ç¨‹åº¦\n"),i("ul",[i("li",[e._v("å¥½åƒæ²¡æœ‰ hyper parameterï¼Œ å¯ä»¥åŠ regularization")])])]),e._v(" "),i("li",[e._v("æ¨¡å‹æœ¬èº«æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£ï¼ˆæ¯”å¦‚decision treeï¼Œlogistic regression å°±å¾ˆå®¹æ˜“ç†è§£ï¼‰\n"),i("ul",[i("li",[e._v("æ˜¯, å¯ä»¥ç”¨odds ratio The odds ratio represents the odds that an outcome will occur given the presence of a specific predictor,")])])]),e._v(" "),i("li",[e._v("èƒ½ä¸èƒ½å¾—åˆ°interpretableçš„feature importance\n"),i("ul",[i("li",[e._v("èƒ½")])])]),e._v(" "),i("li",[e._v("minimum æ˜¯å¦ = global minimum  (æ˜¯å¦æ˜¯convex function)\n"),i("ul",[i("li",[e._v("æ˜¯ï¼Œloss function is convex")])])]),e._v(" "),i("li",[e._v("ï¼ˆè·³è¿‡ï¼‰å¯¹äºimbalanced datasetçš„å¤„ç†èƒ½åŠ›\n"),i("ul",[i("li",[e._v("there may not be sufficient patterns belonging to the minority class to adequately represent its distribution.")])])])]),e._v(" "),i("h2",{attrs:{id:"lasso-ridge-l1-l2-regularization-regression-è¯„ä»·"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#lasso-ridge-l1-l2-regularization-regression-è¯„ä»·"}},[e._v("#")]),e._v(" Lasso / Ridge (L1 / L2 Regularization) Regression è¯„ä»·")]),e._v(" "),i("ul",[i("li",[i("p",[e._v("data assumption æ¯” linear regression å°‘: ä¸éœ€è¦æ®‹å·®æ­£æ€åˆ†å¸ƒï¼Œå› ä¸ºå¾—åˆ°çš„betaä¸æ˜¯unbiasedçš„ï¼Œä¸ç¬¦åˆtåˆ†å¸ƒ")]),e._v(" "),i("ul",[i("li",[e._v("éœ€è¦çš„assumptionï¼š\n"),i("ul",[i("li",[e._v("residual:\n"),i("ul",[i("li",[e._v("iid")]),e._v(" "),i("li",[e._v("0 mean, constant variance")]),e._v(" "),i("li",[e._v("independent of X")])])]),e._v(" "),i("li",[e._v("X / feature:\n"),i("ul",[i("li",[e._v("don't need to be completely independent with each other")]),e._v(" "),i("li",[e._v("linearity: assuming relationship y = wx + b")])])])])])])]),e._v(" "),i("li",[i("p",[e._v("data need scaling")]),e._v(" "),i("ul",[i("li",[i("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-2.png",alt:""}})])])]),e._v(" "),i("li",[i("p",[e._v("works better with high dimension:")]),e._v(" "),i("ul",[i("li",[e._v("L1 allows sparcity -> some beta will be fitted to be 0, so, Lasso Regularization can exclude useless variables from the model and, in general, tends to perform well when we need to remove a lot of useless variables from a model.")])])]),e._v(" "),i("li",[i("p",[e._v("sensitive to outliers comparing to models like SVM")])]),e._v(" "),i("li",[i("p",[e._v("comparing to LR tends to underfit if too big lambda")])]),e._v(" "),i("li",[i("p",[e._v("hyper parameter tuning å®¹æ˜“, åªæœ‰lambda")])]),e._v(" "),i("li",[i("p",[e._v("model is very interpretable")])]),e._v(" "),i("li",[i("p",[e._v("minimum æ˜¯å¦ = global minimum  (æ˜¯å¦æ˜¯convex function):")]),e._v(" "),i("ul",[i("li",[e._v("L2 Ridge regression: loss function is convex,  gradient descent can always reach global minima")]),e._v(" "),i("li",[e._v("L1 Lasso: sometimes not convex, so gradient descent don't always reach global minima")])])])]),e._v(" "),i("h2",{attrs:{id:"naive-baiyes-è¯„ä»·"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#naive-baiyes-è¯„ä»·"}},[e._v("#")]),e._v(" Naive Baiyes è¯„ä»·")]),e._v(" "),i("ul",[i("li",[e._v("data çš„assumption å¤šä¸å¤šï¼š\n"),i("ul",[i("li",[e._v("æœ‰ä¸€ä¸ªï¼š features need to be independent with each other")])])]),e._v(" "),i("li",[e._v("data æ˜¯å¦éœ€è¦å…ˆè¿›è¡Œscale\n"),i("ul",[i("li",[e._v("no")])])]),e._v(" "),i("li",[e._v("higher dimension é«˜ç»´ç‰¹å¾ è¡¨ç°èƒ½åŠ› ï¼ˆæ¯”å¦‚featureæ¯”sampleè¿˜å¤šï¼‰\n"),i("ul",[i("li",[e._v("If your data has ğ‘˜ dimensions, then a fully general ML algorithm which attempts to learn all possible correlations between these features has to deal with 2ğ‘˜ possible feature interactions, and therefore needs on the order of 2ğ‘˜ many data points to be performant.")]),e._v(" "),i("li",[e._v("However because Naive Bayes assumes independence between features, it only needs on the order of ğ‘˜ many data points, exponentially fewer.")])])]),e._v(" "),i("li",[e._v("æ˜¯å¦sensitive to outlier:\n"),i("ul",[i("li",[e._v("sensitive to outliers comparing to SVM")])])]),e._v(" "),i("li",[e._v("æ›´å€¾å‘äºoverfit (low bias, high variance),  è¿˜æ˜¯underfit\n"),i("ul",[i("li",[e._v("underfit, since the interactions are not modeled, some of the information in the data is ignored. This makes it an inherently high bias model; it has a high approximation error but as a result it also does not overfit.")])])]),e._v(" "),i("li",[e._v("è®­ç»ƒé€Ÿåº¦\n"),i("ul",[i("li",[e._v("fast")])])]),e._v(" "),i("li",[e._v("hyper parameter tuningçš„éš¾åº¦/éº»çƒ¦ç¨‹åº¦\n"),i("ul",[i("li",[e._v("easy, only one hyperparameter - pseudocount")])])]),e._v(" "),i("li",[e._v("å¯¹äºimbalanced datasetçš„å¤„ç†èƒ½åŠ›\n"),i("ul",[i("li",[e._v("not good")]),e._v(" "),i("li",[i("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202210050154011.png",alt:""}})]),e._v(" "),i("li",[e._v("Even though the likelihood probabilities are similar to some extent, but the posterior probability is badly affected by prior probabilities. Here in the above example the class +ve prior probability will be 9 times higher than the class -ve, this difference in naive bayes creates a bias for class +ve.")]),e._v(" "),i("li",[e._v("One simple solution is to ignore the prior probabilities. (Or) You can do undersampling or oversampling.")])])]),e._v(" "),i("li",[e._v("(è·³è¿‡)æ¨¡å‹æœ¬èº«æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£ï¼ˆæ¯”å¦‚decision treeï¼Œlogistic regression å°±å¾ˆå®¹æ˜“ç†è§£ï¼‰è·³è¿‡")]),e._v(" "),i("li",[e._v("(è·³è¿‡)èƒ½ä¸èƒ½å¾—åˆ°interpretableçš„feature importance è·³è¿‡")]),e._v(" "),i("li",[e._v("(è·³è¿‡)minimum æ˜¯å¦ = global minimum  (æ˜¯å¦æ˜¯convex function) è·³è¿‡")])]),e._v(" "),i("h2",{attrs:{id:"decision-tree-è¯„ä»·"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#decision-tree-è¯„ä»·"}},[e._v("#")]),e._v(" Decision Tree è¯„ä»·")]),e._v(" "),i("ul",[i("li",[e._v("data çš„assumption å¤šä¸å¤š (æ¯”å¦‚featureç›¸äº’ç‹¬ç«‹ï¼Œæ®‹å·®æ­£æ€åˆ†å¸ƒä¹‹ç±»çš„)\n"),i("ul",[i("li",[e._v("å¥½åƒæ²¡æœ‰")])])]),e._v(" "),i("li",[e._v("data æ˜¯å¦éœ€è¦å…ˆè¿›è¡Œscale\n"),i("ul",[i("li",[e._v("no need")])])]),e._v(" "),i("li",[e._v("higher dimension é«˜ç»´ç‰¹å¾ è¡¨ç°èƒ½åŠ›\n"),i("ul",[i("li",[e._v("tends to become very overfit,  the number of branches grows exponentially with the number of features")])])]),e._v(" "),i("li",[e._v("æ˜¯å¦sensitive to outlier:\n"),i("ul",[i("li",[e._v("not sensitive to outliers since the partitioning happens based on the proportion of samples within the split ranges and not on absolute values.")]),e._v(" "),i("li",[e._v("especially when we use early stopping")])])]),e._v(" "),i("li",[e._v("æ›´å€¾å‘äºoverfit (low bias, high variance),  è¿˜æ˜¯underfit\n"),i("ul",[i("li",[e._v("overfit,very specific rules")])])]),e._v(" "),i("li",[e._v("è®­ç»ƒé€Ÿåº¦\n"),i("ul",[i("li",[e._v("not as fast comparing to logistic regression / naive bayes, but still fast enough comparing to neural network")]),e._v(" "),i("li",[e._v("test time is fast as it's just linearly traversing the test rules.")])])]),e._v(" "),i("li",[e._v("hyper parameter tuningçš„éš¾åº¦/éº»çƒ¦ç¨‹åº¦\n"),i("ul",[i("li",[e._v("å¥½åƒæ²¡æœ‰hyper parameterï¼Œé™¤éå®šä¹‰early stopping")])])]),e._v(" "),i("li",[e._v("å¯¹äºimbalanced datasetçš„å¤„ç†èƒ½åŠ›\n"),i("ul",[i("li",[e._v("ä¸å¤ªå¥½")]),e._v(" "),i("li",[e._v("Decision trees implementations normally use Gini impurity for finding splits. For each rule/condition, when calculating the gini impurity it is a weighted average on. it weights higher on the more prevaling condition (e.g., loves troll when predicting loves popcorn). But more prevaling sample has a higher say on the more prevaling condition.")])])]),e._v(" "),i("li",[e._v("æ¨¡å‹æœ¬èº«æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£ï¼ˆæ¯”å¦‚decision treeï¼Œlogistic regression å°±å¾ˆå®¹æ˜“ç†è§£ï¼‰\n"),i("ul",[i("li",[e._v("yes. has interpretable rules")])])]),e._v(" "),i("li",[e._v("(è·³è¿‡) èƒ½ä¸èƒ½å¾—åˆ°interpretableçš„feature importance")]),e._v(" "),i("li",[e._v("(è·³è¿‡) minimum æ˜¯å¦ = global minimum  (æ˜¯å¦æ˜¯convex function) è·³è¿‡")])]),e._v(" "),i("h2",{attrs:{id:"svm-è¯„ä»·"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#svm-è¯„ä»·"}},[e._v("#")]),e._v(" SVM è¯„ä»·")]),e._v(" "),i("ul",[i("li",[e._v("data çš„assumption å¤šä¸å¤š (æ¯”å¦‚featureç›¸äº’ç‹¬ç«‹ï¼Œæ®‹å·®æ­£æ€åˆ†å¸ƒä¹‹ç±»çš„)\n"),i("ul",[i("li",[e._v("æ¯”è¾ƒå®½æ¾ï¼Œåªéœ€è¦Data is linearly separable. Even if the linear boundary is in an augmented feature space. for example, after the kernel trick")])])]),e._v(" "),i("li",[e._v("data æ˜¯å¦éœ€è¦å…ˆè¿›è¡Œscale\n"),i("ul",[i("li",[e._v("è¦")]),e._v(" "),i("li",[e._v("SVM tries to maximize the distance between the separating plane and the support vectors.")]),e._v(" "),i("li",[e._v("If one feature (i.e. one dimension in this space) has very large values, it will dominate the other features when calculating the distance.")]),e._v(" "),i("li",[e._v("If you rescale all features (e.g. to [0, 1]), they all have the same influence on the distance metric.")])])]),e._v(" "),i("li",[e._v("higher dimension é«˜ç»´ç‰¹å¾ è¡¨ç°èƒ½åŠ› ï¼ˆæ¯”å¦‚featureæ¯”sampleè¿˜å¤šï¼‰\n"),i("ul",[i("li",[e._v("è¿˜è¡Œï¼Œå› ä¸ºå®ƒä¼šé€šè¿‡fitè¿™ä¸ªmodelæ¥æ”¾ç¼© c => regularization parameter, the degree to which our model will accept misclassifications in our datasetï¼Œ so that it generalise well over training data.")]),e._v(" "),i("li",[e._v("æ˜¯å¦å€¾å‘äºå¾—åˆ°ç¨€ç–è§£ ï¼ˆæ˜¯å¦å¯ä»¥é€šè¿‡æ±‚è§£è¿™ä¸ªmodelçš„ç»“æœå»æ‰ä¸€äº›æ²¡ç”¨çš„featureï¼‰\n"),i("ul",[i("li",[e._v("ç”¨hinge loss ä½œä¸ºloss functionï¼Œ å› ä¸ºhinge lossæ˜¯ = max(0, 1-z), æ‰€ä»¥>1æ—¶æ˜¯0ï¼Œæ‰€ä»¥ä¼šæœ‰ 0 è§£")])])])])]),e._v(" "),i("li",[e._v("æ˜¯å¦sensitive to outlier:\n"),i("ul",[i("li",[e._v("ä¸sensitive to outlierï¼Œå¾ˆrobustï¼Œè¿˜æ˜¯å› ä¸ºå®ƒä¼šé€šè¿‡fitè¿™ä¸ªmodelæ¥æ”¾ç¼© cï¼Œè§ä¸Š\n"),i("ul",[i("li",[e._v("è€Œä¸”åªå…³å¿ƒsoft margin å‘¨å›´çš„ç‚¹")])])]),e._v(" "),i("li",[e._v("good model: SVM")])])]),e._v(" "),i("li",[e._v("æ›´å€¾å‘äºoverfit (low bias, high variance),  è¿˜æ˜¯underfit\n"),i("ul",[i("li",[e._v("æ¯”èµ·lr å’Œ nn å’Œ decision tree è¿™äº›æ›´å®¹æ˜“ underfit, å› ä¸ºæœ‰soft margin å’Œ regularization parameter")])])]),e._v(" "),i("li",[e._v("è®­ç»ƒé€Ÿåº¦\n"),i("ul",[i("li",[e._v("ä¸æ˜¯å¾ˆå¿«ï¼Œå› ä¸ºè¿˜æ˜¯è¦ç”¨gradient descent")])])]),e._v(" "),i("li",[e._v("hyper parameter tuningçš„éš¾åº¦/éº»çƒ¦ç¨‹åº¦\n"),i("ul",[i("li",[e._v("éº»çƒ¦ï¼Œæ¯”å¦‚ç¡®å®šå“ªä¸ªkernel")])])]),e._v(" "),i("li",[e._v("æ¨¡å‹æœ¬èº«æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£ï¼ˆæ¯”å¦‚decision treeï¼Œlogistic regression å°±å¾ˆå®¹æ˜“ç†è§£ï¼‰\n"),i("ul",[i("li",[e._v("æ¯”è¾ƒéš¾ï¼Œ æœ‰feature weightsï¼Œ but they don't correspond 1-1 to feature importance such as in logistic regression")])])]),e._v(" "),i("li",[e._v("minimum æ˜¯å¦ = global minimum  (æ˜¯å¦æ˜¯convex function)\n"),i("ul",[i("li",[e._v("æ˜¯convex")])])])]),e._v(" "),i("h2",{attrs:{id:"knn-è¯„ä»·"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#knn-è¯„ä»·"}},[e._v("#")]),e._v(" KNN è¯„ä»·")]),e._v(" "),i("ul",[i("li",[i("p",[e._v("data çš„assumption å¤šä¸å¤š")]),e._v(" "),i("ul",[i("li",[e._v("assuming similar points situated closely with each other.")])])]),e._v(" "),i("li",[i("p",[e._v("data æ˜¯å¦éœ€è¦å…ˆè¿›è¡Œscale")]),e._v(" "),i("ul",[i("li",[e._v("è¦ï¼Œå› ä¸ºè¦è®¡ç®—è·ç¦»")])])]),e._v(" "),i("li",[i("p",[e._v("higher dimension é«˜ç»´ç‰¹å¾ è¡¨ç°èƒ½åŠ› ï¼ˆæ¯”å¦‚featureæ¯”sampleè¿˜å¤šï¼‰")]),e._v(" "),i("ul",[i("li",[e._v("ä¸å¤ªè¡Œï¼Œå› ä¸ºé«˜ç»´çš„æ—¶å€™ï¼š\n"),i("ul",[i("li",[e._v("Our assumption of similar points being situated closely breaksï¼š high dimension creates exponentially higher space, points tend to never be close together.")]),e._v(" "),i("li",[e._v("It becomes computationally more expensive to compute distance and find the nearest neighbors in high-dimensional space")])])])])]),e._v(" "),i("li",[i("p",[e._v("æ˜¯å¦sensitive to outlier:")]),e._v(" "),i("ul",[i("li",[e._v("If â€˜Kâ€™ value is low, the model is susceptible to outliers. => Let take K=1, suppose there is 1 outlier near to our test point and then the model predicts the label corresponding to the outlier.")]),e._v(" "),i("li",[e._v("If â€˜Kâ€™ value is high, the model is robust to outliers.")])])]),e._v(" "),i("li",[i("p",[e._v("æ›´å€¾å‘äºoverfit (low bias, high variance),  è¿˜æ˜¯underfit")]),e._v(" "),i("ul",[i("li",[e._v("k å°ï¼Œ é«˜ç»´æ•°æ®ï¼š overfit")]),e._v(" "),i("li",[e._v("k å¤§ï¼Œä½ç»´æ•°æ®ï¼š underfit")])])]),e._v(" "),i("li",[i("p",[e._v("è®­ç»ƒé€Ÿåº¦")]),e._v(" "),i("ul",[i("li",[e._v("æ…¢Â Â for each testing sample, it requires calculating distance with each training sample")])])]),e._v(" "),i("li",[i("p",[e._v("hyper parameter tuningçš„éš¾åº¦/éº»çƒ¦ç¨‹åº¦")]),e._v(" "),i("ul",[i("li",[e._v("k å¾ˆéš¾å†³å®šï¼Œè¦ç”¨hyperparameter tuning")])])]),e._v(" "),i("li",[i("p",[e._v("å¯¹äºimbalanced datasetçš„å¤„ç†èƒ½åŠ›")]),e._v(" "),i("ul",[i("li",[e._v("theoretically, unbalanced classes are not a problem at all for the k-nearest neighbor algorithm.\n"),i("ul",[i("li",[e._v("Because the algorithm is not influenced in any way by the size of the class, it will not favor any on the basis of size.")])])]),e._v(" "),i("li",[e._v("but:  there may not be sufficient patterns belonging to the minority class to adequately represent its distribution.")])])]),e._v(" "),i("li",[i("p",[e._v("æ¨¡å‹æœ¬èº«æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£ï¼ˆæ¯”å¦‚decision treeï¼Œlogistic regression å°±å¾ˆå®¹æ˜“ç†è§£ï¼‰")]),e._v(" "),i("ul",[i("li",[e._v("not interpretable")])])])])])}),[],!1,null,null,null);i.default=n.exports}}]);