(window.webpackJsonp=window.webpackJsonp||[]).push([[440],{811:function(t,e,r){"use strict";r.r(e);var a=r(9),s=Object(a.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h2",{attrs:{id:"definition"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#definition"}},[t._v("#")]),t._v(" Definition")]),t._v(" "),e("p",[t._v("A supervised learning model, including a set of conditions and leaves, creating a tree structure.")]),t._v(" "),e("p",[t._v("The goal of decision tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://www.youtube.com/watch?v=_L39rN6gz7Y",target:"_blank",rel:"noopener noreferrer"}},[t._v("youtube"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"classification-tree"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#classification-tree"}},[t._v("#")]),t._v(" classification Tree")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209211809589.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209211810180.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209211811045.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209211811207.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"gini-impurity-entropy-information-gain"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#gini-impurity-entropy-information-gain"}},[t._v("#")]),t._v(" Gini Impurity / Entropy / Information Gain")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209211823400.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209211824229.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209211824180.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209211825366.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209211826108.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"regression-trees"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#regression-trees"}},[t._v("#")]),t._v(" Regression Trees")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212129496.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212130493.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212140399.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212141337.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212141492.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212143091.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212143390.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212143564.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212144819.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"regression-tree-with-multiple-features"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#regression-tree-with-multiple-features"}},[t._v("#")]),t._v(" Regression Tree with Multiple Features")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212144608.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212158547.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212159513.png",alt:""}}),t._v(" "),e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209212159827.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"how-to-prevent-overfitting-in-decision-tree"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#how-to-prevent-overfitting-in-decision-tree"}},[t._v("#")]),t._v(" How to prevent overfitting in Decision Tree?")]),t._v(" "),e("ol",[e("li",[e("p",[t._v("early stopping (pre-pruning)， 在完全长成以前停止，以防止过拟合。")]),t._v(" "),e("ol",[e("li",[t._v("限制树的高度，可以利用交叉验证选择")]),t._v(" "),e("li",[t._v("利用分类指标，如果下一次切分没有降低误差，则停止切分")]),t._v(" "),e("li",[t._v("限制树的节点个数，比如某个节点小于100个样本，停止对该节点切分")])])]),t._v(" "),e("li",[e("p",[t._v("post pruning"),e("br"),t._v("\n它首先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。")])])])])}),[],!1,null,null,null);e.default=s.exports}}]);