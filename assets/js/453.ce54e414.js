(window.webpackJsonp=window.webpackJsonp||[]).push([[453],{823:function(t,e,a){"use strict";a.r(e);var i=a(9),s=Object(i.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h2",{attrs:{id:"definition"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#definition"}},[t._v("#")]),t._v(" Definition:")]),t._v(" "),e("ul",[e("li",[t._v("What t-SNE does is find a way to project data into a low dimensional space, so that the clustering in the high dimensional space is preserved")])]),t._v(" "),e("h2",{attrs:{id:"process"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#process"}},[t._v("#")]),t._v(" process")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202210061845422.png",alt:""}})]),t._v(" "),e("ol",[e("li",[t._v("we start with putting the points to random points in low dimensional space (on the line)")]),t._v(" "),e("li",[t._v("at each step, a point in the new dimensional space (on the line) is attracted to points it is near in the original space (the scatter point), and repelled by points it is far from.")])]),t._v(" "),e("h2",{attrs:{id:"detailed-process"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#detailed-process"}},[t._v("#")]),t._v(" detailed process")]),t._v(" "),e("h2",{attrs:{id:"how-t-sne-works"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#how-t-sne-works"}},[t._v("#")]),t._v(" How t-SNE works?")]),t._v(" "),e("h2",{attrs:{id:"probability-distribution"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#probability-distribution"}},[t._v("#")]),t._v(" Probability Distribution")]),t._v(" "),e("p",[t._v("Let‚Äôs start with "),e("strong",[t._v("SNE")]),t._v(" part of t-SNE. I‚Äôm far better with explaining things visually so this is going to be our dataset:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*NYve_va3wHU4zNnj.png",alt:""}})]),t._v(" "),e("p",[t._v("It has 3 different classes and you can easily distinguish them from each other. The first part of the algorithm is to create a "),e("strong",[t._v("probability distribution")]),t._v(" that represents similarities between neighbors. What is ‚Äúsimilarity‚Äù? "),e("a",{attrs:{href:"http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Original paper"),e("OutboundLink")],1),t._v(" states ‚Äú "),e("strong",[t._v("similarity of datapoint")]),t._v(" x‚±º "),e("strong",[t._v("to datapoint")]),t._v(" x·µ¢ "),e("strong",[t._v("is the conditional probability")]),t._v(" p_{j|i}"),e("strong",[t._v(", that")]),t._v(" x·µ¢ "),e("strong",[t._v("would pick")]),t._v(" x‚±º "),e("strong",[t._v("as its neighbor")]),t._v(" ‚Äú.")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*ETuCH0wXgSyit1i3.png",alt:""}})]),t._v(" "),e("p",[t._v("We‚Äôve picked one of the points from the dataset. Now we have to pick another point and calculate Euclidean Distance between them |x·µ¢ ‚Äî x‚±º|")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*g1bTTv6Wu632piCu.png",alt:""}})]),t._v(" "),e("p",[t._v("The next part of the original paper states that it has to be "),e("strong",[t._v("proportional to probability density under a Gaussian centered at")]),t._v(" x·µ¢. So we have to generate Gaussian distribution with mean at x·µ¢_,_ and place our distance on the X-axis.")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*j6P77qstfwQ6mkT8.png",alt:""}})]),t._v(" "),e("p",[t._v("Right now you might wonder about "),e("em",[t._v("œÉ¬≤")]),t._v(" (variance) and that‚Äôs a good thing. But let‚Äôs just ignore it for now and assume I‚Äôve already decided what it should be. After calculating the first point we have to do the same thing for every single point out there.")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*Afr8xsKrl6dwZ10Q.png",alt:""}})]),t._v(" "),e("p",[t._v("You might think, we‚Äôre already done with this part. But that‚Äôs just the beginning.")]),t._v(" "),e("h2",{attrs:{id:"scattered-clusters-and-variance"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#scattered-clusters-and-variance"}},[t._v("#")]),t._v(" Scattered clusters and variance")]),t._v(" "),e("p",[t._v("Up to this point, our clusters were tightly bounded within its group. What if we have a new cluster like that:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*nZrA0hPQqM1Ce7-j.png",alt:""}})]),t._v(" "),e("p",[t._v("We should be able to apply the same process as before, shouldn‚Äôt we?")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*0MzS6wydzd5Dr02Y.png",alt:""}})]),t._v(" "),e("p",[t._v("We‚Äôre still not done. You can distinguish between similar and non-similar points but absolute values of probability are much smaller than in the first example (compare Y-axis values).")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*kpctqZwCMDkqJK3u.png",alt:""}})]),t._v(" "),e("p",[t._v("We can fix that by dividing the current projection value by the sum of the projections.")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/526/1*1gBOzGPwWEN4L_HhYLN-VQ.png",alt:""}})]),t._v(" "),e("p",[t._v("Which if you apply to the first example will look sth like:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1228/1*r3tuMMndpFZswRfIVNCEaw.png",alt:""}})]),t._v(" "),e("p",[t._v("And for the second example:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1262/1*9XfcJTE-gFjGxFgdqOQKsQ.png",alt:""}})]),t._v(" "),e("p",[t._v("This scales all values to have a sum equal to 1. It‚Äôs a good place to mention that p_{i|i}‚Äã is set to be equal to 0, not 1.")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/228/1*2oCxcLq7hvxQcrjmFiClMA.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"dealing-with-different-distances"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#dealing-with-different-distances"}},[t._v("#")]),t._v(" Dealing with different distances")]),t._v(" "),e("p",[t._v("If we take two points and try to calculate conditional probability between them then values of p_{i|j}‚Äã and p_{j|i}‚Äã will be different:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*pTTqRArwYV_tGnF0.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*-JXLaDNYjjSSGVdf.png",alt:""}})]),t._v(" "),e("p",[t._v("The reason for that is because they are coming from two different distributions. Which one should we pick to the calculation then?")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/336/1*gFbUxuUZlcRhIzYXvYzUCA.png",alt:""}})]),t._v(" "),e("p",[t._v("Where "),e("em",[t._v("N")]),t._v(" is a number of dimensions.")]),t._v(" "),e("h2",{attrs:{id:"the-lie"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#the-lie"}},[t._v("#")]),t._v(" The lie üòÉ")]),t._v(" "),e("p",[t._v("Now when we have everything scaled to 1 (yes, the sum of all equals 1), I can tell you that I wasn‚Äôt completely honest about while the process with you üòÉ Calculation all of that would be quite painful for the algorithm and that‚Äôs not what exactly is in "),e("a",{attrs:{href:"http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("t-SNE paper"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/736/1*J4lRX3F6qR9TF9VgH6Vd1Q.png",alt:""}})]),t._v(" "),e("p",[t._v("This is an original formula to calculate p_{j|i}. Why did I lie to you? First, because it‚Äôs easier to get an intuition about how it works. Second, because I was going to show you the trough either way.")]),t._v(" "),e("h2",{attrs:{id:"perplexity"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#perplexity"}},[t._v("#")]),t._v(" Perplexity")]),t._v(" "),e("p",[t._v("If you look at this formula. You can spot that our")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/226/1*2q0ECctHTmnbCop71LlJOQ.png",alt:""}})]),t._v(" "),e("p",[t._v("is")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/446/1*3_qQH7KjQR89ymcDk0Y5Yw.png",alt:""}})]),t._v(" "),e("p",[t._v("If I would show you this straight away, it would be hard to explain where "),e("em",[t._v("œÉ¬≤")]),t._v(" is coming from and what is a dependency between it and our clusters. Now you know that variance depends on Gaussian and the number of points surrounding the center of it. This is the part where "),e("strong",[t._v("perplexity")]),t._v(" value comes. A perplexity is more or less a target number of neighbors for our central point. Basically, the higher the perplexity is the higher value variance has. Our ‚Äúred‚Äù group is close to each other and if we set perplexity to 4, it searches the right value of to ‚Äúfit‚Äù our 4 neighbors. If you want to be more specific then you can quote the original paper:")]),t._v(" "),e("blockquote",[e("p",[e("em",[t._v("SNE performs a binary search for the value of sigma that produces probability distribution with a fixed perplexity that is specified by the user")])])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/496/1*Csv1yl-2zOxC42wV-WGVkQ.png",alt:""}})]),t._v(" "),e("p",[t._v("Where")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/292/1*jhLo78eY9Jky4vMP42RC8Q.png",alt:""}})]),t._v(" "),e("p",[t._v("‚Äã is "),e("strong",[t._v("Shannon entropy")]),t._v(". But unless you want to implement t-SNE yourself, the only thing you need to know is that perplexity you choose is positively correlated with the value of \\mu_i_Œºi_‚Äã and for the same perplexity you will have multiple different \\mu_i_Œºi_‚Äã, base on distances. Typical perplexity value ranges between 5 and 50.")]),t._v(" "),e("h2",{attrs:{id:"original-formula-interpretation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#original-formula-interpretation"}},[t._v("#")]),t._v(" Original formula interpretation")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/736/1*J4lRX3F6qR9TF9VgH6Vd1Q.png",alt:""}})]),t._v(" "),e("p",[t._v("When you look on this formula you might notice that our Gaussian is converted into")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/436/1*xi6CUrwPLG4-1CZwt-hX0Q.png",alt:""}})]),t._v(" "),e("p",[t._v("Let me show you how that looks like:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*sNHrck20Xt7uS7X9.png",alt:""}})]),t._v(" "),e("p",[t._v("If you play with "),e("em",[t._v("œÉ¬≤")]),t._v(" for a while you can notice that the blue curve remains fixed at point "),e("em",[t._v("x")]),t._v("=0. It only stretches when "),e("em",[t._v("œÉ¬≤")]),t._v(" increases.")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*SQWPC3TlgUqnsuSu.png",alt:""}})]),t._v(" "),e("p",[t._v("That helps distinguish neighbor‚Äôs probabilities and because you‚Äôve already understood the whole process you should be able to adjust it to new values.")]),t._v(" "),e("h2",{attrs:{id:"create-low-dimensional-space"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#create-low-dimensional-space"}},[t._v("#")]),t._v(" Create low-dimensional space")]),t._v(" "),e("p",[t._v("The next part of t-SNE is to create low-dimensional space with the same number of points as in the original space. Points should be spread randomly on a new space. The goal of this algorithm is to find similar probability distribution in low-dimensional space. The most obvious choice for new distribution would be to use Gaussian again. That‚Äôs not the best idea, unfortunately. One of the properties of Gaussian is that it has a ‚Äúshort tail‚Äù and because of that it creates a "),e("strong",[t._v("crowding problem")]),t._v(". To solve that we‚Äôre going to use "),e("strong",[t._v("Student t-distribution")]),t._v(" with a single degree of freedom. More of how this distribution was selected and why Gaussian is not the best idea you can find in the "),e("a",{attrs:{href:"http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("paper"),e("OutboundLink")],1),t._v(". I decided not to spend much time on it and allow you to read this article within a reasonable time. So now our new formula will look like:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/632/1*ZDiRzmCfK1xuCzldJ8-tvQ.png",alt:""}})]),t._v(" "),e("p",[t._v("instead of:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/690/1*Hax1tT4LMqH9RqUN4d7ulA.png",alt:""}})]),t._v(" "),e("p",[t._v("If you‚Äôre more ‚Äúvisual‚Äù person this might help (values on X-axis are distributed randomly):")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*z_xyrUJsAlkdKNjX.png",alt:""}})]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*SEhDsFAK-qH6fLJv.png",alt:""}})]),t._v(" "),e("p",[t._v("Using Student distribution has exactly what we need. It ‚Äúfalls‚Äù quickly and has a ‚Äúlong tail‚Äù so points won‚Äôt get squashed into a single point. This time we don‚Äôt have to bother with "),e("em",[t._v("œÉ¬≤")]),t._v(" because we don‚Äôt have one in q_{ij} formula. I won‚Äôt generate the whole process of calculating q_{ij} because it works exactly the same as p_{ij}. Instead, just leave you with those two formulas and skip to sth more important:")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/726/1*l4Gjd2F_cnZQDvSi6zlu1w.png",alt:""}})]),t._v(" "),e("h1",{attrs:{id:"gradient-descent"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#gradient-descent"}},[t._v("#")]),t._v(" Gradient descent")]),t._v(" "),e("p",[t._v("To optimize this distribution t-SNE is using "),e("a",{attrs:{href:"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence",target:"_blank",rel:"noopener noreferrer"}},[e("strong",[t._v("Kullback-Leibler divergence")]),e("OutboundLink")],1),t._v(" between the conditional probabilities p_{j|i} and q_{j|i}")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/846/1*xi-IjvMSJmNu-jfHlZ0qvA.png",alt:""}})]),t._v(" "),e("p",[t._v("I‚Äôm not going through the math here because it‚Äôs not important. What we need is a derivate for (it‚Äôs derived in "),e("strong",[t._v("Appendix A")]),t._v(" inside the "),e("a",{attrs:{href:"http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("original paper"),e("OutboundLink")],1),t._v(").")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/938/1*WK-kP2JJsAbYgw49hQENhA.png",alt:""}})]),t._v(" "),e("p",[t._v("You can treat that gradient as repulsion and attraction between points. A gradient is calculated for each point and describes how ‚Äústrong‚Äù it should be pulled and the direction it should choose. If we start with our random 1D plane and perform gradient on the previous distribution it should look like this.")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://miro.medium.com/max/1400/0*gx5m_CS7gVUn8WLH.gif",alt:""}})]),t._v(" "),e("p",[t._v("Ofc. this is an exaggeration. t-SNE doesn‚Äôt run that quickly. I‚Äôve just skipped a lot of steps in there to make it faster. Besides that, the values here are not completely correct, but it‚Äôs good enough to show you the process.")]),t._v(" "),e("h2",{attrs:{id:"tricks-optimizations-done-in-t-sne-to-perform-better"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#tricks-optimizations-done-in-t-sne-to-perform-better"}},[t._v("#")]),t._v(" Tricks (optimizations) done in t-SNE to perform better")]),t._v(" "),e("p",[t._v("t-SNE performs well on itself but there are some improvements allow it to do even better.")]),t._v(" "),e("h2",{attrs:{id:"early-compression"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#early-compression"}},[t._v("#")]),t._v(" Early Compression")]),t._v(" "),e("p",[t._v("To prevent early clustering t-SNE is adding L2 penalty to the cost function at the early stages. You can treat it as standard regularization because it allows the algorithm not to focus on local groups.")]),t._v(" "),e("h2",{attrs:{id:"early-exaggeration"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#early-exaggeration"}},[t._v("#")]),t._v(" Early Exaggeration")]),t._v(" "),e("p",[t._v("This trick allows moving clusters of (q_{ij}‚Äã) more. This time we‚Äôre multiplying p_{ij}‚Äã in early stages. Because of that clusters don‚Äôt get in each other‚Äôs ways.")]),t._v(" "),e("h1",{attrs:{id:"conclusions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#conclusions"}},[t._v("#")]),t._v(" Conclusions")]),t._v(" "),e("p",[t._v("t-SNE is a great tool to understand high-dimensional datasets. It might be less useful when you want to perform dimensionality reduction for ML training (cannot be reapplied in the same way). It‚Äôs not deterministic and iterative so each time it runs, it could produce a different result. But even with that disadvantages it still remains one of the most popular method in the field.")])])}),[],!1,null,null,null);e.default=s.exports}}]);