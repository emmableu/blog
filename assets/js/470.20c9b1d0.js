(window.webpackJsonp=window.webpackJsonp||[]).push([[470],{842:function(t,e,a){"use strict";a.r(e);var r=a(9),n=Object(r.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("p",[e("a",{attrs:{href:"https://github.com/EvilPsyCHo/Attention-PyTorch",target:"_blank",rel:"noopener noreferrer"}},[t._v("source"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"attention-all-in-one"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#attention-all-in-one"}},[t._v("#")]),t._v(" Attention All in ONE")]),t._v(" "),e("p",[e("strong",[t._v("Kaggle")]),t._v("及更多AI分享，欢迎关注我的"),e("a",{attrs:{href:"https://www.zhihu.com/people/zhouzhirui",target:"_blank",rel:"noopener noreferrer"}},[t._v("知乎帐号"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("作者：爱睡觉的KKY")]),t._v(" "),e("p",[t._v("“注意力”在平时的生活中相信大家都深有体会，当你认真读某本书的时候，会感觉眼睛中只有书中正在读的文字，竖起耳朵去听一个很微弱的声音的时候，这个声音也仿佛放大了，能听的更清楚。深度学习中Attention机制非常类似生物的注意力。人们视觉在感知东西的时候一般不会是一个场景从到头看到尾每次全部都看，而往往是根据需求观察注意特定的一部分。而且当人们发现一个场景经常在某部分出现自己想观察的东西时，人们会进行学习在将来再出现类似场景时把注意力放到该部分上。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260011127.png",alt:""}})]),t._v(" "),e("p",[t._v("不同时刻，在做不同任务的时候，我们的注意力不是一成不变的。就输入下图，我们在做图片翻译到文字的任务时，翻译不同对象的时候，聚焦的图片位置是有区别。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260013167.png",alt:""}})]),t._v(" "),e("p",[t._v("基于以上的直觉，Attention可以用于，学习权重分布：")]),t._v(" "),e("ul",[e("li",[t._v("这个加权可以是保留所有分量均做加权（即soft attention）；也可以是在分布中以某种采样策略选取部分分量（即hard attention），此时常用RL来做；")]),t._v(" "),e("li",[t._v("这个加权可以作用在原图上，也可以作用在特征图上；")]),t._v(" "),e("li",[t._v("这个加权可以在时间维度、空间维度、mapping维度以及feature维度。")])]),t._v(" "),e("p",[t._v('以seq2seq举例，传统的模型decoder输入的上下文c是一成不变的，但这显然不够合理，如果这是一个翻译模型，原文为“我喜欢食物”，当我翻译到"i like"时，我翻译下一个"food"时应该更关注的是"食物"，而不是别的。')]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260014022.png",alt:""}})]),t._v(" "),e("p",[t._v("所以，我们在seq2seq解码过程中，在每一步可以利用Attention，对Encoder每一步的hidden state进行加权，获得不同的语义编码，获得更好的翻译效果。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260015978.png",alt:""}})]),t._v(" "),e("p",[t._v("效果展示,横轴是输入的待翻译原文，纵轴是翻译结果，可以看到翻译到不同的英文单词时，对于原文的单词注意力差别是非常巨大的。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260015126.png",alt:""}})]),t._v(" "),e("p",[t._v("同时，我们还可以利用Attention对多任务进行聚焦、解耦（通过attention mask），使得单一模型能够进行多项任务，且均达到比较良好的性能。多任务模型，可以通过Attention对feature进行权重再分配，聚焦各自关键特征。在图像分割论文***Fully Convolutional Network with Task Partitioning for Inshore Ship Detection in Optical Remote Sensing Images***中:")]),t._v(" "),e("p",[t._v("针对靠岸舰船，本文通过任务解耦的方法来处理。因为高层特征表达能力强，分类更准，但定位不准；底层低位准，但分类不准。为了应对这一问题，本文利用一个深层网络得到一个粗糙的分割结果图（船头/船尾、船身、海洋和陆地分别是一类）即Attention Map；利用一个浅层网络得到船头/船尾预测图，位置比较准，但是有很多虚景。训练中，使用Attention Map对浅层网络的loss进行引导，只反传在粗的船头/船尾位置上的loss，其他地方的loss不反传。相当于，"),e("strong",[t._v("深层的网络能得到一个船头/船尾的大概位置，然后浅层网络只需要关注这些大概位置，然后预测出精细的位置，图像中的其他部分（如船身、海洋和陆地）都不关注，从而降低了学习的难度。")])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260017775.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"attention-计算原理"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#attention-计算原理"}},[t._v("#")]),t._v(" Attention 计算原理")]),t._v(" "),e("p",[t._v("$Attention(Q,K,V)=softmax(score({Q, K})V$")]),t._v(" "),e("p",[t._v("Google 2017年论文"),e("a",{attrs:{href:"w"}},[t._v("Attention is All you need")]),t._v("中，为Attention做了一个抽象定义：")]),t._v(" "),e("blockquote",[e("p",[t._v("An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.")]),t._v(" "),e("p",[t._v("注意力是将一个查询和键值对映射到输出的方法，Q、K、V均为向量，输出通过对V进行加权求和得到，权重就是Q、K相似度。")])]),t._v(" "),e("hr"),t._v(" "),e("p",[t._v("这是一个非常General的定义，我们举例说明一下QKV分别代表什么，在机器翻译中，Encoder 编码的信息是我们原始语义信息（V），Decoder的上一步输出的hidden state 定义的当前正在翻译的内容，是我们需要处理的问题（Q），此外我们还需要构造一个信息门牌（K），通过（Q）计算每个门牌的权重，用于给原始语义信息（V)做加权。")]),t._v(" "),e("p",[t._v("计算Attention Weighted Value有三个步骤：")]),t._v(" "),e("ol",[e("li",[t._v("首先的到Q、K、V （在不同任务中，QKV需要用不同方式去构造）")]),t._v(" "),e("li",[t._v("计算Q、K相似度")]),t._v(" "),e("li",[t._v("将相似度进行Softmax处理，得到权重")]),t._v(" "),e("li",[t._v("根据权重对V进行加权")])]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260019652.png",alt:""}})]),t._v(" "),e("p",[t._v("而在整个过程中，最关键的是如何构造Q、K、V，然后是如何选择 "),e("code",[t._v("score")]),t._v("函数去计算相似度，这里先用几个案例讲解一下如何的到Q、K、V及score function。")]),t._v(" "),e("h1",{attrs:{id:"文本分类-attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#文本分类-attention"}},[t._v("#")]),t._v(" 文本分类&Attention")]),t._v(" "),e("p",[t._v("论文《Understanding Attention for Text Classification》中使用了一种比较简单且广泛使用的Q、K、V构造方式。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260023739.png",alt:""}})]),t._v(" "),e("p",[t._v("输入文本长度为n，经过LSTM后得到每一步的hidden state 向量 $h_i$，这个向量同时看成K和V，查询向量Q使用一个可学习的模型参数向量W表示，并且使用dot product （点积运算）作为"),e("code",[t._v("score function")]),t._v("，这样就可以计算每一个K的score：")]),t._v(" "),e("p",[t._v("$$ score_i = \\frac{h_i^{T} W}{\\gamma} $$")]),t._v(" "),e("p",[t._v("其中，$\\gamma$是一个比较大的常数，用于对点积结果做放缩，使得最后softmax后的权重更加平滑，一般情况下我们可以设置成K向量维度大小的根号 $\\sqrt{d_K}$。")]),t._v(" "),e("p",[t._v("$$ weight = softmax(score) $$")]),t._v(" "),e("p",[t._v("其中每个分量按照softmax的到 $weight_i = \\frac{e^{score_i}}{\\sum{e^{score_j}}}$，这样我们就的到了Attention权重，只需要用其对V做加权求和就可以了。")]),t._v(" "),e("p",[t._v("将分类Attention权重可视化可以发现，重点的单词被给与了更高的权重，PAD特殊字符几乎0权重。")]),t._v(" "),e("h1",{attrs:{id:"transformer-with-selfattention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer-with-selfattention"}},[t._v("#")]),t._v(" Transformer with SelfAttention")]),t._v(" "),e("p",[t._v("《Attention is All You Need》是一个里程碑式的论文，提出了一种完全基于注意力机制的新模型结构 - Transformer，在NLP/CV/序列等大量领域均取得突破。")]),t._v(" "),e("h2",{attrs:{id:"transformer基础结构"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer基础结构"}},[t._v("#")]),t._v(" Transformer基础结构")]),t._v(" "),e("p",[t._v("Transformer采用Encoder-Decoder架构，下图中左侧为encoder,右侧为decoder，Transformer encoder和decoder中使用3种网络结构：全连接层、多头自注意力层和LayerNorm层组成TransformerLayer，再对Transformer Layer堆叠N次。")]),t._v(" "),e("h2",{attrs:{id:"layernorm"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#layernorm"}},[t._v("#")]),t._v(" LayerNorm")]),t._v(" "),e("p",[t._v("Batch Normalization 的处理对象是对一批样本， Layer Normalization 的处理对象是单个样本。Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。在Pytorch中，torch.nn.LayerNorm 实现了这个方法。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260025035.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"多头自注意力机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#多头自注意力机制"}},[t._v("#")]),t._v(" 多头自注意力机制")]),t._v(" "),e("p",[t._v("重点是自注意力（Self Attention）。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260033577.png",alt:""}})]),t._v(" "),e("p",[t._v("根据前面注意力机制的描述，我们只需要构造Q、K、V，可以通过点积计算相似度获得Attention 权重。而self-attention的特殊指出就在于， Q、K、V都来自输入本身！我们只需要构造3个线性映射$W_Q$、$W_K$、$W_V$，将原始输入映射成Q、K、V，就可以计算Attention了。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260034565.png",alt:""}})]),t._v(" "),e("p",[t._v("多个自注意力机制输出结果合并，再经过一个线性映射$W_O$，就是多头自注意力输出结果了。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260035891.png",alt:""}})]),t._v(" "),e("h2",{attrs:{id:"transformer-layer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer-layer"}},[t._v("#")]),t._v(" Transformer Layer")]),t._v(" "),e("p",[t._v("一个完整的Transformer Layer就是由全链接层、多头自注意力层及LayerNorm层构成的，具体结构如下图。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209260035690.png",alt:""}})]),t._v(" "),e("p",[t._v("需要注意的是，Transformer Layer 输入和输出的时序长度总是横等的，但是由于自注意力机制的存在，每个时间步都包含了其他时间步的所有信息，因此利用Transformer结构来做一些分类回归单步回归的问题时候，我们通常只会使用首个输出或者最后一步的输出结果，加入一个分类或者回归层来实现。PyTorch已经实现了Transformer Layer，我们来看看：")]),t._v(" "),e("div",{staticClass:"language-python line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" nn\n\ntransformer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("TransformerEncoderLayer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    d_model"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("36")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入特征维度size")]),t._v("\n    nhead"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("   "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 多头数量")]),t._v("\n    batch_first"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 类似LSTM/RNN的参数，是否设置地一个维度为batch size维度")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模拟一个输入")]),t._v("\nx "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rand"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("36")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\noutput "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" transformer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# torch.Size([4, 12, 36])")]),t._v("\n")])]),t._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[t._v("1")]),e("br"),e("span",{staticClass:"line-number"},[t._v("2")]),e("br"),e("span",{staticClass:"line-number"},[t._v("3")]),e("br"),e("span",{staticClass:"line-number"},[t._v("4")]),e("br"),e("span",{staticClass:"line-number"},[t._v("5")]),e("br"),e("span",{staticClass:"line-number"},[t._v("6")]),e("br"),e("span",{staticClass:"line-number"},[t._v("7")]),e("br"),e("span",{staticClass:"line-number"},[t._v("8")]),e("br"),e("span",{staticClass:"line-number"},[t._v("9")]),e("br"),e("span",{staticClass:"line-number"},[t._v("10")]),e("br"),e("span",{staticClass:"line-number"},[t._v("11")]),e("br"),e("span",{staticClass:"line-number"},[t._v("12")]),e("br"),e("span",{staticClass:"line-number"},[t._v("13")]),e("br"),e("span",{staticClass:"line-number"},[t._v("14")]),e("br"),e("span",{staticClass:"line-number"},[t._v("15")]),e("br")])]),e("h3",{attrs:{id:"paper"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#paper"}},[t._v("#")]),t._v(" Paper")]),t._v(" "),e("ol",[e("li",[e("a",{attrs:{href:"https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Hierarchical Attention Networks for Document Classification"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer"}},[t._v("Attention Is All You Need"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:""}},[t._v("Neural Machine Translation by Jointly Learning to Align and Translate")])]),t._v(" "),e("li",[e("a",{attrs:{href:""}},[t._v("Show, Attend and Tell: Neural Image Caption Generation with Visual Attention")])]),t._v(" "),e("li",[e("a",{attrs:{href:"ww"}},[t._v("Fully Convolutional Network with Task Partitioning for Inshore Ship Detection in Optical Remote Sensing Images")])]),t._v(" "),e("li",[e("a",{attrs:{href:""}},[t._v("Effective Approaches to Attention-based Neural Machine Translation")])]),t._v(" "),e("li",[e("a",{attrs:{href:"https://link.springer.com/content/pdf/10.1007/s41095-022-0271-y.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Vision Attention Survey"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Convolutional Block Attention Module"),e("OutboundLink")],1)])]),t._v(" "),e("h3",{attrs:{id:"github"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#github"}},[t._v("#")]),t._v(" github")]),t._v(" "),e("ol",[e("li",[e("a",{attrs:{href:"https://github.com/thomlake/pytorch-attention",target:"_blank",rel:"noopener noreferrer"}},[t._v("pytorch-attention"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://github.com/keon/seq2seq",target:"_blank",rel:"noopener noreferrer"}},[t._v("seq2seq"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://github.com/AuCson/PyTorch-Batch-Attention-Seq2seq",target:"_blank",rel:"noopener noreferrer"}},[t._v("PyTorch-Batch-Attention-Seq2seq"),e("OutboundLink")],1)])]),t._v(" "),e("h3",{attrs:{id:"blog"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#blog"}},[t._v("#")]),t._v(" Blog")]),t._v(" "),e("ol",[e("li",[e("a",{attrs:{href:"https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247486960&idx=1&sn=1b4b9d7ec7a9f40fa8a9df6b6f53bbfb&chksm=96e9d270a19e5b668875392da1d1aaa28ffd0af17d44f7ee81c2754c78cc35edf2e35be2c6a1&scene=21#wechat_redirect",target:"_blank",rel:"noopener noreferrer"}},[t._v("一文读懂「Attention is All You Need」| 附代码实现 "),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://blog.csdn.net/bvl10101111/article/details/78470716",target:"_blank",rel:"noopener noreferrer"}},[t._v("Attention Model（mechanism） 的 套路"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://blog.csdn.net/yideqianfenzhiyi/article/details/79422857",target:"_blank",rel:"noopener noreferrer"}},[t._v("【计算机视觉】深入理解Attention机制"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://www.cnblogs.com/robert-dlut/p/8638283.html%5D",target:"_blank",rel:"noopener noreferrer"}},[t._v("自然语言处理中的自注意力机制"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://blog.csdn.net/u014595019/article/details/52826423",target:"_blank",rel:"noopener noreferrer"}},[t._v("Encoder-Decoder模型和Attention模型"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/102035273",target:"_blank",rel:"noopener noreferrer"}},[t._v("CBAM"),e("OutboundLink")],1)])])])}),[],!1,null,null,null);e.default=n.exports}}]);