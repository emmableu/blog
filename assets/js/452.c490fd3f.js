(window.webpackJsonp=window.webpackJsonp||[]).push([[452],{830:function(e,t,a){"use strict";a.r(t);var s=a(9),i=Object(s.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"definition"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#definition"}},[e._v("#")]),e._v(" Definition")]),e._v(" "),t("p",[e._v("gradient boosting:\n- A training algorithm where weak models are trained to iteratively improve the quality (reduce the loss) of a strong model. For example, a weak model could be a linear or small decision tree model. The strong model becomes the sum of all the previously trained weak models.\n- at each iteration, a weak model is trained to predict the loss gradient of the strong model. Then, the strong model's output is updated by subtracting the predicted gradient, similar to gradient descent")]),e._v(" "),t("h2",{attrs:{id:"gradient-boost-过程"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gradient-boost-过程"}},[e._v("#")]),e._v(" gradient boost 过程")]),e._v(" "),t("p",[e._v("Suppose, we were trying to predict the price of a house given their age, square footage and location.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302210359.png",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"step-1-calculate-the-average-of-the-target-label"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-1-calculate-the-average-of-the-target-label"}},[e._v("#")]),e._v(" Step 1: Calculate the average of the target label")]),e._v(" "),t("p",[e._v("When tackling regression problems, we start with a leaf that is the average value of the variable we want to predict. This leaf will be used as a baseline to approach the correct solution in the proceeding steps.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302210774.png",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"step-2-calculate-the-residuals"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-2-calculate-the-residuals"}},[e._v("#")]),e._v(" Step 2: Calculate the residuals")]),e._v(" "),t("p",[e._v("For every sample, we calculate the residual with the proceeding formula.")]),e._v(" "),t("blockquote",[t("p",[e._v("residual = actual value – predicted value")])]),e._v(" "),t("p",[e._v("In our example, the predicted value is the equal to the mean calculated in the previous step and the actual value can be found in the price column of each sample. After computing the residuals, we get the following table.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302211021.png",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"step-3-construct-a-decision-tree"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-3-construct-a-decision-tree"}},[e._v("#")]),e._v(" Step 3: Construct a decision tree")]),e._v(" "),t("p",[e._v("Next, we build a tree with the goal of predicting the residuals. In other words, every leaf will contain a prediction as to the value of the residual (not the desired label).")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302213623.png",alt:""}})]),e._v(" "),t("p",[e._v("In the event there are more residuals than leaves, some residuals will end up inside the same leaf. When this happens, we compute their average and place that inside the leaf.")]),e._v(" "),t("p",[e._v("Thus, the tree becomes:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302213246.png",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"step-4-predict-the-target-label-using-all-of-the-trees-within-the-ensemble"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-4-predict-the-target-label-using-all-of-the-trees-within-the-ensemble"}},[e._v("#")]),e._v(" Step 4: Predict the target label using all of the trees within the ensemble")]),e._v(" "),t("p",[e._v("Each sample passes through the decision nodes of the newly formed tree until it reaches a given lead. The residual in said leaf is used to predict the house price.")]),e._v(" "),t("p",[e._v("例如，对左下角的这个node，就是这样算：")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302220441.png",alt:""}})]),e._v(" "),t("p",[e._v("或者中间的这个node：")]),e._v(" "),t("p",[e._v("predicted_price = 688 + 0.1 * (-208) = 667.2")]),e._v(" "),t("h3",{attrs:{id:"step-5-compute-the-new-residuals"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-5-compute-the-new-residuals"}},[e._v("#")]),e._v(" Step 5: Compute the new residuals")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302301353.png",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"step-6-repeat-steps-3-5-until-the-number-of-iterations-matches-the-number-specified-by-the-hyperparameter-i-e-iterations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-6-repeat-steps-3-5-until-the-number-of-iterations-matches-the-number-specified-by-the-hyperparameter-i-e-iterations"}},[e._v("#")]),e._v(" Step 6: Repeat steps 3 - 5 until the number of iterations matches the number specified by the hyperparameter (i.e., # iterations)")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302223105.png",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"step-7-once-trained-use-all-of-the-trees-in-the-ensemble-to-make-a-final-prediction-as-to-the-value-of-the-target-variable"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-7-once-trained-use-all-of-the-trees-in-the-ensemble-to-make-a-final-prediction-as-to-the-value-of-the-target-variable"}},[e._v("#")]),e._v(" Step 7: Once trained, use all of the trees in the ensemble to make a final prediction as to the value of the target variable")]),e._v(" "),t("p",[e._v("The final prediction will be equal to the mean we computed in the first step, plus all of the residuals predicted by the trees that make up the forest multiplied by the learning rate.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302223078.png",alt:""}})]),e._v(" "),t("h2",{attrs:{id:"xgboost-step-by-step"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#xgboost-step-by-step"}},[e._v("#")]),e._v(" XGBoost step by step")]),e._v(" "),t("p",[e._v("Let’s start with our training dataset which consists of five people. We recorded their ages, whether or not they have a master’s degree, and their salary (in thousands). Our goal is to predict "),t("em",[e._v("Salary")]),e._v(" using the XGBoost Algorithm.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*_KUzdQTdTUfjPposrAxSfQ.png",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"step-1-make-an-initial-prediction-and-calculate-residuals"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-1-make-an-initial-prediction-and-calculate-residuals"}},[e._v("#")]),e._v(" Step 1: Make an Initial Prediction and Calculate Residuals")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://towardsdatascience.com/xgboost-regression-explain-it-to-me-like-im-10-2cf324b0bbdb",target:"_blank",rel:"noopener noreferrer"}},[e._v("source"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("This prediction can be anything. But let’s assume our initial prediction is the average value of the variables we want to predict.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*GjxcMQDQQqU-UfgfvBeb1A.png",alt:""}})]),e._v(" "),t("p",[e._v("We can calculate residuals using the following formula:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*AEwarDLHuDQZmMlB-2VDpw.png",alt:""}})]),e._v(" "),t("p",[e._v("Here, our Observed Values are the values in the "),t("em",[e._v("Salary")]),e._v(" column and all Predicted Values are equal to 70 because that is what we chose our initial prediction to be.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*D5eCwr_TYcwuDP8NBOAOsg.png",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"step-2-build-an-xgboost-tree"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-2-build-an-xgboost-tree"}},[e._v("#")]),e._v(" Step 2: Build an XGBoost Tree")]),e._v(" "),t("p",[e._v("Each tree starts with a single leaf and all the residuals go into that leaf.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1036/1*yYq5mmVkYk1Lwl_aspUq6A.png",alt:""}})]),e._v(" "),t("p",[e._v("Now we need to calculate something called a "),t("strong",[e._v("Similarity Score")]),e._v(" of this leaf.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*ddxctrOTVbqPhppkcxCzFw.png",alt:""}})]),e._v(" "),t("p",[e._v("λ (lambda) is a regularization parameter that reduces the prediction’s sensitivity to individual observations and prevents the overfitting of data (this is when a model fits exactly against the training dataset). The default value of λ is 1 so we will let λ = 1 in this example.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*3bc39LtxpRTqQ0kivyrQlw.png",alt:""}})]),e._v(" "),t("p",[e._v("Now we should see if we can do a better job clustering the residuals if we split them into two groups using thresholds based on our predictors — "),t("em",[e._v("Age")]),e._v(" and "),t("em",[e._v("Master’s Degree?.")]),e._v(" Splitting the "),t("em",[e._v("Residuals")]),e._v(" basically means that we are adding branches to our tree.")]),e._v(" "),t("p",[e._v("First, let’s try splitting the leaf using "),t("em",[e._v("Master’s Degree")]),t("strong",[e._v("?")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*iofkdqDDgFDhOHRxjmjz5w.png",alt:""}})]),e._v(" "),t("p",[e._v("And then calculate the "),t("strong",[e._v("Similarity Scores")]),e._v(" for the left and right leaves of the above split:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*rNlX9qaQqc8xGSUXvkwkKg.png",alt:""}})]),e._v(" "),t("p",[e._v("Now we need to quantify how much better the leaves cluster similar "),t("em",[e._v("Residuals")]),e._v(" than the root does. We can do this by calculating the "),t("strong",[e._v("Gain")]),e._v(" of splitting the "),t("em",[e._v("Residuals")]),e._v(" into two groups. If the "),t("strong",[e._v("Gain")]),e._v(" is positive, then it’s a good idea to split, otherwise, it is not.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1222/0*rIeNeoesHLox_5p8.png",alt:""}})]),e._v(" "),t("p",[e._v("Then we compare this "),t("strong",[e._v("Gain")]),e._v(" to those of the splits in "),t("em",[e._v("Age")]),e._v(". Since "),t("em",[e._v("Age")]),e._v(" is a continuous variable, the process to find the different splits is a little more involved. First, we arrange the rows of our dataset according to the ascending order of "),t("em",[e._v("Age")]),e._v(". Then we calculate the average values of the adjacent values in "),t("em",[e._v("Age")]),e._v(".")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/550/0*JruyoPHXdM9LxPPW",alt:""}})]),e._v(" "),t("p",[e._v("Now we split the "),t("em",[e._v("Residuals")]),e._v(" using the four averages as thresholds and calculate "),t("strong",[e._v("Gain")]),e._v(" for each of the splits. The first split uses "),t("em",[e._v("Age < 23.5")]),e._v(":")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1222/1*4285F3yPyqgvkDf0a3JtJA.png",alt:""}})]),e._v(" "),t("p",[e._v("For this split, we find the "),t("strong",[e._v("Similarity Score")]),e._v(" and "),t("strong",[e._v("Gain")]),e._v(" the same way we did for "),t("em",[e._v("Master’s Degree?")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*QG-8M5rLc_WFo293kdn32Q.png",alt:""}})]),e._v(" "),t("p",[e._v("Do the same thing for the rest of the "),t("em",[e._v("Age")]),e._v(" splits:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*HrG-WfVT9O5ve7Yso3dL9g.png",alt:""}})]),e._v(" "),t("p",[e._v("Out of the one "),t("em",[e._v("Mater’s Degree?")]),e._v(" split and four "),t("em",[e._v("Age")]),e._v(" splits, the "),t("em",[e._v("Master’s Degree")]),e._v(" split has the greatest "),t("strong",[e._v("Gain")]),e._v(" value, so we’ll use that as our initial split. Now we can add more branches to the tree by splitting our "),t("em",[e._v("Master’s Degree?")]),e._v(" leaves again using the same process described above. But, only this time, we use the initial "),t("em",[e._v("Master’s Degree?")]),e._v(" leaves as our root nodes and try splitting them by getting the greatest "),t("strong",[e._v("Gain")]),e._v(" value that is greater than 0.")]),e._v(" "),t("p",[e._v("Let’s start with the left node. For this node, we only consider the observations that have the value ‘Yes’ in "),t("em",[e._v("Master’s Degree?")]),e._v(" because only those observations land in the left node.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*ZaDD16j0hBb3kIp4iOgPcA.png",alt:""}})]),e._v(" "),t("p",[e._v("So we calculate the "),t("strong",[e._v("Gain")]),e._v(" of the "),t("em",[e._v("Age")]),e._v(" splits using the same process as before, but this time using the "),t("em",[e._v("Residuals")]),e._v(" in the highlighted rows only.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*ArdKYPYzu9oRcJCsoOgOKQ.png",alt:""}})]),e._v(" "),t("p",[e._v("Since only "),t("em",[e._v("Age < 25")]),e._v(" gives us a positive "),t("strong",[e._v("Gain")]),e._v(", we split the left node using this threshold. Moving onto our right node, we only look at values with ‘No’ values in "),t("em",[e._v("Master’s Degree?")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*wT0cScVpEqPSa4OAJSuJLg.png",alt:""}})]),e._v(" "),t("p",[e._v("We only have two observations in our right node, so the only split possible is "),t("em",[e._v("Age < 24.5")]),e._v(" because that is the average of the two "),t("em",[e._v("Age")]),e._v(" values in the highlighted rows.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*I4xa7BIP1YcZOyl_K1eBEg.png",alt:""}})]),e._v(" "),t("p",[e._v("The "),t("strong",[e._v("Gain")]),e._v(" of this split is positive, so our final tree is:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1298/1*LOq80DzUJSo3ndw1P9PoAQ.png",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"step-3-prune-the-tree"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-3-prune-the-tree"}},[e._v("#")]),e._v(" "),t("strong",[t("em",[e._v("Step 3: Prune the Tree")])])]),e._v(" "),t("p",[e._v("Pruning is another way we can avoid overfitting the data. To do this we start from the bottom of our tree and work our way up to see if a split is valid or not. To establish validity, we use γ (gamma). If "),t("strong",[e._v("Gain —")]),e._v(" γ is positive then we keep the split, otherwise, we remove it. The default value of γ is 0, but for illustrative purposes, let’s set our γ to 50. From previous calculations we know the "),t("strong",[e._v("Gain")]),e._v(" values:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*I1ukSNGxqbB1tAHZBqw7qA.png",alt:""}})]),e._v(" "),t("p",[e._v("Since "),t("strong",[e._v("Gain —")]),e._v(" γ is positive for all splits except that of "),t("em",[e._v("Age < 24.5")]),e._v(", we can remove that branch. So the resulting tree is:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1168/1*n0b6WmYcJXw4UCpy0DCGRA.png",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"step-4-calculate-the-output-values-of-leaves"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-4-calculate-the-output-values-of-leaves"}},[e._v("#")]),e._v(" "),t("strong",[t("em",[e._v("Step 4: Calculate the Output Values of Leaves")])])]),e._v(" "),t("p",[e._v("We are almost there! All we have to do now is calculate a single value in our leaf nodes because we can not have a leaf node giving us multiple outputs.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/0*XmYMrpfsKZF6XWOb",alt:""}})]),e._v(" "),t("p",[e._v("This is similar to the formula to calculate "),t("strong",[e._v("Similarity Score")]),e._v(" except we are not squaring the "),t("em",[e._v("Residuals")]),e._v(". Using the formula and λ = 1, "),t("em",[t("em",[e._v("drum roll")])]),e._v(" our final tree is:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*FSIFRB1N8bR52vixSWR19A.png",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"step-5-make-new-predictions"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-5-make-new-predictions"}},[e._v("#")]),e._v(" "),t("strong",[t("em",[e._v("Step 5: Make New Predictions")])])]),e._v(" "),t("p",[e._v("Now that all that hard model building is behind us, we come to the exciting part and see how much our predictions improve using our new model. We can make predictions using this formula:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*tIEzTeLj_yHOIQWVVqPQ9w.png",alt:""}})]),e._v(" "),t("p",[e._v("The XGBoost Learning Rate is ɛ (eta) and the default value is 0.3. So the predicted value of our first observation will be:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*EYwsm84uQB4-WNQfezfwCA.png",alt:""}})]),e._v(" "),t("p",[e._v("Similarly, we can calculate the rest of the predicted values:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*7tDf4x_6I-WehIG2yI7BMw.png",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"step-6-calculate-residuals-using-the-new-predictions"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-6-calculate-residuals-using-the-new-predictions"}},[e._v("#")]),e._v(" "),t("strong",[t("em",[e._v("Step 6: Calculate Residuals Using the New Predictions")])])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*tIqXoaKD_41QeCBfe2xxqQ.png",alt:""}})]),e._v(" "),t("p",[e._v("We see that the new "),t("em",[e._v("Residuals")]),e._v(" are smaller than the ones before, this indicates that we’ve taken a small step in the right direction. As we repeat this process, our "),t("em",[e._v("Residuals")]),e._v(" will get smaller and smaller indicating that our predicted values are getting closer to the observed values.")]),e._v(" "),t("h3",{attrs:{id:"step-7-repeat-steps-2-6"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#step-7-repeat-steps-2-6"}},[e._v("#")]),e._v(" "),t("strong",[t("em",[e._v("Step 7: Repeat Steps 2–6")])])]),e._v(" "),t("p",[e._v("Now we just repeat the same process over and over again, building a new tree, making predictions, and calculating "),t("em",[e._v("Residuals")]),e._v(" at each iteration. We do this until the "),t("em",[e._v("Residuals")]),e._v(" are super small or we reached the maximum number of iterations we set for our algorithm. If the tree we built at each iteration is indicated by Tᵢ, where "),t("em",[e._v("i")]),e._v(" is the current iteration, then the formula to calculate predictions is:")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://miro.medium.com/max/1400/1*MiHVOymdS80S8o_uv96Ubw.png",alt:""}})]),e._v(" "),t("p",[e._v("And that’s it. Thanks for reading and good luck with the rest of your algorithmic journey!")])])}),[],!1,null,null,null);t.default=i.exports}}]);