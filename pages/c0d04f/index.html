<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Makemore 3 - Activations &amp; Gradients, BatchNorm | Emma&#39;s Blog</title>
    <meta name="generator" content="VuePress 1.9.9">
    <link rel="icon" href="/blog/img/favicon.ico">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
    <meta name="description" content="blog">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/blog/assets/css/0.styles.f2b65211.css" as="style"><link rel="preload" href="/blog/assets/js/app.fa6bfa40.js" as="script"><link rel="preload" href="/blog/assets/js/2.7ce49225.js" as="script"><link rel="preload" href="/blog/assets/js/423.5502a762.js" as="script"><link rel="prefetch" href="/blog/assets/js/10.9b4fdd47.js"><link rel="prefetch" href="/blog/assets/js/100.e7eefa33.js"><link rel="prefetch" href="/blog/assets/js/101.229b595f.js"><link rel="prefetch" href="/blog/assets/js/102.0869e27b.js"><link rel="prefetch" href="/blog/assets/js/103.67051ee3.js"><link rel="prefetch" href="/blog/assets/js/104.5e9ec206.js"><link rel="prefetch" href="/blog/assets/js/105.812b7b15.js"><link rel="prefetch" href="/blog/assets/js/106.d2afd06e.js"><link rel="prefetch" href="/blog/assets/js/107.0e992a28.js"><link rel="prefetch" href="/blog/assets/js/108.d7af612a.js"><link rel="prefetch" href="/blog/assets/js/109.0511ab99.js"><link rel="prefetch" href="/blog/assets/js/11.2658286a.js"><link rel="prefetch" href="/blog/assets/js/110.66eb4fe8.js"><link rel="prefetch" href="/blog/assets/js/111.c4a3506e.js"><link rel="prefetch" href="/blog/assets/js/112.96ad5e97.js"><link rel="prefetch" href="/blog/assets/js/113.d898d02f.js"><link rel="prefetch" href="/blog/assets/js/114.d26fe027.js"><link rel="prefetch" href="/blog/assets/js/115.adbc8f5f.js"><link rel="prefetch" href="/blog/assets/js/116.1932a92b.js"><link rel="prefetch" href="/blog/assets/js/117.969e94f6.js"><link rel="prefetch" href="/blog/assets/js/118.8a40c4b7.js"><link rel="prefetch" href="/blog/assets/js/119.4d5ac55d.js"><link rel="prefetch" href="/blog/assets/js/12.e397a550.js"><link rel="prefetch" href="/blog/assets/js/120.1ea31920.js"><link rel="prefetch" href="/blog/assets/js/121.0f8bbe64.js"><link rel="prefetch" href="/blog/assets/js/122.62cb2b98.js"><link rel="prefetch" href="/blog/assets/js/123.36fb89d8.js"><link rel="prefetch" href="/blog/assets/js/124.51e9f053.js"><link rel="prefetch" href="/blog/assets/js/125.707afbcb.js"><link rel="prefetch" href="/blog/assets/js/126.3f936437.js"><link rel="prefetch" href="/blog/assets/js/127.63ddddea.js"><link rel="prefetch" href="/blog/assets/js/128.45eedbf7.js"><link rel="prefetch" href="/blog/assets/js/129.309b8006.js"><link rel="prefetch" href="/blog/assets/js/13.19ccc9bf.js"><link rel="prefetch" href="/blog/assets/js/130.341188a0.js"><link rel="prefetch" href="/blog/assets/js/131.e6979d87.js"><link rel="prefetch" href="/blog/assets/js/132.162f434b.js"><link rel="prefetch" href="/blog/assets/js/133.3dbe6586.js"><link rel="prefetch" href="/blog/assets/js/134.020e2147.js"><link rel="prefetch" href="/blog/assets/js/135.c830889a.js"><link rel="prefetch" href="/blog/assets/js/136.1f295f91.js"><link rel="prefetch" href="/blog/assets/js/137.e7ff1cf2.js"><link rel="prefetch" href="/blog/assets/js/138.926cd296.js"><link rel="prefetch" href="/blog/assets/js/139.d8973f4f.js"><link rel="prefetch" href="/blog/assets/js/14.3973f32e.js"><link rel="prefetch" href="/blog/assets/js/140.3f410d2a.js"><link rel="prefetch" href="/blog/assets/js/141.29a6ec98.js"><link rel="prefetch" href="/blog/assets/js/142.76cee5a1.js"><link rel="prefetch" href="/blog/assets/js/143.1e68adfd.js"><link rel="prefetch" href="/blog/assets/js/144.0e90fa28.js"><link rel="prefetch" href="/blog/assets/js/145.b461e3eb.js"><link rel="prefetch" href="/blog/assets/js/146.0efa1a6d.js"><link rel="prefetch" href="/blog/assets/js/147.6cb2ae59.js"><link rel="prefetch" href="/blog/assets/js/148.7f9717b8.js"><link rel="prefetch" href="/blog/assets/js/149.b70d37e3.js"><link rel="prefetch" href="/blog/assets/js/15.60c21287.js"><link rel="prefetch" href="/blog/assets/js/150.ef9c5330.js"><link rel="prefetch" href="/blog/assets/js/151.495f2fe3.js"><link rel="prefetch" href="/blog/assets/js/152.399484c7.js"><link rel="prefetch" href="/blog/assets/js/153.b3f76d44.js"><link rel="prefetch" href="/blog/assets/js/154.9b4bcee9.js"><link rel="prefetch" href="/blog/assets/js/155.86d30484.js"><link rel="prefetch" href="/blog/assets/js/156.69309282.js"><link rel="prefetch" href="/blog/assets/js/157.50628274.js"><link rel="prefetch" href="/blog/assets/js/158.6201075e.js"><link rel="prefetch" href="/blog/assets/js/159.6b236af8.js"><link rel="prefetch" href="/blog/assets/js/16.f60df41a.js"><link rel="prefetch" href="/blog/assets/js/160.7cef879a.js"><link rel="prefetch" href="/blog/assets/js/161.e069cc00.js"><link rel="prefetch" href="/blog/assets/js/162.c840bba7.js"><link rel="prefetch" href="/blog/assets/js/163.d92fc034.js"><link rel="prefetch" href="/blog/assets/js/164.1654d287.js"><link rel="prefetch" href="/blog/assets/js/165.488a29ef.js"><link rel="prefetch" href="/blog/assets/js/166.f45211e9.js"><link rel="prefetch" href="/blog/assets/js/167.7c7dab06.js"><link rel="prefetch" href="/blog/assets/js/168.5487e398.js"><link rel="prefetch" href="/blog/assets/js/169.d33859fc.js"><link rel="prefetch" href="/blog/assets/js/17.e4f94e71.js"><link rel="prefetch" href="/blog/assets/js/170.8138d2a9.js"><link rel="prefetch" href="/blog/assets/js/171.f9803ede.js"><link rel="prefetch" href="/blog/assets/js/172.d083e11e.js"><link rel="prefetch" href="/blog/assets/js/173.8bc9a3da.js"><link rel="prefetch" href="/blog/assets/js/174.7bdf84df.js"><link rel="prefetch" href="/blog/assets/js/175.662c4a7d.js"><link rel="prefetch" href="/blog/assets/js/176.7501abe7.js"><link rel="prefetch" href="/blog/assets/js/177.27a6ea08.js"><link rel="prefetch" href="/blog/assets/js/178.542a2e15.js"><link rel="prefetch" href="/blog/assets/js/179.bd90a759.js"><link rel="prefetch" href="/blog/assets/js/18.ce8d611d.js"><link rel="prefetch" href="/blog/assets/js/180.bad5b48f.js"><link rel="prefetch" href="/blog/assets/js/181.75204ef2.js"><link rel="prefetch" href="/blog/assets/js/182.07c3b582.js"><link rel="prefetch" href="/blog/assets/js/183.684e4590.js"><link rel="prefetch" href="/blog/assets/js/184.1ddc109c.js"><link rel="prefetch" href="/blog/assets/js/185.23eefeba.js"><link rel="prefetch" href="/blog/assets/js/186.5023b10a.js"><link rel="prefetch" href="/blog/assets/js/187.82cdd79b.js"><link rel="prefetch" href="/blog/assets/js/188.dfbce339.js"><link rel="prefetch" href="/blog/assets/js/189.3c758dee.js"><link rel="prefetch" href="/blog/assets/js/19.98c0abc0.js"><link rel="prefetch" href="/blog/assets/js/190.b1eff947.js"><link rel="prefetch" href="/blog/assets/js/191.c88913b2.js"><link rel="prefetch" href="/blog/assets/js/192.54f0647d.js"><link rel="prefetch" href="/blog/assets/js/193.54d6343e.js"><link rel="prefetch" href="/blog/assets/js/194.abe54e8a.js"><link rel="prefetch" href="/blog/assets/js/195.099dd0e8.js"><link rel="prefetch" href="/blog/assets/js/196.ab3cc2bc.js"><link rel="prefetch" href="/blog/assets/js/197.a895813f.js"><link rel="prefetch" href="/blog/assets/js/198.94d4bf63.js"><link rel="prefetch" href="/blog/assets/js/199.22ba978c.js"><link rel="prefetch" href="/blog/assets/js/20.1bac6a12.js"><link rel="prefetch" href="/blog/assets/js/200.e0c0c5db.js"><link rel="prefetch" href="/blog/assets/js/201.3decc849.js"><link rel="prefetch" href="/blog/assets/js/202.ddbf28b3.js"><link rel="prefetch" href="/blog/assets/js/203.2397895a.js"><link rel="prefetch" href="/blog/assets/js/204.5f519032.js"><link rel="prefetch" href="/blog/assets/js/205.0f9caae2.js"><link rel="prefetch" href="/blog/assets/js/206.1f6339b7.js"><link rel="prefetch" href="/blog/assets/js/207.3757cf50.js"><link rel="prefetch" href="/blog/assets/js/208.a7f08e37.js"><link rel="prefetch" href="/blog/assets/js/209.c5597e2f.js"><link rel="prefetch" href="/blog/assets/js/21.5d7ecf97.js"><link rel="prefetch" href="/blog/assets/js/210.cba479b0.js"><link rel="prefetch" href="/blog/assets/js/211.0e468dce.js"><link rel="prefetch" href="/blog/assets/js/212.1276f952.js"><link rel="prefetch" href="/blog/assets/js/213.0ab1cbd2.js"><link rel="prefetch" href="/blog/assets/js/214.beab31e6.js"><link rel="prefetch" href="/blog/assets/js/215.85ce8952.js"><link rel="prefetch" href="/blog/assets/js/216.7ae13582.js"><link rel="prefetch" href="/blog/assets/js/217.21c5c4ae.js"><link rel="prefetch" href="/blog/assets/js/218.459846f9.js"><link rel="prefetch" href="/blog/assets/js/219.3ec1a6f1.js"><link rel="prefetch" href="/blog/assets/js/22.2bdcb11a.js"><link rel="prefetch" href="/blog/assets/js/220.0efee18a.js"><link rel="prefetch" href="/blog/assets/js/221.ecd509c6.js"><link rel="prefetch" href="/blog/assets/js/222.068be0e3.js"><link rel="prefetch" href="/blog/assets/js/223.0d7a3417.js"><link rel="prefetch" href="/blog/assets/js/224.e45bfd31.js"><link rel="prefetch" href="/blog/assets/js/225.c94c8651.js"><link rel="prefetch" href="/blog/assets/js/226.2dfa74fd.js"><link rel="prefetch" href="/blog/assets/js/227.38105785.js"><link rel="prefetch" href="/blog/assets/js/228.76ceed84.js"><link rel="prefetch" href="/blog/assets/js/229.54907291.js"><link rel="prefetch" href="/blog/assets/js/23.9bdaa3ab.js"><link rel="prefetch" href="/blog/assets/js/230.bab07940.js"><link rel="prefetch" href="/blog/assets/js/231.15ca6ff3.js"><link rel="prefetch" href="/blog/assets/js/232.22e17e11.js"><link rel="prefetch" href="/blog/assets/js/233.562e078c.js"><link rel="prefetch" href="/blog/assets/js/234.dba05df0.js"><link rel="prefetch" href="/blog/assets/js/235.3e377078.js"><link rel="prefetch" href="/blog/assets/js/236.ee8ef4f7.js"><link rel="prefetch" href="/blog/assets/js/237.342d5a94.js"><link rel="prefetch" href="/blog/assets/js/238.d9a5194f.js"><link rel="prefetch" href="/blog/assets/js/239.3f0ef554.js"><link rel="prefetch" href="/blog/assets/js/24.a091ba45.js"><link rel="prefetch" href="/blog/assets/js/240.73553fb4.js"><link rel="prefetch" href="/blog/assets/js/241.dc725e0b.js"><link rel="prefetch" href="/blog/assets/js/242.a19631f5.js"><link rel="prefetch" href="/blog/assets/js/243.8dad3f92.js"><link rel="prefetch" href="/blog/assets/js/244.1e7085b7.js"><link rel="prefetch" href="/blog/assets/js/245.e32e5959.js"><link rel="prefetch" href="/blog/assets/js/246.abb8f6fc.js"><link rel="prefetch" href="/blog/assets/js/247.40778e7b.js"><link rel="prefetch" href="/blog/assets/js/248.f3129994.js"><link rel="prefetch" href="/blog/assets/js/249.284fbd88.js"><link rel="prefetch" href="/blog/assets/js/25.01fddff0.js"><link rel="prefetch" href="/blog/assets/js/250.e5604caf.js"><link rel="prefetch" href="/blog/assets/js/251.4db1b5d4.js"><link rel="prefetch" href="/blog/assets/js/252.8ee09136.js"><link rel="prefetch" href="/blog/assets/js/253.886ea653.js"><link rel="prefetch" href="/blog/assets/js/254.9570cd86.js"><link rel="prefetch" href="/blog/assets/js/255.3f45535c.js"><link rel="prefetch" href="/blog/assets/js/256.5e65a4d3.js"><link rel="prefetch" href="/blog/assets/js/257.d73448be.js"><link rel="prefetch" href="/blog/assets/js/258.fd2f8dac.js"><link rel="prefetch" href="/blog/assets/js/259.41805386.js"><link rel="prefetch" href="/blog/assets/js/26.ab34c3b9.js"><link rel="prefetch" href="/blog/assets/js/260.940745e5.js"><link rel="prefetch" href="/blog/assets/js/261.4e120cbf.js"><link rel="prefetch" href="/blog/assets/js/262.387220ac.js"><link rel="prefetch" href="/blog/assets/js/263.2ce8ccb8.js"><link rel="prefetch" href="/blog/assets/js/264.f6306429.js"><link rel="prefetch" href="/blog/assets/js/265.f42b5960.js"><link rel="prefetch" href="/blog/assets/js/266.fe571a04.js"><link rel="prefetch" href="/blog/assets/js/267.cdceab63.js"><link rel="prefetch" href="/blog/assets/js/268.e76d12d5.js"><link rel="prefetch" href="/blog/assets/js/269.325b654d.js"><link rel="prefetch" href="/blog/assets/js/27.211e008f.js"><link rel="prefetch" href="/blog/assets/js/270.c5a09ebc.js"><link rel="prefetch" href="/blog/assets/js/271.8582d9ef.js"><link rel="prefetch" href="/blog/assets/js/272.25a996e7.js"><link rel="prefetch" href="/blog/assets/js/273.3cea407a.js"><link rel="prefetch" href="/blog/assets/js/274.e8037b9f.js"><link rel="prefetch" href="/blog/assets/js/275.c24fadc6.js"><link rel="prefetch" href="/blog/assets/js/276.15f6846b.js"><link rel="prefetch" href="/blog/assets/js/277.312bdd0d.js"><link rel="prefetch" href="/blog/assets/js/278.9a3d8365.js"><link rel="prefetch" href="/blog/assets/js/279.f033b4f8.js"><link rel="prefetch" href="/blog/assets/js/28.66852ae6.js"><link rel="prefetch" href="/blog/assets/js/280.35226986.js"><link rel="prefetch" href="/blog/assets/js/281.24522fa7.js"><link rel="prefetch" href="/blog/assets/js/282.2f2bf22d.js"><link rel="prefetch" href="/blog/assets/js/283.7a318e26.js"><link rel="prefetch" href="/blog/assets/js/284.dc2ed524.js"><link rel="prefetch" href="/blog/assets/js/285.0740fa3d.js"><link rel="prefetch" href="/blog/assets/js/286.d800de15.js"><link rel="prefetch" href="/blog/assets/js/287.37d62b42.js"><link rel="prefetch" href="/blog/assets/js/288.9aef5358.js"><link rel="prefetch" href="/blog/assets/js/289.2bf079d7.js"><link rel="prefetch" href="/blog/assets/js/29.141730bc.js"><link rel="prefetch" href="/blog/assets/js/290.23861619.js"><link rel="prefetch" href="/blog/assets/js/291.b90053de.js"><link rel="prefetch" href="/blog/assets/js/292.1ecc1810.js"><link rel="prefetch" href="/blog/assets/js/293.162a2bfd.js"><link rel="prefetch" href="/blog/assets/js/294.7e872ddf.js"><link rel="prefetch" href="/blog/assets/js/295.a67c9dc4.js"><link rel="prefetch" href="/blog/assets/js/296.cd5f1739.js"><link rel="prefetch" href="/blog/assets/js/297.5fba221f.js"><link rel="prefetch" href="/blog/assets/js/298.2698065d.js"><link rel="prefetch" href="/blog/assets/js/299.9ad4582c.js"><link rel="prefetch" href="/blog/assets/js/3.fb6b6218.js"><link rel="prefetch" href="/blog/assets/js/30.057cc531.js"><link rel="prefetch" href="/blog/assets/js/300.52c1803f.js"><link rel="prefetch" href="/blog/assets/js/301.b03a706d.js"><link rel="prefetch" href="/blog/assets/js/302.8ea88eb9.js"><link rel="prefetch" href="/blog/assets/js/303.1502ae39.js"><link rel="prefetch" href="/blog/assets/js/304.f20513d3.js"><link rel="prefetch" href="/blog/assets/js/305.aa35fcf1.js"><link rel="prefetch" href="/blog/assets/js/306.1897421f.js"><link rel="prefetch" href="/blog/assets/js/307.c3d97536.js"><link rel="prefetch" href="/blog/assets/js/308.4f22d76b.js"><link rel="prefetch" href="/blog/assets/js/309.7e7f7c52.js"><link rel="prefetch" href="/blog/assets/js/31.0a0b4625.js"><link rel="prefetch" href="/blog/assets/js/310.2e22d9e1.js"><link rel="prefetch" href="/blog/assets/js/311.974bbf97.js"><link rel="prefetch" href="/blog/assets/js/312.f28c61dc.js"><link rel="prefetch" href="/blog/assets/js/313.3c51e0fa.js"><link rel="prefetch" href="/blog/assets/js/314.d998ea43.js"><link rel="prefetch" href="/blog/assets/js/315.3e9a511f.js"><link rel="prefetch" href="/blog/assets/js/316.5969895b.js"><link rel="prefetch" href="/blog/assets/js/317.fc61c649.js"><link rel="prefetch" href="/blog/assets/js/318.0e38281f.js"><link rel="prefetch" href="/blog/assets/js/319.3c721d92.js"><link rel="prefetch" href="/blog/assets/js/32.ea991562.js"><link rel="prefetch" href="/blog/assets/js/320.b4ab4b4f.js"><link rel="prefetch" href="/blog/assets/js/321.56f69d1a.js"><link rel="prefetch" href="/blog/assets/js/322.20ab832b.js"><link rel="prefetch" href="/blog/assets/js/323.4b2d7ea3.js"><link rel="prefetch" href="/blog/assets/js/324.d81f5847.js"><link rel="prefetch" href="/blog/assets/js/325.7edfab8f.js"><link rel="prefetch" href="/blog/assets/js/326.45f4da77.js"><link rel="prefetch" href="/blog/assets/js/327.ba458f2c.js"><link rel="prefetch" href="/blog/assets/js/328.d86d670d.js"><link rel="prefetch" href="/blog/assets/js/329.f7eac607.js"><link rel="prefetch" href="/blog/assets/js/33.59727a1a.js"><link rel="prefetch" href="/blog/assets/js/330.10d24789.js"><link rel="prefetch" href="/blog/assets/js/331.60d73f78.js"><link rel="prefetch" href="/blog/assets/js/332.6c5b6b35.js"><link rel="prefetch" href="/blog/assets/js/333.7b1b290d.js"><link rel="prefetch" href="/blog/assets/js/334.285526b7.js"><link rel="prefetch" href="/blog/assets/js/335.4780d81e.js"><link rel="prefetch" href="/blog/assets/js/336.5ecc836e.js"><link rel="prefetch" href="/blog/assets/js/337.6fba51e2.js"><link rel="prefetch" href="/blog/assets/js/338.d646ffe0.js"><link rel="prefetch" href="/blog/assets/js/339.45c11835.js"><link rel="prefetch" href="/blog/assets/js/34.e0063628.js"><link rel="prefetch" href="/blog/assets/js/340.f3ae78d4.js"><link rel="prefetch" href="/blog/assets/js/341.0e5e0af1.js"><link rel="prefetch" href="/blog/assets/js/342.5e9536e2.js"><link rel="prefetch" href="/blog/assets/js/343.d2f5b148.js"><link rel="prefetch" href="/blog/assets/js/344.b076b331.js"><link rel="prefetch" href="/blog/assets/js/345.ccb92f9a.js"><link rel="prefetch" href="/blog/assets/js/346.c1d7088c.js"><link rel="prefetch" href="/blog/assets/js/347.76e6b595.js"><link rel="prefetch" href="/blog/assets/js/348.d08fd4eb.js"><link rel="prefetch" href="/blog/assets/js/349.b0633c03.js"><link rel="prefetch" href="/blog/assets/js/35.d02fcd5b.js"><link rel="prefetch" href="/blog/assets/js/350.58bd91c8.js"><link rel="prefetch" href="/blog/assets/js/351.4eeb8781.js"><link rel="prefetch" href="/blog/assets/js/352.92e0273b.js"><link rel="prefetch" href="/blog/assets/js/353.4316b720.js"><link rel="prefetch" href="/blog/assets/js/354.8b1ec3de.js"><link rel="prefetch" href="/blog/assets/js/355.d09b94bf.js"><link rel="prefetch" href="/blog/assets/js/356.5b548a21.js"><link rel="prefetch" href="/blog/assets/js/357.deef470d.js"><link rel="prefetch" href="/blog/assets/js/358.1b8619f3.js"><link rel="prefetch" href="/blog/assets/js/359.62109421.js"><link rel="prefetch" href="/blog/assets/js/36.9a2bf2cb.js"><link rel="prefetch" href="/blog/assets/js/360.b0fbc8bc.js"><link rel="prefetch" href="/blog/assets/js/361.9f4082fe.js"><link rel="prefetch" href="/blog/assets/js/362.998cb7ce.js"><link rel="prefetch" href="/blog/assets/js/363.59416eb7.js"><link rel="prefetch" href="/blog/assets/js/364.673a993e.js"><link rel="prefetch" href="/blog/assets/js/365.2ce18574.js"><link rel="prefetch" href="/blog/assets/js/366.06fb2db3.js"><link rel="prefetch" href="/blog/assets/js/367.82a80c1b.js"><link rel="prefetch" href="/blog/assets/js/368.c6eded31.js"><link rel="prefetch" href="/blog/assets/js/369.cf8dcc59.js"><link rel="prefetch" href="/blog/assets/js/37.14ce705f.js"><link rel="prefetch" href="/blog/assets/js/370.cc5c5295.js"><link rel="prefetch" href="/blog/assets/js/371.03d01383.js"><link rel="prefetch" href="/blog/assets/js/372.84d458d3.js"><link rel="prefetch" href="/blog/assets/js/373.69e36286.js"><link rel="prefetch" href="/blog/assets/js/374.325318a2.js"><link rel="prefetch" href="/blog/assets/js/375.0faf9354.js"><link rel="prefetch" href="/blog/assets/js/376.e383d5f8.js"><link rel="prefetch" href="/blog/assets/js/377.a45039dd.js"><link rel="prefetch" href="/blog/assets/js/378.e6bf1f8a.js"><link rel="prefetch" href="/blog/assets/js/379.ab3908c4.js"><link rel="prefetch" href="/blog/assets/js/38.d8358ff1.js"><link rel="prefetch" href="/blog/assets/js/380.9155ba09.js"><link rel="prefetch" href="/blog/assets/js/381.964c9828.js"><link rel="prefetch" href="/blog/assets/js/382.b2c17375.js"><link rel="prefetch" href="/blog/assets/js/383.f74bb63d.js"><link rel="prefetch" href="/blog/assets/js/384.f46a5f9f.js"><link rel="prefetch" href="/blog/assets/js/385.2b0fad5e.js"><link rel="prefetch" href="/blog/assets/js/386.b7fd30fd.js"><link rel="prefetch" href="/blog/assets/js/387.e387b6d2.js"><link rel="prefetch" href="/blog/assets/js/388.66733dc7.js"><link rel="prefetch" href="/blog/assets/js/389.3b9dc956.js"><link rel="prefetch" href="/blog/assets/js/39.52d6d224.js"><link rel="prefetch" href="/blog/assets/js/390.aa1a3f13.js"><link rel="prefetch" href="/blog/assets/js/391.b98a3f89.js"><link rel="prefetch" href="/blog/assets/js/392.51947d62.js"><link rel="prefetch" href="/blog/assets/js/393.4e8ca73f.js"><link rel="prefetch" href="/blog/assets/js/394.4eae3c2a.js"><link rel="prefetch" href="/blog/assets/js/395.b41e57dd.js"><link rel="prefetch" href="/blog/assets/js/396.f19229fd.js"><link rel="prefetch" href="/blog/assets/js/397.8caf94d9.js"><link rel="prefetch" href="/blog/assets/js/398.a79fa068.js"><link rel="prefetch" href="/blog/assets/js/399.1f8f72a8.js"><link rel="prefetch" href="/blog/assets/js/4.72adf461.js"><link rel="prefetch" href="/blog/assets/js/40.42598248.js"><link rel="prefetch" href="/blog/assets/js/400.31d2590c.js"><link rel="prefetch" href="/blog/assets/js/401.430ee10a.js"><link rel="prefetch" href="/blog/assets/js/402.0969de14.js"><link rel="prefetch" href="/blog/assets/js/403.22305621.js"><link rel="prefetch" href="/blog/assets/js/404.c24c1ea8.js"><link rel="prefetch" href="/blog/assets/js/405.a7531e74.js"><link rel="prefetch" href="/blog/assets/js/406.44308bfa.js"><link rel="prefetch" href="/blog/assets/js/407.5bea8380.js"><link rel="prefetch" href="/blog/assets/js/408.47124f69.js"><link rel="prefetch" href="/blog/assets/js/409.f2eb55a4.js"><link rel="prefetch" href="/blog/assets/js/41.78de33b5.js"><link rel="prefetch" href="/blog/assets/js/410.75f37834.js"><link rel="prefetch" href="/blog/assets/js/411.af5a1d9f.js"><link rel="prefetch" href="/blog/assets/js/412.70ce0662.js"><link rel="prefetch" href="/blog/assets/js/413.4773178b.js"><link rel="prefetch" href="/blog/assets/js/414.b91b3c53.js"><link rel="prefetch" href="/blog/assets/js/415.93254e87.js"><link rel="prefetch" href="/blog/assets/js/416.30e849d5.js"><link rel="prefetch" href="/blog/assets/js/417.1d626e6d.js"><link rel="prefetch" href="/blog/assets/js/418.c2830038.js"><link rel="prefetch" href="/blog/assets/js/419.de704363.js"><link rel="prefetch" href="/blog/assets/js/42.f02bfe3b.js"><link rel="prefetch" href="/blog/assets/js/420.934cfa49.js"><link rel="prefetch" href="/blog/assets/js/421.9a9fcf9c.js"><link rel="prefetch" href="/blog/assets/js/422.716d7de1.js"><link rel="prefetch" href="/blog/assets/js/424.9362a268.js"><link rel="prefetch" href="/blog/assets/js/425.40010459.js"><link rel="prefetch" href="/blog/assets/js/426.9012302f.js"><link rel="prefetch" href="/blog/assets/js/427.2f4f8329.js"><link rel="prefetch" href="/blog/assets/js/428.48911c6b.js"><link rel="prefetch" href="/blog/assets/js/429.6beb6f8a.js"><link rel="prefetch" href="/blog/assets/js/43.027dffdd.js"><link rel="prefetch" href="/blog/assets/js/430.20547f3b.js"><link rel="prefetch" href="/blog/assets/js/431.f7d8b272.js"><link rel="prefetch" href="/blog/assets/js/432.29f6c6c5.js"><link rel="prefetch" href="/blog/assets/js/433.053472dd.js"><link rel="prefetch" href="/blog/assets/js/434.fe7a6b42.js"><link rel="prefetch" href="/blog/assets/js/435.7b095c52.js"><link rel="prefetch" href="/blog/assets/js/436.8e65b85b.js"><link rel="prefetch" href="/blog/assets/js/437.6f2be763.js"><link rel="prefetch" href="/blog/assets/js/438.696a4a24.js"><link rel="prefetch" href="/blog/assets/js/439.8e0ffda2.js"><link rel="prefetch" href="/blog/assets/js/44.60d7bff7.js"><link rel="prefetch" href="/blog/assets/js/440.0f9d7601.js"><link rel="prefetch" href="/blog/assets/js/441.9293a0c0.js"><link rel="prefetch" href="/blog/assets/js/442.3ab911fb.js"><link rel="prefetch" href="/blog/assets/js/443.84e874a2.js"><link rel="prefetch" href="/blog/assets/js/444.a5560886.js"><link rel="prefetch" href="/blog/assets/js/445.ee43400f.js"><link rel="prefetch" href="/blog/assets/js/446.e451b4a7.js"><link rel="prefetch" href="/blog/assets/js/447.af57b4dd.js"><link rel="prefetch" href="/blog/assets/js/448.8369a48d.js"><link rel="prefetch" href="/blog/assets/js/449.81a81117.js"><link rel="prefetch" href="/blog/assets/js/45.54197661.js"><link rel="prefetch" href="/blog/assets/js/450.ed1865ae.js"><link rel="prefetch" href="/blog/assets/js/451.c6fed955.js"><link rel="prefetch" href="/blog/assets/js/452.c490fd3f.js"><link rel="prefetch" href="/blog/assets/js/453.ce54e414.js"><link rel="prefetch" href="/blog/assets/js/454.0fcc3b33.js"><link rel="prefetch" href="/blog/assets/js/455.49b6deb2.js"><link rel="prefetch" href="/blog/assets/js/456.cea18d70.js"><link rel="prefetch" href="/blog/assets/js/457.eeb4e4ea.js"><link rel="prefetch" href="/blog/assets/js/458.6536f79d.js"><link rel="prefetch" href="/blog/assets/js/459.08049472.js"><link rel="prefetch" href="/blog/assets/js/46.2ea156c5.js"><link rel="prefetch" href="/blog/assets/js/460.ce2134bf.js"><link rel="prefetch" href="/blog/assets/js/461.4ef74cbc.js"><link rel="prefetch" href="/blog/assets/js/462.0135ab05.js"><link rel="prefetch" href="/blog/assets/js/463.75c97bc9.js"><link rel="prefetch" href="/blog/assets/js/464.0731e3a8.js"><link rel="prefetch" href="/blog/assets/js/465.58716aaa.js"><link rel="prefetch" href="/blog/assets/js/466.5d1c9632.js"><link rel="prefetch" href="/blog/assets/js/467.4a0cc264.js"><link rel="prefetch" href="/blog/assets/js/468.4d62ec95.js"><link rel="prefetch" href="/blog/assets/js/469.371bf824.js"><link rel="prefetch" href="/blog/assets/js/47.9c2846f6.js"><link rel="prefetch" href="/blog/assets/js/470.20c9b1d0.js"><link rel="prefetch" href="/blog/assets/js/471.b55b7293.js"><link rel="prefetch" href="/blog/assets/js/472.af9bed94.js"><link rel="prefetch" href="/blog/assets/js/473.763ed78b.js"><link rel="prefetch" href="/blog/assets/js/474.0c984479.js"><link rel="prefetch" href="/blog/assets/js/475.64086e43.js"><link rel="prefetch" href="/blog/assets/js/476.87a9a0cc.js"><link rel="prefetch" href="/blog/assets/js/477.bf829a5b.js"><link rel="prefetch" href="/blog/assets/js/478.e4b446ce.js"><link rel="prefetch" href="/blog/assets/js/479.2b9f77a6.js"><link rel="prefetch" href="/blog/assets/js/48.9a1455cf.js"><link rel="prefetch" href="/blog/assets/js/480.17cc6a47.js"><link rel="prefetch" href="/blog/assets/js/481.fa5b561f.js"><link rel="prefetch" href="/blog/assets/js/482.03b3eb32.js"><link rel="prefetch" href="/blog/assets/js/483.31191665.js"><link rel="prefetch" href="/blog/assets/js/484.d8a4c9bc.js"><link rel="prefetch" href="/blog/assets/js/485.e249dec7.js"><link rel="prefetch" href="/blog/assets/js/486.0fc0098f.js"><link rel="prefetch" href="/blog/assets/js/487.324062ec.js"><link rel="prefetch" href="/blog/assets/js/488.8ca4dee4.js"><link rel="prefetch" href="/blog/assets/js/489.efc83e4a.js"><link rel="prefetch" href="/blog/assets/js/49.697a7a9f.js"><link rel="prefetch" href="/blog/assets/js/490.45710d33.js"><link rel="prefetch" href="/blog/assets/js/491.8642a532.js"><link rel="prefetch" href="/blog/assets/js/492.6fd7cdf1.js"><link rel="prefetch" href="/blog/assets/js/493.e66269d0.js"><link rel="prefetch" href="/blog/assets/js/494.8617e18f.js"><link rel="prefetch" href="/blog/assets/js/495.a701486e.js"><link rel="prefetch" href="/blog/assets/js/496.6c880390.js"><link rel="prefetch" href="/blog/assets/js/497.d270d556.js"><link rel="prefetch" href="/blog/assets/js/498.88cbb5cd.js"><link rel="prefetch" href="/blog/assets/js/499.9e5d240e.js"><link rel="prefetch" href="/blog/assets/js/5.15a5c21b.js"><link rel="prefetch" href="/blog/assets/js/50.c6827336.js"><link rel="prefetch" href="/blog/assets/js/500.75b98d7e.js"><link rel="prefetch" href="/blog/assets/js/501.56835982.js"><link rel="prefetch" href="/blog/assets/js/502.6b40298b.js"><link rel="prefetch" href="/blog/assets/js/503.c1d698cc.js"><link rel="prefetch" href="/blog/assets/js/504.ccad05c2.js"><link rel="prefetch" href="/blog/assets/js/505.6f0caa3b.js"><link rel="prefetch" href="/blog/assets/js/506.aebe2376.js"><link rel="prefetch" href="/blog/assets/js/507.211ef741.js"><link rel="prefetch" href="/blog/assets/js/508.fa181cc2.js"><link rel="prefetch" href="/blog/assets/js/509.3d4bb4eb.js"><link rel="prefetch" href="/blog/assets/js/51.e12228d3.js"><link rel="prefetch" href="/blog/assets/js/510.7bc8cf92.js"><link rel="prefetch" href="/blog/assets/js/511.11fdfdc1.js"><link rel="prefetch" href="/blog/assets/js/512.ecc52d98.js"><link rel="prefetch" href="/blog/assets/js/513.7297573b.js"><link rel="prefetch" href="/blog/assets/js/514.7b747cce.js"><link rel="prefetch" href="/blog/assets/js/515.026fed32.js"><link rel="prefetch" href="/blog/assets/js/516.ed28c764.js"><link rel="prefetch" href="/blog/assets/js/517.2f57e972.js"><link rel="prefetch" href="/blog/assets/js/518.f88b52b4.js"><link rel="prefetch" href="/blog/assets/js/519.7c75386a.js"><link rel="prefetch" href="/blog/assets/js/52.c2faa40b.js"><link rel="prefetch" href="/blog/assets/js/520.75a48154.js"><link rel="prefetch" href="/blog/assets/js/521.895157dd.js"><link rel="prefetch" href="/blog/assets/js/522.7d0e847a.js"><link rel="prefetch" href="/blog/assets/js/523.01484cdc.js"><link rel="prefetch" href="/blog/assets/js/524.4d90ab64.js"><link rel="prefetch" href="/blog/assets/js/525.d1549e6c.js"><link rel="prefetch" href="/blog/assets/js/526.05483b68.js"><link rel="prefetch" href="/blog/assets/js/527.c3429fe2.js"><link rel="prefetch" href="/blog/assets/js/528.6203d42b.js"><link rel="prefetch" href="/blog/assets/js/529.5d865bd5.js"><link rel="prefetch" href="/blog/assets/js/53.5709a523.js"><link rel="prefetch" href="/blog/assets/js/530.a70a1acb.js"><link rel="prefetch" href="/blog/assets/js/531.66905b3e.js"><link rel="prefetch" href="/blog/assets/js/532.9f948fd2.js"><link rel="prefetch" href="/blog/assets/js/533.9c3dcf3c.js"><link rel="prefetch" href="/blog/assets/js/534.bcecc9a9.js"><link rel="prefetch" href="/blog/assets/js/535.01da0e85.js"><link rel="prefetch" href="/blog/assets/js/536.18503588.js"><link rel="prefetch" href="/blog/assets/js/537.3b07f2d9.js"><link rel="prefetch" href="/blog/assets/js/538.5435ae36.js"><link rel="prefetch" href="/blog/assets/js/539.bb504961.js"><link rel="prefetch" href="/blog/assets/js/54.07a4a582.js"><link rel="prefetch" href="/blog/assets/js/540.dc306ab2.js"><link rel="prefetch" href="/blog/assets/js/541.3a27f51c.js"><link rel="prefetch" href="/blog/assets/js/542.5760d344.js"><link rel="prefetch" href="/blog/assets/js/543.c2b318db.js"><link rel="prefetch" href="/blog/assets/js/544.3e7dfe68.js"><link rel="prefetch" href="/blog/assets/js/545.b9d38be9.js"><link rel="prefetch" href="/blog/assets/js/546.57f76a93.js"><link rel="prefetch" href="/blog/assets/js/547.a24678cb.js"><link rel="prefetch" href="/blog/assets/js/548.92a9e320.js"><link rel="prefetch" href="/blog/assets/js/549.ba3f7475.js"><link rel="prefetch" href="/blog/assets/js/55.f0ff0d7c.js"><link rel="prefetch" href="/blog/assets/js/550.85bf139d.js"><link rel="prefetch" href="/blog/assets/js/551.36783254.js"><link rel="prefetch" href="/blog/assets/js/552.e5e86f31.js"><link rel="prefetch" href="/blog/assets/js/553.7be649df.js"><link rel="prefetch" href="/blog/assets/js/554.8be323b3.js"><link rel="prefetch" href="/blog/assets/js/555.97049205.js"><link rel="prefetch" href="/blog/assets/js/556.a85939ec.js"><link rel="prefetch" href="/blog/assets/js/557.f0ea5bef.js"><link rel="prefetch" href="/blog/assets/js/558.f8d4d2d7.js"><link rel="prefetch" href="/blog/assets/js/559.2ef926c7.js"><link rel="prefetch" href="/blog/assets/js/56.c0cbcd65.js"><link rel="prefetch" href="/blog/assets/js/560.56eb82c5.js"><link rel="prefetch" href="/blog/assets/js/561.03a320f1.js"><link rel="prefetch" href="/blog/assets/js/562.649a7599.js"><link rel="prefetch" href="/blog/assets/js/563.8721466c.js"><link rel="prefetch" href="/blog/assets/js/564.3baad1bf.js"><link rel="prefetch" href="/blog/assets/js/565.e5e5c627.js"><link rel="prefetch" href="/blog/assets/js/566.315c4528.js"><link rel="prefetch" href="/blog/assets/js/567.b12e27ea.js"><link rel="prefetch" href="/blog/assets/js/568.225226ec.js"><link rel="prefetch" href="/blog/assets/js/569.da29d7d5.js"><link rel="prefetch" href="/blog/assets/js/57.19d05b74.js"><link rel="prefetch" href="/blog/assets/js/570.acc63d86.js"><link rel="prefetch" href="/blog/assets/js/571.a1637cdc.js"><link rel="prefetch" href="/blog/assets/js/572.bb88c6a3.js"><link rel="prefetch" href="/blog/assets/js/573.3fdb3549.js"><link rel="prefetch" href="/blog/assets/js/574.e83e19eb.js"><link rel="prefetch" href="/blog/assets/js/575.4314526d.js"><link rel="prefetch" href="/blog/assets/js/576.45816383.js"><link rel="prefetch" href="/blog/assets/js/577.27b63977.js"><link rel="prefetch" href="/blog/assets/js/578.20d32e8a.js"><link rel="prefetch" href="/blog/assets/js/579.55cf4824.js"><link rel="prefetch" href="/blog/assets/js/58.6e93d3bf.js"><link rel="prefetch" href="/blog/assets/js/580.d531b052.js"><link rel="prefetch" href="/blog/assets/js/581.f627031b.js"><link rel="prefetch" href="/blog/assets/js/582.7a256d37.js"><link rel="prefetch" href="/blog/assets/js/583.7e6734b3.js"><link rel="prefetch" href="/blog/assets/js/584.dfb3a9cb.js"><link rel="prefetch" href="/blog/assets/js/585.a71b9e6e.js"><link rel="prefetch" href="/blog/assets/js/586.5137eb0c.js"><link rel="prefetch" href="/blog/assets/js/587.5b00ff50.js"><link rel="prefetch" href="/blog/assets/js/588.6683a53f.js"><link rel="prefetch" href="/blog/assets/js/589.c2bb0000.js"><link rel="prefetch" href="/blog/assets/js/59.74303d32.js"><link rel="prefetch" href="/blog/assets/js/590.ac1e550c.js"><link rel="prefetch" href="/blog/assets/js/591.124f0d02.js"><link rel="prefetch" href="/blog/assets/js/592.c930684a.js"><link rel="prefetch" href="/blog/assets/js/593.5010d3e0.js"><link rel="prefetch" href="/blog/assets/js/594.26376a7e.js"><link rel="prefetch" href="/blog/assets/js/595.b0a30b26.js"><link rel="prefetch" href="/blog/assets/js/596.b3f21662.js"><link rel="prefetch" href="/blog/assets/js/597.c26a075e.js"><link rel="prefetch" href="/blog/assets/js/598.baaa2008.js"><link rel="prefetch" href="/blog/assets/js/599.76aefe16.js"><link rel="prefetch" href="/blog/assets/js/6.c39ed483.js"><link rel="prefetch" href="/blog/assets/js/60.ef364511.js"><link rel="prefetch" href="/blog/assets/js/600.adf240b2.js"><link rel="prefetch" href="/blog/assets/js/601.e5c38ddf.js"><link rel="prefetch" href="/blog/assets/js/602.c9ffa321.js"><link rel="prefetch" href="/blog/assets/js/603.36192b41.js"><link rel="prefetch" href="/blog/assets/js/604.5c4f5034.js"><link rel="prefetch" href="/blog/assets/js/605.51fbb0a4.js"><link rel="prefetch" href="/blog/assets/js/606.88fe8276.js"><link rel="prefetch" href="/blog/assets/js/607.7dc7d0b1.js"><link rel="prefetch" href="/blog/assets/js/608.0b5d9f52.js"><link rel="prefetch" href="/blog/assets/js/609.a7830f0c.js"><link rel="prefetch" href="/blog/assets/js/61.436e46a1.js"><link rel="prefetch" href="/blog/assets/js/610.031fa893.js"><link rel="prefetch" href="/blog/assets/js/611.90e0e4d3.js"><link rel="prefetch" href="/blog/assets/js/612.2b9a79d8.js"><link rel="prefetch" href="/blog/assets/js/613.d38f2851.js"><link rel="prefetch" href="/blog/assets/js/614.6cfef72e.js"><link rel="prefetch" href="/blog/assets/js/615.73222425.js"><link rel="prefetch" href="/blog/assets/js/616.771db440.js"><link rel="prefetch" href="/blog/assets/js/617.a4729ad6.js"><link rel="prefetch" href="/blog/assets/js/618.5ea9e128.js"><link rel="prefetch" href="/blog/assets/js/619.10f5472f.js"><link rel="prefetch" href="/blog/assets/js/62.9490960b.js"><link rel="prefetch" href="/blog/assets/js/620.d0069dce.js"><link rel="prefetch" href="/blog/assets/js/621.d19ea508.js"><link rel="prefetch" href="/blog/assets/js/622.9e5920b2.js"><link rel="prefetch" href="/blog/assets/js/623.78d93448.js"><link rel="prefetch" href="/blog/assets/js/624.d37d7fce.js"><link rel="prefetch" href="/blog/assets/js/625.447be412.js"><link rel="prefetch" href="/blog/assets/js/626.e59af340.js"><link rel="prefetch" href="/blog/assets/js/627.7e6d7c95.js"><link rel="prefetch" href="/blog/assets/js/628.d135b01b.js"><link rel="prefetch" href="/blog/assets/js/629.8886d8ea.js"><link rel="prefetch" href="/blog/assets/js/63.6cc1bcab.js"><link rel="prefetch" href="/blog/assets/js/630.22d4ced4.js"><link rel="prefetch" href="/blog/assets/js/631.9e7843f6.js"><link rel="prefetch" href="/blog/assets/js/632.4803808d.js"><link rel="prefetch" href="/blog/assets/js/633.50e4112f.js"><link rel="prefetch" href="/blog/assets/js/634.bb993c38.js"><link rel="prefetch" href="/blog/assets/js/635.5d6b9e6f.js"><link rel="prefetch" href="/blog/assets/js/636.9b48556d.js"><link rel="prefetch" href="/blog/assets/js/637.c42758ba.js"><link rel="prefetch" href="/blog/assets/js/638.68cc6fae.js"><link rel="prefetch" href="/blog/assets/js/639.169fa156.js"><link rel="prefetch" href="/blog/assets/js/64.abdee43a.js"><link rel="prefetch" href="/blog/assets/js/640.fbe8a02e.js"><link rel="prefetch" href="/blog/assets/js/641.ec22012c.js"><link rel="prefetch" href="/blog/assets/js/642.d4d1938f.js"><link rel="prefetch" href="/blog/assets/js/643.10235c62.js"><link rel="prefetch" href="/blog/assets/js/644.c1fadb2f.js"><link rel="prefetch" href="/blog/assets/js/645.c149df88.js"><link rel="prefetch" href="/blog/assets/js/646.c6c3f8e0.js"><link rel="prefetch" href="/blog/assets/js/647.6958847b.js"><link rel="prefetch" href="/blog/assets/js/648.5c76d596.js"><link rel="prefetch" href="/blog/assets/js/649.d6beec02.js"><link rel="prefetch" href="/blog/assets/js/65.68211d02.js"><link rel="prefetch" href="/blog/assets/js/650.89cbe447.js"><link rel="prefetch" href="/blog/assets/js/651.92467f48.js"><link rel="prefetch" href="/blog/assets/js/652.4845f96a.js"><link rel="prefetch" href="/blog/assets/js/653.a47fdb3b.js"><link rel="prefetch" href="/blog/assets/js/654.fcea3f5b.js"><link rel="prefetch" href="/blog/assets/js/655.4e38720d.js"><link rel="prefetch" href="/blog/assets/js/656.a397f16b.js"><link rel="prefetch" href="/blog/assets/js/657.dde34611.js"><link rel="prefetch" href="/blog/assets/js/658.0039ce51.js"><link rel="prefetch" href="/blog/assets/js/659.a0c401bc.js"><link rel="prefetch" href="/blog/assets/js/66.82efb6b8.js"><link rel="prefetch" href="/blog/assets/js/660.23dd5e68.js"><link rel="prefetch" href="/blog/assets/js/661.bc079b94.js"><link rel="prefetch" href="/blog/assets/js/662.f43d86c6.js"><link rel="prefetch" href="/blog/assets/js/663.a612b987.js"><link rel="prefetch" href="/blog/assets/js/664.f19257f7.js"><link rel="prefetch" href="/blog/assets/js/665.0cbf6fc1.js"><link rel="prefetch" href="/blog/assets/js/666.7a7c5b50.js"><link rel="prefetch" href="/blog/assets/js/667.70bcb46c.js"><link rel="prefetch" href="/blog/assets/js/668.c1c81ab6.js"><link rel="prefetch" href="/blog/assets/js/669.75638ed7.js"><link rel="prefetch" href="/blog/assets/js/67.9f6abed5.js"><link rel="prefetch" href="/blog/assets/js/670.2f90459b.js"><link rel="prefetch" href="/blog/assets/js/671.2ab85c06.js"><link rel="prefetch" href="/blog/assets/js/672.6975af2f.js"><link rel="prefetch" href="/blog/assets/js/673.4a5e0d55.js"><link rel="prefetch" href="/blog/assets/js/674.4b1db2e3.js"><link rel="prefetch" href="/blog/assets/js/675.162422a7.js"><link rel="prefetch" href="/blog/assets/js/676.5712b341.js"><link rel="prefetch" href="/blog/assets/js/677.f0d472cb.js"><link rel="prefetch" href="/blog/assets/js/678.a25199d2.js"><link rel="prefetch" href="/blog/assets/js/679.eebf998e.js"><link rel="prefetch" href="/blog/assets/js/68.b54c5d39.js"><link rel="prefetch" href="/blog/assets/js/680.9b45dc26.js"><link rel="prefetch" href="/blog/assets/js/681.d998bda3.js"><link rel="prefetch" href="/blog/assets/js/682.26a6b3b8.js"><link rel="prefetch" href="/blog/assets/js/683.3d3255b3.js"><link rel="prefetch" href="/blog/assets/js/684.01350009.js"><link rel="prefetch" href="/blog/assets/js/685.d94d2d49.js"><link rel="prefetch" href="/blog/assets/js/686.14b9a854.js"><link rel="prefetch" href="/blog/assets/js/687.6801763c.js"><link rel="prefetch" href="/blog/assets/js/688.c869d83c.js"><link rel="prefetch" href="/blog/assets/js/689.721c5842.js"><link rel="prefetch" href="/blog/assets/js/69.96961d17.js"><link rel="prefetch" href="/blog/assets/js/690.c58e3111.js"><link rel="prefetch" href="/blog/assets/js/691.a3ba298b.js"><link rel="prefetch" href="/blog/assets/js/692.976c9961.js"><link rel="prefetch" href="/blog/assets/js/693.dfa1dc96.js"><link rel="prefetch" href="/blog/assets/js/694.d5095ad8.js"><link rel="prefetch" href="/blog/assets/js/695.75a9299f.js"><link rel="prefetch" href="/blog/assets/js/696.47cd8b93.js"><link rel="prefetch" href="/blog/assets/js/697.f7bd7f56.js"><link rel="prefetch" href="/blog/assets/js/698.bfe02dd1.js"><link rel="prefetch" href="/blog/assets/js/699.5b6fee9c.js"><link rel="prefetch" href="/blog/assets/js/7.8763782e.js"><link rel="prefetch" href="/blog/assets/js/70.1c7c67c2.js"><link rel="prefetch" href="/blog/assets/js/700.49ecc111.js"><link rel="prefetch" href="/blog/assets/js/701.b19efefd.js"><link rel="prefetch" href="/blog/assets/js/702.d89fba61.js"><link rel="prefetch" href="/blog/assets/js/703.cbbe6ab7.js"><link rel="prefetch" href="/blog/assets/js/704.f221501a.js"><link rel="prefetch" href="/blog/assets/js/705.9b1567f8.js"><link rel="prefetch" href="/blog/assets/js/706.83b5a9bc.js"><link rel="prefetch" href="/blog/assets/js/707.38bfe6a4.js"><link rel="prefetch" href="/blog/assets/js/708.c4a02d49.js"><link rel="prefetch" href="/blog/assets/js/709.c37c1d0b.js"><link rel="prefetch" href="/blog/assets/js/71.136106d8.js"><link rel="prefetch" href="/blog/assets/js/710.b6daefb8.js"><link rel="prefetch" href="/blog/assets/js/711.60a8d807.js"><link rel="prefetch" href="/blog/assets/js/712.f1600b25.js"><link rel="prefetch" href="/blog/assets/js/713.9b84fe0f.js"><link rel="prefetch" href="/blog/assets/js/714.85f8dfd7.js"><link rel="prefetch" href="/blog/assets/js/715.2e7ac623.js"><link rel="prefetch" href="/blog/assets/js/716.d4016dae.js"><link rel="prefetch" href="/blog/assets/js/72.d1192ec4.js"><link rel="prefetch" href="/blog/assets/js/73.cefbd395.js"><link rel="prefetch" href="/blog/assets/js/74.f7652c2d.js"><link rel="prefetch" href="/blog/assets/js/75.93794018.js"><link rel="prefetch" href="/blog/assets/js/76.ea74a9e5.js"><link rel="prefetch" href="/blog/assets/js/77.5ab8d933.js"><link rel="prefetch" href="/blog/assets/js/78.d99be8f9.js"><link rel="prefetch" href="/blog/assets/js/79.514e7b92.js"><link rel="prefetch" href="/blog/assets/js/8.4f413a3f.js"><link rel="prefetch" href="/blog/assets/js/80.311c1c3d.js"><link rel="prefetch" href="/blog/assets/js/81.20e611a2.js"><link rel="prefetch" href="/blog/assets/js/82.33311f3c.js"><link rel="prefetch" href="/blog/assets/js/83.d0445fd9.js"><link rel="prefetch" href="/blog/assets/js/84.cdf6237f.js"><link rel="prefetch" href="/blog/assets/js/85.39e8a5b1.js"><link rel="prefetch" href="/blog/assets/js/86.5c32af0a.js"><link rel="prefetch" href="/blog/assets/js/87.66683f39.js"><link rel="prefetch" href="/blog/assets/js/88.7ad0a079.js"><link rel="prefetch" href="/blog/assets/js/89.2d677ef4.js"><link rel="prefetch" href="/blog/assets/js/9.4a0f256f.js"><link rel="prefetch" href="/blog/assets/js/90.322cff52.js"><link rel="prefetch" href="/blog/assets/js/91.5f07c279.js"><link rel="prefetch" href="/blog/assets/js/92.d6c06cd2.js"><link rel="prefetch" href="/blog/assets/js/93.6ba1a145.js"><link rel="prefetch" href="/blog/assets/js/94.b917fb97.js"><link rel="prefetch" href="/blog/assets/js/95.05ebd8ca.js"><link rel="prefetch" href="/blog/assets/js/96.649aca28.js"><link rel="prefetch" href="/blog/assets/js/97.6cfc54b2.js"><link rel="prefetch" href="/blog/assets/js/98.754036a7.js"><link rel="prefetch" href="/blog/assets/js/99.c0a4fe5b.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.f2b65211.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu have-body-img"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/img/logo.png" alt="Emma's Blog" class="logo"> <span class="site-name can-hide">Emma's Blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/" class="nav-link">首页</a></div><div class="nav-item"><a href="/blog/bytag/" class="nav-link">By Tag</a></div><div class="nav-item"><a href="/blog/google/" class="nav-link">Google</a></div><div class="nav-item"><a href="/blog/ml/" class="nav-link">机器学习</a></div><div class="nav-item"><a href="/blog/bq/" class="nav-link">BQ</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Companies" class="dropdown-title"><!----> <span class="title" style="display:;">Companies</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/ls/" class="nav-link">Facebook</a></li><li class="dropdown-item"><!----> <a href="/blog/design/" class="nav-link">System Design</a></li><li class="dropdown-item"><!----> <a href="/blog/twosigma/" class="nav-link">标准差</a></li><li class="dropdown-item"><!----> <a href="/blog/leetcode/" class="nav-link">其它</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/blog/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/blog/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/blog/archives/" class="nav-link">归档</a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="https://raw.githubusercontent.com/emmableu/image/master/202204101726398.png"> <div class="blogger-info"><h3>emmableu</h3> <span></span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/blog/" class="nav-link">首页</a></div><div class="nav-item"><a href="/blog/bytag/" class="nav-link">By Tag</a></div><div class="nav-item"><a href="/blog/google/" class="nav-link">Google</a></div><div class="nav-item"><a href="/blog/ml/" class="nav-link">机器学习</a></div><div class="nav-item"><a href="/blog/bq/" class="nav-link">BQ</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Companies" class="dropdown-title"><!----> <span class="title" style="display:;">Companies</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/ls/" class="nav-link">Facebook</a></li><li class="dropdown-item"><!----> <a href="/blog/design/" class="nav-link">System Design</a></li><li class="dropdown-item"><!----> <a href="/blog/twosigma/" class="nav-link">标准差</a></li><li class="dropdown-item"><!----> <a href="/blog/leetcode/" class="nav-link">其它</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/blog/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/blog/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/blog/archives/" class="nav-link">归档</a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Zero To Hero</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/blog/pages/5de0af/" class="sidebar-link">Micrograd</a></li><li><a href="/blog/pages/072835/" class="sidebar-link">Makemore 1.1 - bigram</a></li><li><a href="/blog/pages/3d7624/" class="sidebar-link">Makemore 1.2 - trigram 1</a></li><li><a href="/blog/pages/fd8228/" class="sidebar-link">Makemore 2 - MLP</a></li><li><a href="/blog/pages/c0d04f/" aria-current="page" class="active sidebar-link">Makemore 3 - Activations &amp; Gradients, BatchNorm</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/blog/pages/c0d04f/#_6-1-running-bn-mean-and-bn-std" class="sidebar-link">6.1 Running bnmean and bnstd.</a></li><li class="sidebar-sub-header level2"><a href="/blog/pages/c0d04f/#_6-2-removal-of-b1" class="sidebar-link">6.2: Removal of b1</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/blog/pages/c0d04f/#_1-overconfidence-and-generalization" class="sidebar-link">1. Overconfidence and Generalization</a></li><li class="sidebar-sub-header level3"><a href="/blog/pages/c0d04f/#_2-softmax-saturation" class="sidebar-link">2. Softmax Saturation</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/blog/pages/c0d04f/#_11-1-activation-distribution" class="sidebar-link">11.1 Activation Distribution</a></li><li class="sidebar-sub-header level2"><a href="/blog/pages/c0d04f/#_11-2-gradient-distribution" class="sidebar-link">11.2 Gradient Distribution</a></li><li class="sidebar-sub-header level2"><a href="/blog/pages/c0d04f/#_11-3-weights-gradient-distribution" class="sidebar-link">11.3 Weights Gradient Distribution</a></li><li class="sidebar-sub-header level2"><a href="/blog/pages/c0d04f/#_11-4-change-of-update-rate" class="sidebar-link">11.4 Change of Update Rate</a></li></ul></li><li><a href="/blog/pages/049b6e/" class="sidebar-link">Makemore 4 - Backpropagation Ninja</a></li><li><a href="/blog/pages/aed865/" class="sidebar-link">Makemore 5 - WaveNet</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Machine Learning General</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Machine Learning Concepts</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Reinforcement Learning</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>PyTorch</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span> Complete ML Projects</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper bg-style-1"><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/blog/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/blog/ml/#机器学习八股文" data-v-06225672>机器学习八股文</a></li><li data-v-06225672><a href="/blog/ml/#Zero To Hero" data-v-06225672>Zero To Hero</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="javascript:;" data-v-06225672>emmableu</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2025-07-04</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><!---->Makemore 3 - Activations &amp; Gradients, BatchNorm<!----></h1>  <div class="theme-vdoing-content content__default"><p>Links:</p> <ul><li>Youtube: <a href="https://www.youtube.com/watch?v=P6sfmUTpUmc" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=P6sfmUTpUmc<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>makemore on github: <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa2JnOEZmY2VxN0s2b1NiWWhEZTMyNFY1M0Y0Z3xBQ3Jtc0tuRGNQOFEtODcyVGY3dGNOdjd1TGtzUUpvcUFrV0NtR0Yzd00ySUd3aXg2SXVxLUQ3SVVvUWdUSk5iVnV5RzZwQURDLVkyTF92Wk1tWjBJMmFaNkcxeGlNT3ZsTGIxM1VVdF92RVp4UlNiT3l4Y3BCZw&amp;q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fmakemore&amp;v=P6sfmUTpUmc" target="_blank" rel="noopener noreferrer">https://github.com/karpathy/makemore<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>jupyter notebook I built in this video: <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa0N3SlZVY1I2c0xKYUJVZTczYzJZN1dOZUs2Z3xBQ3Jtc0tuWVU5UUwwN0JmQVQwZ3VUVkdQdWRFa0FGY0l4Wjg2YWdNQmc3cWFLZERkM1V3dlBIcjJueTR1bWhIU0hZN1VzQTNOT0p6Nm1Jc2RHd1hJaXNsMEpuT1Bkd0tmSDRPRlhoWWVuVW1jVGx0ZTJ4VXdJRQ&amp;q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fnn-zero-to-hero%2Fblob%2Fmaster%2Flectures%2Fmakemore%2Fmakemore_part3_bn.ipynb&amp;v=P6sfmUTpUmc" target="_blank" rel="noopener noreferrer">https://github.com/karpathy/nn-zero-t...<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>collab notebook: <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqazJRSGF2MGdTbjJzenRWYnlfU2xUQVAxZ2UxUXxBQ3Jtc0tsVFctZUE1VFlHZUlVLVMxYTF0ZE9OYXNIazE3MFVJOUotd3NUQkk0OEZacHpCTV83dmt1eE1ZWEhTUXptZlFPOE5hYW5PbElSRFYtVmNpb0NxU0ZUcmp0MkJzY0NGNzZHX1gtMEFLYXBMWVhvaUstcw&amp;q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1H5CSy-OnisagUgDUXhHwo1ng2pjKHYSN%3Fusp%3Dsharing&amp;v=P6sfmUTpUmc" target="_blank" rel="noopener noreferrer">https://colab.research.google.com/dri...<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <p>Useful links:</p> <ul><li>&quot;Kaiming init&quot; paper:<a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbjVBbU9tX2NXeDRYclo4R1hMT3A0eU9iUFRmZ3xBQ3Jtc0ttSmZ3N2IyTks5RDhyM1hZcjhYTHpXQWtBdXlHYzZQY050OEtBcGZmLVljbFVGS3VwLUxpbWhYb0d6VXJ2NkJTdEJoUC1rVDY0LS1neDdZZVVURmdIZDRVUkExTzh1czc0a2VKM05WSlo0VmhtQ2l6TQ&amp;q=https%3A%2F%2Farxiv.org%2Fabs%2F1502.01852&amp;v=P6sfmUTpUmc" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1502.01852<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>BatchNorm paper: <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbWtfREtEMndUb0E2VHpHako4TFpDWmlMaXg2d3xBQ3Jtc0tuOVRjMXBseUY3djgwbU83eFpIQldTTGxVY0xyRWtBa2tidjk4al93RExPOVRuMjF4M3FYU1JDZUNkR00zVVVHcmJNTUFXYlFVS2Qwd19LX1RJRmVwd29GTXVnWkZuNWNSMC1KZ21kRkJWd19vTVhTdw&amp;q=https%3A%2F%2Farxiv.org%2Fabs%2F1502.03167&amp;v=P6sfmUTpUmc" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1502.03167<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>Bengio et al. 2003 MLP language model paper (pdf): <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqazVuU2NUbWg4N0RhbmZ6a1M4bmk1MjRsMFpGd3xBQ3Jtc0tuUEFDVUN4VUpOUHVsS1QzTm9mVkN2UVlKRDJNTzEyZEVFNkZLWThBYmkwUkRtTGtCQzQyS3FkLXMyMWYzUWoxQndaUUpHQ3VJb1Z5bzJrNmJsUWtqQ05Obl91NGs5cHBGblVxc2VOdGpPX0hQSWdsdw&amp;q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fbengio03a%2Fbengio03a.pdf&amp;v=P6sfmUTpUmc" target="_blank" rel="noopener noreferrer">https://www.jmlr.org/papers/volume3/b...<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>Good paper illustrating some of the problems with batchnorm in practice: <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa0VGNldiSkcxOVpIVlVBeF9VWGlJWTJOQVJFd3xBQ3Jtc0tsbEdCNDl6RU5FQXdoOTFQVmIzX3NWUVpybWtZQnhmbk5naXNVcThfVWFxX2dQaXlJbWJGaHo4T3I2Rk5LSUFiYnVwdFJFaVdGTUNjY2lhNHFqUGZWTGQ5Y1JKZm5hS0E0bVBpeUNOTTVMYkRMdlFfNA&amp;q=https%3A%2F%2Farxiv.org%2Fabs%2F2105.07576&amp;v=P6sfmUTpUmc" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2105.07576<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h1 id="course-notes"><a href="#course-notes" class="header-anchor">#</a> Course Notes</h1> <hr> <h1 id="_0-this-is-the-code-we-end-up-with-from-last-section-we-will-fix-some-issues-with-the-code-today"><a href="#_0-this-is-the-code-we-end-up-with-from-last-section-we-will-fix-some-issues-with-the-code-today" class="header-anchor">#</a> 0. This is the code we end up with from last section. We will fix some issues with the code today.</h1> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

words <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'makemore-master/names.txt'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>splitlines<span class="token punctuation">(</span><span class="token punctuation">)</span>
words<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">]</span>

<span class="token comment"># build the vocabulary of characters and mappings to/from integers</span>
chars <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
stoi <span class="token operator">=</span> <span class="token punctuation">{</span>s<span class="token punctuation">:</span>i<span class="token operator">+</span><span class="token number">1</span> <span class="token keyword">for</span> i<span class="token punctuation">,</span>s <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>chars<span class="token punctuation">)</span><span class="token punctuation">}</span>
stoi<span class="token punctuation">[</span><span class="token string">'.'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
itos <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span>s <span class="token keyword">for</span> s<span class="token punctuation">,</span>i <span class="token keyword">in</span> stoi<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
vocab_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>itos<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>itos<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>vocab_size<span class="token punctuation">)</span>

<span class="token comment"># build the dataset</span>
block_size <span class="token operator">=</span> <span class="token number">3</span>  <span class="token comment"># context length: how many characters do we take to predict the next one?</span>

<span class="token keyword">def</span> <span class="token function">build_dataset</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">:</span>
    X<span class="token punctuation">,</span> Y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token keyword">for</span> w <span class="token keyword">in</span> words<span class="token punctuation">:</span>
        w <span class="token operator">=</span> <span class="token string">'...'</span> <span class="token operator">+</span> w <span class="token operator">+</span> <span class="token string">&quot;.&quot;</span>
        <span class="token keyword">for</span> c1<span class="token punctuation">,</span> c2<span class="token punctuation">,</span> c3<span class="token punctuation">,</span> c4 <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> w<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> w<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> w<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            X<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>stoi<span class="token punctuation">[</span>ele<span class="token punctuation">]</span> <span class="token keyword">for</span> ele <span class="token keyword">in</span> <span class="token punctuation">[</span>c1<span class="token punctuation">,</span>c2<span class="token punctuation">,</span>c3<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            Y<span class="token punctuation">.</span>append<span class="token punctuation">(</span>stoi<span class="token punctuation">[</span>c4<span class="token punctuation">]</span><span class="token punctuation">)</span>
    X <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>Y<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> Y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    <span class="token keyword">return</span> X<span class="token punctuation">,</span> Y

<span class="token keyword">import</span> random

random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span>
random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>words<span class="token punctuation">)</span>
n1 <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">0.8</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">)</span>
n2 <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">0.9</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">)</span>

Xtr<span class="token punctuation">,</span> Ytr <span class="token operator">=</span> build_dataset<span class="token punctuation">(</span>words<span class="token punctuation">[</span><span class="token punctuation">:</span>n1<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 80%</span>
Xdev<span class="token punctuation">,</span> Ydev <span class="token operator">=</span> build_dataset<span class="token punctuation">(</span>words<span class="token punctuation">[</span>n1<span class="token punctuation">:</span>n2<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 10%</span>
Xte<span class="token punctuation">,</span> Yte <span class="token operator">=</span> build_dataset<span class="token punctuation">(</span>words<span class="token punctuation">[</span>n2<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 10%s</span>

<span class="token comment"># MLP revisited</span>
n_embd <span class="token operator">=</span> <span class="token number">10</span> <span class="token comment"># the dimensionality of the character embedding vectors</span>
n_hidden <span class="token operator">=</span> <span class="token number">200</span> <span class="token comment"># the number of neurons in the hidden layer of the MLP</span>

g <span class="token operator">=</span> torch<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">2147483647</span><span class="token punctuation">)</span> <span class="token comment"># for reproducibility</span>
C  <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> n_embd<span class="token punctuation">)</span><span class="token punctuation">,</span>            generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
W1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>n_embd <span class="token operator">*</span> block_size<span class="token punctuation">,</span> n_hidden<span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
b1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span>                        generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
W2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">,</span>          generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span>                      generator<span class="token operator">=</span>g<span class="token punctuation">)</span> 

parameters <span class="token operator">=</span> <span class="token punctuation">[</span>C<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>nelement<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> parameters<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># number of parameters in total</span>
<span class="token keyword">for</span> p <span class="token keyword">in</span> parameters<span class="token punctuation">:</span>
  p<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>

<span class="token comment"># same optimization as last time</span>
max_steps <span class="token operator">=</span> <span class="token number">200000</span>
batch_size <span class="token operator">=</span> <span class="token number">32</span>
lossi <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token comment"># minibatch construct</span>
    ix <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> Xtr<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
    Xb<span class="token punctuation">,</span> Yb <span class="token operator">=</span> Xtr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">,</span> Ytr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span>  <span class="token comment"># batch X,Y</span>

    emb <span class="token operator">=</span> C<span class="token punctuation">[</span>Xb<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">[</span>Xb<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># [7, 3, 2] =&gt; [7, 6]</span>
    tanh <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>emb @ W1 <span class="token operator">+</span> b1<span class="token punctuation">)</span>  <span class="token comment"># [7, 100]</span>
    logits <span class="token operator">=</span> tanh @ W2 <span class="token operator">+</span> b2  <span class="token comment"># [7, 27]</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> Yb<span class="token punctuation">)</span>

    <span class="token comment"># backward pass</span>
    <span class="token keyword">for</span> p <span class="token keyword">in</span> parameters<span class="token punctuation">:</span>
        p<span class="token punctuation">.</span>grad <span class="token operator">=</span> <span class="token boolean">None</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># update</span>
    lr <span class="token operator">=</span> <span class="token number">0.1</span> <span class="token keyword">if</span> i <span class="token operator">&lt;</span> <span class="token number">100000</span> <span class="token keyword">else</span> <span class="token number">0.01</span>  <span class="token comment"># step learning rate decay</span>
    <span class="token keyword">for</span> p <span class="token keyword">in</span> parameters<span class="token punctuation">:</span>
        p<span class="token punctuation">.</span>data <span class="token operator">+=</span> <span class="token operator">-</span>lr <span class="token operator">*</span> p<span class="token punctuation">.</span>grad

    <span class="token comment"># track stats</span>
    <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">10000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>  <span class="token comment"># print every once in a while</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">:</span><span class="token format-spec">7d</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>max_steps<span class="token punctuation">:</span><span class="token format-spec">7d</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    lossi<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>log10<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>lossi<span class="token punctuation">)</span>

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br><span class="line-number">81</span><br><span class="line-number">82</span><br><span class="line-number">83</span><br><span class="line-number">84</span><br><span class="line-number">85</span><br><span class="line-number">86</span><br><span class="line-number">87</span><br><span class="line-number">88</span><br><span class="line-number">89</span><br><span class="line-number">90</span><br><span class="line-number">91</span><br><span class="line-number">92</span><br></div></div><p>The code plots the below</p> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042308448.png" alt=""></p> <h1 id="_1-avoid-hockey-shape-loss-function"><a href="#_1-avoid-hockey-shape-loss-function" class="header-anchor">#</a> 1. Avoid Hockey Shape Loss Function</h1> <p>In the above code, the parameters are not initialized well, original logits are a bit extreme (e.g., 4, 5,6, instead of 0.2, -0.2, etc).</p> <p>This will cause initial loss to be very high (e..g, optimal loss = 2, but initial loss = 27).</p> <p>This will cause for the first few rounds of batches, we are only making the weights less to the extreme.</p> <p>To avoid that, when we initialize parameters, we can do</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>g <span class="token operator">=</span> torch<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">2147483647</span><span class="token punctuation">)</span> <span class="token comment"># for reproducibility</span>
C  <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> n_embd<span class="token punctuation">)</span><span class="token punctuation">,</span>            generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
W1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>n_embd <span class="token operator">*</span> block_size<span class="token punctuation">,</span> n_hidden<span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span> 
b1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span>                        generator<span class="token operator">=</span>g<span class="token punctuation">)</span> 
W2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">,</span>          generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span>
b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span>                      generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>this way the loss function will look like below</p> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042308156.png" alt=""></p> <h1 id="_2-avoid-dead-neurons-from-saturated-tanh"><a href="#_2-avoid-dead-neurons-from-saturated-tanh" class="header-anchor">#</a> 2.  Avoid Dead Neurons from Saturated Tanh</h1> <p>remember the training function, in the middle there’s a tanh function</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token comment"># minibatch construct</span>
    ix <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> Xtr<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
    Xb<span class="token punctuation">,</span> Yb <span class="token operator">=</span> Xtr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">,</span> Ytr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span>  <span class="token comment"># batch X,Y</span>

    emb <span class="token operator">=</span> C<span class="token punctuation">[</span>Xb<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">[</span>Xb<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># [7, 3, 2] =&gt; [7, 6]</span>
    tanh <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>emb @ W1 <span class="token operator">+</span> b1<span class="token punctuation">)</span>  <span class="token comment"># [7, 100]</span>
    logits <span class="token operator">=</span> tanh @ W2 <span class="token operator">+</span> b2  <span class="token comment"># [7, 27]</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> Yb<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>However, as we can see in the below image, almost all activation function has a place where gradient is almost zero, when the data is very big or very small</p> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042309938.png" alt=""></p> <p>We can examine our tanh layer to see what it looks like</p> <p>Using the tanh layer from the <strong>first</strong> step/epoch to plot a histogram:</p> <p>Why we can use the <strong>first epoch,</strong> instead of doing it at the end of the training?</p> <ul><li>that’s because we don’t want the data to be <strong>initialized</strong> wrong.</li></ul> <div class="language-python line-numbers-mode"><pre class="language-python"><code>plt<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>tanh<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>It looks like this</p> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042309419.png" alt=""></p> <p>This means the tanh layer is “very activated / saturated”. This is a bad thing, because the gradient of tanh at -1 and 1 are almost zero (see the above plots for the activation functions).</p> <p>We can also see how many data has abs &gt; 0.99</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token builtin">abs</span><span class="token punctuation">(</span>tanh<span class="token punctuation">)</span><span class="token operator">&gt;</span><span class="token number">0.99</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">'grey'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>colorbar<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>there’s lots of white pixels, meaning those that have abs &gt; 0.99</p> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042311663.png" alt=""></p> <p>why did this happen? let’s check the input to tanh, which is emb@W1 + b1</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>plt<span class="token punctuation">.</span>hist<span class="token punctuation">(</span><span class="token punctuation">(</span>emb @ W1 <span class="token operator">+</span> b1<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042312964.png" alt=""></p> <p>We can see that many data are quite extreme (with abs over 3). If we check back the tanh plot, tanh(3) is already 0.9951</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
tanh <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tanh<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&quot;&quot;&quot;
output:
tensor(0.7616)
tensor(0.9640)
tensor(0.9951)
&quot;&quot;&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042312540.png" alt=""></p> <p>To fix this issue, we need to make sure W1 and b1 are not too extreme either.</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>g <span class="token operator">=</span> torch<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">2147483647</span><span class="token punctuation">)</span> <span class="token comment"># for reproducibility</span>
C  <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> n_embd<span class="token punctuation">)</span><span class="token punctuation">,</span>            generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
W1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>n_embd <span class="token operator">*</span> block_size<span class="token punctuation">,</span> n_hidden<span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.1</span>
b1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span>                        generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span>
W2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">,</span>          generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span>
b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span>                      generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> 
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p><strong>Confirming the above is effective:</strong></p> <p>Here’s the new <code>emb@W1 + b1</code> data (at the first epoch)</p> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042312631.png" alt=""></p> <p>Here’s the new histogram of the tanh data (at the first epoch)</p> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042313033.png" alt=""></p> <p>Here’s the imshow graph, every data is 0</p> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042313346.png" alt=""></p> <h1 id="_3-final-way-to-verify-if-the-above-optimization-make-sense"><a href="#_3-final-way-to-verify-if-the-above-optimization-make-sense" class="header-anchor">#</a> 3.  Final way to verify if the above optimization make sense</h1> <p>The Final way to verify if the above optimization make sense is to check the loss.</p> <p>With the same amount of epoch, with these updates, the final loss should be less.</p> <p>These updates are very important for more complicated models, as the model will be much less tolerable to biases introduced from initialization.</p> <h1 id="_4-calculate-the-init-scale-kaiming-init"><a href="#_4-calculate-the-init-scale-kaiming-init" class="header-anchor">#</a> 4. Calculate the Init Scale: Kaiming Init</h1> <p>How do we know what multiplier is best for W1?</p> <p>Based on the Kaiming init paper (&quot;Kaiming init&quot; paper:<a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbjVBbU9tX2NXeDRYclo4R1hMT3A0eU9iUFRmZ3xBQ3Jtc0ttSmZ3N2IyTks5RDhyM1hZcjhYTHpXQWtBdXlHYzZQY050OEtBcGZmLVljbFVGS3VwLUxpbWhYb0d6VXJ2NkJTdEJoUC1rVDY0LS1neDdZZVVURmdIZDRVUkExTzh1czc0a2VKM05WSlo0VmhtQ2l6TQ&amp;q=https%3A%2F%2Farxiv.org%2Fabs%2F1502.01852&amp;v=P6sfmUTpUmc" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1502.01852<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>), we need to do</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>W1 <span class="token operator">=</span> <span class="token punctuation">(</span>W1 <span class="token operator">*</span> <span class="token number">5</span><span class="token operator">/</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">30</span> <span class="token operator">**</span> <span class="token number">0.5</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p><strong>why divide by <code>30 ** 0.5</code> ?</strong></p> <p>remember W1 is initialized to be</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>W1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>n_embd <span class="token operator">*</span> block_size<span class="token punctuation">,</span> n_hidden<span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>n-embd = 10, block_size = 3</p> <p>Intuitively, if a data originally has mean = 0, std = 1</p> <p>then the 30 items of W1 increased the std to be <code>sqrt(30)</code></p> <p>you can imagine that as multiply a set of data with mean = 0, std = 1 by 30</p> <p>the resulting std is roughly sqrt(30)</p> <p>So to make the std back to 1, we devide it by <code>sqrt(30)</code></p> <p><strong>Why multiply by 5/3 ?</strong></p> <p>Intuitively, this is because tanh is a squashing function, we need to counter the squashing effect by giving it some additional gain.</p> <p><strong>In Torch, the above can be done directly by <code>torch.nn.kaiming_normal_</code></strong> <a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_" target="_blank" rel="noopener noreferrer">https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>What we described above is the default <code>fan_in</code> mode.</p> <p><strong>Will the scaling be compatible with backpropagation?</strong></p> <p>Yes, according to this &quot;Kaiming init&quot; paper:<a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbjVBbU9tX2NXeDRYclo4R1hMT3A0eU9iUFRmZ3xBQ3Jtc0ttSmZ3N2IyTks5RDhyM1hZcjhYTHpXQWtBdXlHYzZQY050OEtBcGZmLVljbFVGS3VwLUxpbWhYb0d6VXJ2NkJTdEJoUC1rVDY0LS1neDdZZVVURmdIZDRVUkExTzh1czc0a2VKM05WSlo0VmhtQ2l6TQ&amp;q=https%3A%2F%2Farxiv.org%2Fabs%2F1502.01852&amp;v=P6sfmUTpUmc" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1502.01852<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, the scaling is not only appropriate for forwarding with std = 1, it is also compatible with backpropagation.</p> <p><strong>Updated initialization script:</strong></p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>g <span class="token operator">=</span> torch<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">2147483647</span><span class="token punctuation">)</span> 
C  <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> n_embd<span class="token punctuation">)</span><span class="token punctuation">,</span>            generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
W1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>n_embd <span class="token operator">*</span> block_size<span class="token punctuation">,</span> n_hidden<span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token operator">/</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>n_embd <span class="token operator">*</span> block_size<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">0.5</span>
b1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span>                        generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span>
W2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">,</span>          generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span>
b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span>                      generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><h1 id="_5-batch-normalization"><a href="#_5-batch-normalization" class="header-anchor">#</a> 5. Batch Normalization</h1> <p>The above methods more finicky and fragile, compared to some modern methods to avoid the above issues.</p> <p>Intuition: If our goal is to have the hidden layer has 0 mean and 1 std, why don’t we just <strong>batch normalize</strong> them to have 0 mean and 1 std?</p> <p>from this paper: BatchNorm paper: <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbWtfREtEMndUb0E2VHpHako4TFpDWmlMaXg2d3xBQ3Jtc0tuOVRjMXBseUY3djgwbU83eFpIQldTTGxVY0xyRWtBa2tidjk4al93RExPOVRuMjF4M3FYU1JDZUNkR00zVVVHcmJNTUFXYlFVS2Qwd19LX1RJRmVwd29GTXVnWkZuNWNSMC1KZ21kRkJWd19vTVhTdw&amp;q=https%3A%2F%2Farxiv.org%2Fabs%2F1502.03167&amp;v=P6sfmUTpUmc" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1502.03167<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042313135.png" alt=""></p> <p>Implementation:</p> <p>initialize another 2 hyperparameters:</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>bngain <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>n_hidden<span class="token punctuation">)</span>
bnbias <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>Then, during training</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token comment"># minibatch construct</span>
    ix <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> Xtr<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
    Xb<span class="token punctuation">,</span> Yb <span class="token operator">=</span> Xtr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">,</span> Ytr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span>  <span class="token comment"># batch X,Y</span>

    emb <span class="token operator">=</span> C<span class="token punctuation">[</span>Xb<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">[</span>Xb<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># [7, 3, 2] =&gt; [7, 6]</span>
    hpreact <span class="token operator">=</span> emb @ W1 <span class="token operator">+</span> b1
    hpreact <span class="token operator">=</span> bngain <span class="token operator">*</span> <span class="token punctuation">(</span>hpreact <span class="token operator">-</span> hpreact<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> hpreact<span class="token punctuation">.</span>std<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">+</span> bnbias
    tanh <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>hpreact<span class="token punctuation">)</span>  <span class="token comment"># [7, 100]</span>
    logits <span class="token operator">=</span> tanh @ W2 <span class="token operator">+</span> b2  <span class="token comment"># [7, 27]</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> Yb<span class="token punctuation">)</span>

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p><strong>Think:</strong> we already have it normalized to mean=0 std=1 during the normalize step, <strong>why do we need a scale and shift step?</strong></p> <p><strong>Idea:</strong> We don’t want the data to keep being a standard normal distribution during training, we want to introduce some variabilities so that the weights can scale and shift and allow us to back propagate and have the model fit to the training data.</p> <p>Batch normalization layer is usually added after a <strong>linear</strong> or <strong>convolutional</strong> layer.</p> <h1 id="_6-two-additional-feature-bugfix-for-batch-normalization"><a href="#_6-two-additional-feature-bugfix-for-batch-normalization" class="header-anchor">#</a> 6. Two Additional Feature / Bugfix for Batch Normalization</h1> <h2 id="_6-1-running-bn-mean-and-bn-std"><a href="#_6-1-running-bn-mean-and-bn-std" class="header-anchor">#</a> 6.1 Running bn_mean and bn_std.</h2> <p>As we added this line <code>hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias</code> during training, we need to add it in eval too.</p> <p>However, issue is, when we are evaluating one sample at a time, we don’t know the mean and std.</p> <p>To resolve that, there are two ways:</p> <ol><li>At the end of training, run one pass to get the overall bn_mean and bn_std, use that as the eval bn_mean and bn_std.</li> <li>(More commonly used): calculate running bn_mean and bn_std as a parallel step during training.</li></ol> <p>To do that, on top of the existing training code,</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token comment"># minibatch construct</span>
    ix <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> Xtr<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
    Xb<span class="token punctuation">,</span> Yb <span class="token operator">=</span> Xtr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">,</span> Ytr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span>  <span class="token comment"># batch X,Y</span>

    emb <span class="token operator">=</span> C<span class="token punctuation">[</span>Xb<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">[</span>Xb<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># [7, 3, 2] =&gt; [7, 6]</span>
    hpreact <span class="token operator">=</span> emb @ W1 <span class="token operator">+</span> b1
    hpreact <span class="token operator">=</span> bngain <span class="token operator">*</span> <span class="token punctuation">(</span>hpreact <span class="token operator">-</span> hpreact<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> hpreact<span class="token punctuation">.</span>std<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">+</span> bnbias
    tanh <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>hpreact<span class="token punctuation">)</span>  <span class="token comment"># [7, 100]</span>
    logits <span class="token operator">=</span> tanh @ W2 <span class="token operator">+</span> b2  <span class="token comment"># [7, 27]</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> Yb<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>We add a few lines so that it looks like</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>running_bn_mean<span class="token punctuation">,</span> running_bn_std <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token comment"># minibatch construct</span>
    ix <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> Xtr<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
    Xb<span class="token punctuation">,</span> Yb <span class="token operator">=</span> Xtr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">,</span> Ytr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span>  <span class="token comment"># batch X,Y</span>

    emb <span class="token operator">=</span> C<span class="token punctuation">[</span>Xb<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">[</span>Xb<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># [7, 3, 2] =&gt; [7, 6]</span>
    hpreact <span class="token operator">=</span> emb @ W1 <span class="token operator">+</span> b1
    bn_mean <span class="token operator">=</span> hpreact<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>		    
		bn_std <span class="token operator">=</span> hpreact<span class="token punctuation">.</span>std<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

				running_bn_mean <span class="token operator">=</span> <span class="token number">0.999</span> <span class="token operator">*</span> running_bn_mean <span class="token operator">+</span> <span class="token number">0.001</span> <span class="token operator">*</span> bn_mean
				running_bn_std <span class="token operator">=</span> <span class="token number">0.999</span> <span class="token operator">*</span> running_bn_std <span class="token operator">+</span> <span class="token number">0.001</span> <span class="token operator">*</span> bn_std
		    
    hpreact <span class="token operator">=</span> bngain <span class="token operator">*</span> <span class="token punctuation">(</span>hpreact <span class="token operator">-</span> bn_mean<span class="token punctuation">)</span> <span class="token operator">/</span> bn_std <span class="token operator">+</span> bnbias
    tanh <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>hpreact<span class="token punctuation">)</span>  <span class="token comment"># [7, 100]</span>
    logits <span class="token operator">=</span> tanh @ W2 <span class="token operator">+</span> b2  <span class="token comment"># [7, 27]</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> Yb<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>This is how torch does it in the batch normalization layer too.</p> <h2 id="_6-2-removal-of-b1"><a href="#_6-2-removal-of-b1" class="header-anchor">#</a> 6.2: Removal of b1</h2> <p>We no longer need b1, as it will be subtracted in the next step during normalization.</p> <p>so we can instead just do</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>hpreact <span class="token operator">=</span> emb @ W1
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h1 id="_7-side-effect-of-batch-normalization"><a href="#_7-side-effect-of-batch-normalization" class="header-anchor">#</a> 7. Side Effect of Batch Normalization</h1> <p>generating logits are no longer</p> <p>Before, samples in the batches are processed independently.</p> <p>Now, samples inside the batches can be affected by other samples.</p> <ul><li>e.g., one sample having high value will cause other samples to reduce the value</li></ul> <p>But, this is a good thing.</p> <p>It pads out other data, kind of a data augmentation. By introducing these noises, it makes it harder for the model to overfit for single data samples.</p> <p>However, it couples the effect of the model on each individual batches. This would cause issues. So people have been trying to remove it and move on to other normalizations, such as layer normalizations. But it has been hard, as it is quite effective to use batch normalization.</p> <h1 id="_8-implement-batch-normalization-using-nn-batchnorm1d-tips-caveats"><a href="#_8-implement-batch-normalization-using-nn-batchnorm1d-tips-caveats" class="header-anchor">#</a> 8. Implement Batch Normalization using <code>nn.BatchNorm1d</code> (tips &amp; caveats)</h1> <p>To recap, the code is what we discussed from above about implementing batch normalization</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># MLP revisited</span>
n_embd <span class="token operator">=</span> <span class="token number">10</span> <span class="token comment"># the dimensionality of the character embedding vectors</span>
n_hidden <span class="token operator">=</span> <span class="token number">200</span> <span class="token comment"># the number of neurons in the hidden layer of the MLP</span>

g <span class="token operator">=</span> torch<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">2147483647</span><span class="token punctuation">)</span> <span class="token comment"># for reproducibility</span>
C  <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> n_embd<span class="token punctuation">)</span><span class="token punctuation">,</span>            generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
W1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>n_embd <span class="token operator">*</span> block_size<span class="token punctuation">,</span> n_hidden<span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token operator">/</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>n_embd <span class="token operator">*</span> block_size<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">0.5</span>
W2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">,</span>          generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span>
b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span>                      generator<span class="token operator">=</span>g<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0</span>
bngain <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>n_hidden<span class="token punctuation">)</span>
bnbias <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden<span class="token punctuation">)</span>

parameters <span class="token operator">=</span> <span class="token punctuation">[</span>C<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> bngain<span class="token punctuation">,</span> bnbias<span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>nelement<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> parameters<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># number of parameters in total</span>
<span class="token keyword">for</span> p <span class="token keyword">in</span> parameters<span class="token punctuation">:</span>
  p<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>

<span class="token comment"># same optimization as last time</span>
max_steps <span class="token operator">=</span> <span class="token number">2</span>
batch_size <span class="token operator">=</span> <span class="token number">32</span>
lossi <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
tanh_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

bnmean_running<span class="token punctuation">,</span> bnstd_running <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token comment"># minibatch construct</span>
    ix <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> Xtr<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span>
    Xb<span class="token punctuation">,</span> Yb <span class="token operator">=</span> Xtr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">,</span> Ytr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span>  <span class="token comment"># batch X,Y</span>

    emb <span class="token operator">=</span> C<span class="token punctuation">[</span>Xb<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">[</span>Xb<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># [7, 3, 2] =&gt; [7, 6]</span>
    hpreact <span class="token operator">=</span> emb @ W1 <span class="token operator">+</span> b1
    bnmean_i <span class="token operator">=</span> hpreact<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    bnstd_i <span class="token operator">=</span> hpreact<span class="token punctuation">.</span>std<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
    hpreact <span class="token operator">=</span> bngain <span class="token operator">*</span> <span class="token punctuation">(</span>hpreact <span class="token operator">-</span> bnmean_i<span class="token punctuation">)</span> <span class="token operator">/</span> bnstd_i <span class="token operator">+</span> bnbias
    
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        bnmean_running <span class="token operator">=</span> <span class="token number">0.999</span> <span class="token operator">*</span> bnmean_running <span class="token operator">+</span> <span class="token number">0.001</span> <span class="token operator">*</span> bnmean_i
        bnstd_running <span class="token operator">=</span> <span class="token number">0.999</span> <span class="token operator">*</span> bnstd_running <span class="token operator">+</span> <span class="token number">0.001</span> <span class="token operator">*</span> bnstd_i
    
    tanh <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>hpreact<span class="token punctuation">)</span>  <span class="token comment"># [7, 100]</span>
    logits <span class="token operator">=</span> tanh @ W2 <span class="token operator">+</span> b2  <span class="token comment"># [7, 27]</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> Yb<span class="token punctuation">)</span>

    <span class="token comment"># backward pass</span>
    <span class="token keyword">for</span> p <span class="token keyword">in</span> parameters<span class="token punctuation">:</span>
        p<span class="token punctuation">.</span>grad <span class="token operator">=</span> <span class="token boolean">None</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># update</span>
    lr <span class="token operator">=</span> <span class="token number">0.1</span> <span class="token keyword">if</span> i <span class="token operator">&lt;</span> <span class="token number">100000</span> <span class="token keyword">else</span> <span class="token number">0.01</span>  <span class="token comment"># step learning rate decay</span>
    <span class="token keyword">for</span> p <span class="token keyword">in</span> parameters<span class="token punctuation">:</span>
        p<span class="token punctuation">.</span>data <span class="token operator">+=</span> <span class="token operator">-</span>lr <span class="token operator">*</span> p<span class="token punctuation">.</span>grad

    <span class="token comment"># track stats</span>
    <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">10000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>  <span class="token comment"># print every once in a while</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">:</span><span class="token format-spec">7d</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>max_steps<span class="token punctuation">:</span><span class="token format-spec">7d</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    lossi<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>log10<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br></div></div><p>in the above, this is how BatchNorm is running</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>during init
bngain <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>n_hidden<span class="token punctuation">)</span>
bnbias <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden<span class="token punctuation">)</span>

<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>during training
    bnmean_i <span class="token operator">=</span> hpreact<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    bnstd_i <span class="token operator">=</span> hpreact<span class="token punctuation">.</span>std<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
    hpreact <span class="token operator">=</span> bngain <span class="token operator">*</span> <span class="token punctuation">(</span>hpreact <span class="token operator">-</span> bnmean_i<span class="token punctuation">)</span> <span class="token operator">/</span> bnstd_i <span class="token operator">+</span> bnbias
    
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        bnmean_running <span class="token operator">=</span> <span class="token number">0.999</span> <span class="token operator">*</span> bnmean_running <span class="token operator">+</span> <span class="token number">0.001</span> <span class="token operator">*</span> bnmean_i
        bnstd_running <span class="token operator">=</span> <span class="token number">0.999</span> <span class="token operator">*</span> bnstd_running <span class="token operator">+</span> <span class="token number">0.001</span> <span class="token operator">*</span> bnstd_i
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>Now let’s do the same using nn.Linear and nn.BatchNorm1d</p> <p>check torch’s documentation https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d</p> <blockquote><p>torch.nn.BatchNorm1d(<em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em>, <em>device=None</em>, <em>dtype=None</em>)</p></blockquote> <p>to do that in torch, we can say:</p> <p>batchnorm = nn.BatchNorm1d(n_hidden, momentum=0.001).</p> <p>the 0.001 from <code>bnmean_running = 0.999 * bnmean_running + 0.001 * bnmean_i</code>  is the momentum.</p> <p>If batch size is large, we can use 0.1/0.001 they are all fine,</p> <p>but if the batch size is small (e.g., like us, we only have 32), it will cause the running mean to fluctuate and have difficulties to converge.</p> <p>Note, before the nn.BatchNorm1d layer, we can set up the hpreact layer to have (bias=False).</p> <p>e.g., <code>nn.Linear(bias=False)</code>. This is same as we remove b1 in the above implementation, as that parameter is useless - they get subtracted during the batch norm layer anyway.</p> <h1 id="_8-implement-all-above-in-pytorch"><a href="#_8-implement-all-above-in-pytorch" class="header-anchor">#</a> 8. Implement all above in PyTorch</h1> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> random
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn

words <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'makemore-master/names.txt'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>splitlines<span class="token punctuation">(</span><span class="token punctuation">)</span>
words <span class="token operator">=</span> words

<span class="token comment"># build the vocabulary of characters and mappings to/from integers</span>
chars <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
stoi <span class="token operator">=</span> <span class="token punctuation">{</span>s<span class="token punctuation">:</span> i <span class="token operator">+</span> <span class="token number">1</span> <span class="token keyword">for</span> i<span class="token punctuation">,</span> s <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>chars<span class="token punctuation">)</span><span class="token punctuation">}</span>
stoi<span class="token punctuation">[</span><span class="token string">'.'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
itos <span class="token operator">=</span> <span class="token punctuation">{</span>i<span class="token punctuation">:</span> s <span class="token keyword">for</span> s<span class="token punctuation">,</span> i <span class="token keyword">in</span> stoi<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
vocab_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>itos<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>itos<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>vocab_size<span class="token punctuation">)</span>

<span class="token comment"># build the dataset</span>
block_size <span class="token operator">=</span> <span class="token number">3</span>  <span class="token comment"># context length: how many characters do we take to predict the next one?</span>

<span class="token keyword">def</span> <span class="token function">build_dataset</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">:</span>
    X<span class="token punctuation">,</span> Y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token keyword">for</span> w <span class="token keyword">in</span> words<span class="token punctuation">:</span>
        w <span class="token operator">=</span> <span class="token string">'...'</span> <span class="token operator">+</span> w <span class="token operator">+</span> <span class="token string">&quot;.&quot;</span>
        <span class="token keyword">for</span> c1<span class="token punctuation">,</span> c2<span class="token punctuation">,</span> c3<span class="token punctuation">,</span> c4 <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> w<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> w<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> w<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            X<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>stoi<span class="token punctuation">[</span>ele<span class="token punctuation">]</span> <span class="token keyword">for</span> ele <span class="token keyword">in</span> <span class="token punctuation">[</span>c1<span class="token punctuation">,</span>c2<span class="token punctuation">,</span>c3<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            Y<span class="token punctuation">.</span>append<span class="token punctuation">(</span>stoi<span class="token punctuation">[</span>c4<span class="token punctuation">]</span><span class="token punctuation">)</span>
    X <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>Y<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> Y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    <span class="token keyword">return</span> X<span class="token punctuation">,</span> Y

random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span>
random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>words<span class="token punctuation">)</span>
n1 <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">0.8</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">)</span>
n2 <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">0.9</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">)</span>

Xtr<span class="token punctuation">,</span> Ytr <span class="token operator">=</span> build_dataset<span class="token punctuation">(</span>words<span class="token punctuation">[</span><span class="token punctuation">:</span>n1<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 80%</span>
Xdev<span class="token punctuation">,</span> Ydev <span class="token operator">=</span> build_dataset<span class="token punctuation">(</span>words<span class="token punctuation">[</span>n1<span class="token punctuation">:</span>n2<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 10%</span>
Xte<span class="token punctuation">,</span> Yte <span class="token operator">=</span> build_dataset<span class="token punctuation">(</span>words<span class="token punctuation">[</span>n2<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 10%s</span>

<span class="token comment"># MLP revisited</span>
embd_dim <span class="token operator">=</span> <span class="token number">10</span>  <span class="token comment"># the dimensionality of the character embedding vectors</span>
n_hidden <span class="token operator">=</span> <span class="token number">200</span>  <span class="token comment"># the number of neurons in the hidden layer of the MLP</span>

<span class="token keyword">class</span> <span class="token class-name">EmbeddingConcat</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> X<span class="token punctuation">.</span>view<span class="token punctuation">(</span>X<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">build_network</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    layers <span class="token operator">=</span> <span class="token punctuation">[</span>
        nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span>embd_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
        EmbeddingConcat<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>

        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>block_size <span class="token operator">*</span> embd_dim<span class="token punctuation">,</span> out_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span>num_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>

        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> out_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span>num_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>

        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> out_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span>num_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>

        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> out_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span>num_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>

        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> out_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span>num_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>

        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>n_hidden<span class="token punctuation">,</span> out_features<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span>num_features<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span><span class="token punctuation">,</span>

    <span class="token punctuation">]</span>
    parameters <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> layer <span class="token keyword">in</span> layers<span class="token punctuation">:</span>
        parameters<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>layer<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    layers<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">*=</span> <span class="token number">0.1</span> <span class="token comment"># make the last layer less confident</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;</span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">[</span>p<span class="token punctuation">.</span>nelement<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> parameters<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> p <span class="token keyword">in</span> parameters<span class="token punctuation">:</span>
        p<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
    <span class="token keyword">return</span> layers<span class="token punctuation">,</span> parameters

build_network<span class="token punctuation">(</span><span class="token punctuation">)</span>

loss_fn <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
NUM_EPOCHS <span class="token operator">=</span> <span class="token number">1</span>
step_size <span class="token operator">=</span> <span class="token number">0.1</span>

<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    layers<span class="token punctuation">,</span> parameters <span class="token operator">=</span> build_network<span class="token punctuation">(</span><span class="token punctuation">)</span>
    lossi <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    dataset <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span>
    dataloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> drop_last<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment">#use drop_last = true to avoid one sample batch</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>NUM_EPOCHS<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> train_x<span class="token punctuation">,</span> train_y <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
            <span class="token keyword">for</span> p <span class="token keyword">in</span> parameters<span class="token punctuation">:</span>
                p<span class="token punctuation">.</span>grad <span class="token operator">=</span> <span class="token boolean">None</span>
            y_pred <span class="token operator">=</span> train_x
            <span class="token keyword">for</span> layer <span class="token keyword">in</span> layers<span class="token punctuation">:</span>
                y_pred <span class="token operator">=</span> layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>y_pred<span class="token punctuation">)</span>
            loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> train_y<span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;</span><span class="token interpolation"><span class="token punctuation">{</span>loss<span class="token operator">=</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            lossi<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>log10<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> p <span class="token keyword">in</span> parameters<span class="token punctuation">:</span>
                p<span class="token punctuation">.</span>data <span class="token operator">-=</span> step_size <span class="token operator">*</span> p<span class="token punctuation">.</span>grad
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>lossi<span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> layers

g <span class="token operator">=</span> torch<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">generate</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    layers <span class="token operator">=</span> train<span class="token punctuation">(</span>Xtr<span class="token punctuation">,</span> Ytr<span class="token punctuation">)</span>
    word_lst <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> layer <span class="token keyword">in</span> layers<span class="token punctuation">:</span>
        layer<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            xi <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
            cur_word <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            y <span class="token operator">=</span> <span class="token string">'[INIT]'</span>
            <span class="token keyword">while</span> y <span class="token operator">!=</span> <span class="token string">&quot;.&quot;</span><span class="token punctuation">:</span>
                y_prob <span class="token operator">=</span> xi
                <span class="token keyword">for</span> layer <span class="token keyword">in</span> layers<span class="token punctuation">:</span>
                    y_prob <span class="token operator">=</span> layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>y_prob<span class="token punctuation">)</span>
                y_prob <span class="token operator">=</span> y_prob<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
                iy <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>y_prob<span class="token punctuation">,</span> num_samples<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
                y <span class="token operator">=</span> itos<span class="token punctuation">[</span>iy<span class="token punctuation">]</span>
                xi <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>xi<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span>iy<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
                cur_word<span class="token punctuation">.</span>append<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
            word_lst<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">&quot;&quot;</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>cur_word<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>word_lst<span class="token punctuation">)</span>

generate<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br><span class="line-number">81</span><br><span class="line-number">82</span><br><span class="line-number">83</span><br><span class="line-number">84</span><br><span class="line-number">85</span><br><span class="line-number">86</span><br><span class="line-number">87</span><br><span class="line-number">88</span><br><span class="line-number">89</span><br><span class="line-number">90</span><br><span class="line-number">91</span><br><span class="line-number">92</span><br><span class="line-number">93</span><br><span class="line-number">94</span><br><span class="line-number">95</span><br><span class="line-number">96</span><br><span class="line-number">97</span><br><span class="line-number">98</span><br><span class="line-number">99</span><br><span class="line-number">100</span><br><span class="line-number">101</span><br><span class="line-number">102</span><br><span class="line-number">103</span><br><span class="line-number">104</span><br><span class="line-number">105</span><br><span class="line-number">106</span><br><span class="line-number">107</span><br><span class="line-number">108</span><br><span class="line-number">109</span><br><span class="line-number">110</span><br><span class="line-number">111</span><br><span class="line-number">112</span><br><span class="line-number">113</span><br><span class="line-number">114</span><br><span class="line-number">115</span><br><span class="line-number">116</span><br><span class="line-number">117</span><br><span class="line-number">118</span><br><span class="line-number">119</span><br><span class="line-number">120</span><br><span class="line-number">121</span><br><span class="line-number">122</span><br><span class="line-number">123</span><br><span class="line-number">124</span><br><span class="line-number">125</span><br><span class="line-number">126</span><br><span class="line-number">127</span><br><span class="line-number">128</span><br><span class="line-number">129</span><br><span class="line-number">130</span><br><span class="line-number">131</span><br><span class="line-number">132</span><br><span class="line-number">133</span><br><span class="line-number">134</span><br><span class="line-number">135</span><br><span class="line-number">136</span><br><span class="line-number">137</span><br><span class="line-number">138</span><br><span class="line-number">139</span><br><span class="line-number">140</span><br><span class="line-number">141</span><br><span class="line-number">142</span><br><span class="line-number">143</span><br></div></div><h1 id="_10-important-to-remember"><a href="#_10-important-to-remember" class="header-anchor">#</a> 10. Important to Remember:</h1> <ol><li>Softmax() should not be put into the model for forward call, as it will mess up with the cross entropy loss function and the backward gradient descent process.</li> <li>Note that the last layer (here the batch norm layer) is made less confident by multiplying its weights by 0.1 at initialization. Below are the reason that’s explained by ChatGPT:</li></ol> <p>In neural networks, especially in classification tasks, we sometimes want to make the last layer less confident to prevent the model from overfitting and becoming too certain about its predictions. This is particularly relevant in the following contexts:</p> <h3 id="_1-overconfidence-and-generalization"><a href="#_1-overconfidence-and-generalization" class="header-anchor">#</a> 1. <strong>Overconfidence and Generalization</strong></h3> <ul><li><strong>Overconfidence</strong>: If the network becomes too confident in its predictions (i.e., assigning very high probabilities to certain classes), it can lead to <strong>overfitting</strong> on the training data, which harms generalization to unseen data.</li> <li><strong>Generalization</strong>: By making the last layer less confident, we introduce uncertainty, which encourages the model to rely on the patterns in the data rather than memorizing specific details of the training set.</li></ul> <h3 id="_2-softmax-saturation"><a href="#_2-softmax-saturation" class="header-anchor">#</a> 2. <strong>Softmax Saturation</strong></h3> <ul><li>In classification tasks, the softmax function is often used in the final layer to convert logits (raw scores) into probabilities. When the model becomes overconfident, the softmax outputs approach extreme values (close to 0 or 1), which can cause the gradient to vanish during training, slowing down the learning process.</li> <li>Reducing the confidence makes the softmax distribution more spread out, keeping the gradients larger and allowing better gradient flow during backpropagation.</li></ul> <p>My understanding is that, as the gradient descent goes, the last layer is getting first affected, so its initial values affects the first few steps of gradient descent the most. So making them less confident is helpful for stabilizing the gradient descent process.</p> <h1 id="_11-ways-to-check-the-gradient-and-issues-with-the-model"><a href="#_11-ways-to-check-the-gradient-and-issues-with-the-model" class="header-anchor">#</a> 11. Ways to check the gradient and issues with the model:</h1> <p>(Note: I didn’t work on this in much detail, as it’s hard to verify how this would impact the final results. I will use these methods during my text classification project).</p> <h2 id="_11-1-activation-distribution"><a href="#_11-1-activation-distribution" class="header-anchor">#</a> 11.1 Activation Distribution</h2> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042314268.png" alt=""></p> <p>In the first few rounds of training, we want the results from tanh layer to be <strong>evenly distributed between -1 and 1.</strong></p> <h2 id="_11-2-gradient-distribution"><a href="#_11-2-gradient-distribution" class="header-anchor">#</a> 11.2 Gradient Distribution</h2> <p>We want most gradient to be around 0. and have a fat curve between -0.005, 0.005, etc.</p> <p>We most importantly don’t want the tail to extend to e.g., beyond 1 or even beyond 5.</p> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042314067.png" alt=""></p> <h2 id="_11-3-weights-gradient-distribution"><a href="#_11-3-weights-gradient-distribution" class="header-anchor">#</a> 11.3 Weights Gradient Distribution</h2> <p>Follow the same rule as the gradient distribution</p> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042314598.png" alt=""></p> <h2 id="_11-4-change-of-update-rate"><a href="#_11-4-change-of-update-rate" class="header-anchor">#</a> 11.4 Change of Update Rate</h2> <p>We want update rate to converge at 1e-3.</p> <p><img src="https://raw.githubusercontent.com/emmableu/image/master/202507042314129.png" alt=""></p></div></div>  <div class="page-edit"><!----> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2025/07/04, 23:19:22</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/blog/pages/fd8228/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">Makemore 2 - MLP</div></a> <a href="/blog/pages/049b6e/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">Makemore 4 - Backpropagation Ninja</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/blog/pages/fd8228/" class="prev">Makemore 2 - MLP</a></span> <span class="next"><a href="/blog/pages/049b6e/">Makemore 4 - Backpropagation Ninja</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/blog/archives" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/blog/pages/aed865/"><div>
            Makemore 5 - WaveNet
            <!----></div></a> <span class="date">07-04</span></dt></dl><dl><dd>02</dd> <dt><a href="/blog/pages/049b6e/"><div>
            Makemore 4 - Backpropagation Ninja
            <!----></div></a> <span class="date">07-04</span></dt></dl><dl><dd>03</dd> <dt><a href="/blog/pages/fd8228/"><div>
            Makemore 2 - MLP
            <!----></div></a> <span class="date">07-03</span></dt></dl> <dl><dd></dd> <dt><a href="/blog/archives" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><!----> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2025
    <span>emmableu | <a href="https://github.com/emmableu/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <div class="body-bg" style="background:url() center center / cover no-repeat;opacity:0.5;"></div> <!----> <!----></div><div class="global-ui"><div></div></div></div>
    <script src="/blog/assets/js/app.fa6bfa40.js" defer></script><script src="/blog/assets/js/2.7ce49225.js" defer></script><script src="/blog/assets/js/423.5502a762.js" defer></script>
  </body>
</html>
